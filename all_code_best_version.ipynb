{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a23b447",
   "metadata": {},
   "source": [
    "# User Note:\n",
    "I highly recommend looking at [this notebook in NBViewer](https://nbviewer.org/github/jesshuang98/nimh-mlt-risk-taking/blob/main/all_code_best_version.ipynb) to take advantage of internal HMTL anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfab64",
   "metadata": {},
   "source": [
    "# Context\n",
    "* Scientific Goal: Understand if and how emotional valence affects risk-taking\n",
    "* Data Set: 35 subjects participating in a 90 trial mood manipulation task, divided into a 20 subject train and 15 subject test set\n",
    "* Statistical Goal: Create a statistical test to detect if mood belongs in the Markov blanket of risk-taking behavior\n",
    "* Computational Goal: Explore the data sets, build task-specific features, build features that encode competing scientific hypotheses about emotional valence and risk-taking, examine the stability of features over varying regularization and data resampling, build logisitc regressions and neural networks and interpret their coefficients\n",
    "* [Full thesis](https://drive.google.com/file/d/1suI2_G2Yr0xgezhOJGKc8gicD7TjD2y3/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbff184",
   "metadata": {},
   "source": [
    "# Table Of Contents: <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Requirements and Imports](#Requirements-and-Imports)\n",
    "* [Load and Explore Data](#Load)\n",
    "    * [Per Trial Data](#per-trial)\n",
    "        * [load data and calculate task-based features](#task-features)\n",
    "        * [explore](#e1)\n",
    "    * [Per Subject Data](#per-subj)\n",
    "        * [create a per subject data frame 'gps'](#gps)\n",
    "        * [explore](#e2)\n",
    "* [Model Evaluation Helper Functions](#Evaluating-Models:-Helper-Functions)\n",
    "* [Subsetting Data Helper Functions](#Subsetting-Data-Helper-Functions)\n",
    "* [Heatmaps Helper Functions](#Heatmaps-Helper-Functions)\n",
    "* [Feature Selection](#feat)\n",
    "    * [Create Normalized Main Features and Evaluate Collinearity](#Create-Normalized-Main-Features-and-Evaluate-Collinearity)\n",
    "        * [define helper functions](#define-helper-functions-1)\n",
    "        * [generate and save features](#generate-and-save-features)\n",
    "        * [evaluate collinearity](#evaluate-collinearity)\n",
    "        * [visualize standardization](#visualize-standardization)\n",
    "    * [Calculate Quadratic Features](#Calculate-Quadratic-Features)\n",
    "    * [General Function to Standardize Features](#General-Function-to-Standardize-Features)\n",
    "    * [Stability Plots and Regularization Plots](#Stability-Plots-and-Regularization-Plots)\n",
    "        * [define helper functions](#define-helper-functions-2)\n",
    "        * [create plots](#create-plots)\n",
    "    * [Average Stability Calculations](#Average-Stability-Calculations)\n",
    "        * [save stability values](#save-stability-values)\n",
    "        * [save indicies of stability values](#save-indicies-of-stability-values)\n",
    "    * [Picking Number of Most Stable Features](#Picking-Number-of-Most-Stable-Features)\n",
    "* [Logistic Regression Modeling and Interpretation](#Logistic-Regression-Modeling-and-Interpretation)\n",
    "    * [Estimate LR LOOCV accuracy](#Estimate-LR-LOOCV-accuracy)\n",
    "    * [Estimate Weights of a Model Trained on Entire Data Set](#Estimate-Weights-of-a-Model-Trained-on-Entire-Data-Set)\n",
    "    * [Visualize Weights of a Model Trained on Entire Data Set](#Visualize-Weights-of-a-Model-Trained-on-Entire-Data-Set)\n",
    "    * [Compare Weights of a Model Trained on Entire Data Set](#Compare-Weights-of-a-Model-Trained-on-Entire-Data-Set)\n",
    "* [Neural Network Modeling and Interpretation](#Neural-Network-Modeling-and-Interpretation)\n",
    "    * [Estimate NN LOOCV accuracy](#Estimate-NN-LOOCV-accuracy)\n",
    "    * [Generate Hessian Values](#Generate-Hessian-Values)\n",
    "    * [Visualize Hessian Matrices](#Visualize-Hessian-Matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eaf0f2",
   "metadata": {},
   "source": [
    "# Requirements and Imports <a class=\"anchor\" id=\"Requirements-and-Imports\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f903810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics\n",
    "from scipy.stats.stats import pearsonr\n",
    "import math\n",
    "import csv\n",
    "from decimal import Decimal\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef5775",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff18bda",
   "metadata": {},
   "source": [
    "# Load and Explore Data <a class=\"anchor\" id=\"Load\"></a>\n",
    "Output: \n",
    "* visualizations of per trial and per subject data\n",
    "* a csv file called 'gps_[experiment].csv' which lists subjects and their characteristics. The subject ids in this file will be used for future analyses that look at cross-subject validation for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b2295",
   "metadata": {},
   "source": [
    "## Per Trial Data <a class=\"anchor\" id=\"per-trial\"></a>\n",
    "* What is the demographic information of subjects?\n",
    "* What is the range of all subject characteristics?\n",
    "* How does the choice to gamble change over time per subject?\n",
    "* How do attributes of the gambling task change over time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3271a",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5168cd",
   "metadata": {},
   "source": [
    "### load data and calculate task-based features <a class=\"anchor\" id=\"task-features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"10data_for_jess_random.csv\")\n",
    "ntrials = data.time.max() + 1\n",
    "nrows = data.shape[0]\n",
    "\n",
    "data['PredictionError'] = data['Actual'] \\\n",
    "                          - data['Gamble'] * (data['Outcome1Amount'] + data['Outcome2Amount']) / 2 \\\n",
    "                          - (1 - data['Gamble']) * data['CertainAmount']\n",
    "data['ExpectedGamble'] = (data['Outcome1Amount'] + data['Outcome2Amount']) / 2\n",
    "data['DiffCertainExpectedGamble'] = data['CertainAmount'] - data['ExpectedGamble']\n",
    "data['HigherOutcome1'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "data['LowerOutcome1'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "data['Mood'] = data['Happiness']\n",
    "\n",
    "# map subject to all row indices\n",
    "ID = {}\n",
    "for i in range(nrows):\n",
    "    id = int(data.iloc[i,0])\n",
    "    ID[id] = ID.get(id, []) + [i]\n",
    "nsubj = len(ID)\n",
    "\n",
    "# map subject characteristics to strings\n",
    "def ptype2str(x):\n",
    "    str = \"\"\n",
    "    if x == 1:\n",
    "        str = \"Depressed\"\n",
    "    else:\n",
    "        str = \"Healthy\"\n",
    "    return str\n",
    "\n",
    "def gender2str(x):\n",
    "    str = \"\"\n",
    "    if x == 1:\n",
    "        str = \"Female\"\n",
    "    else:\n",
    "        str = \"Male\"\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589e668",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e764f",
   "metadata": {},
   "source": [
    "### explore <a class=\"anchor\" id=\"e1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a32fc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store all subjects' trials\n",
    "subjDF = {}\n",
    "subjs = ID.keys()\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "    subjDF[subj] = subjD\n",
    "\n",
    "subjs = ID.keys()\n",
    "gps = []\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "    gp = sum(subjD.loc[:,'Gamble']) / subjD.loc[:,'Gamble'].size\n",
    "    gps.append(gp)\n",
    "hgp = max(gps)\n",
    "lgp = min(gps)\n",
    "print (hgp, lgp)\n",
    "\n",
    "# Visualizing all probabilities\n",
    "subjs = ID.keys()\n",
    "c = 0\n",
    "gps = []\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "    gp = sum(subjD.loc[:,'Gamble']) / subjD.loc[:,'Gamble'].size\n",
    "    gps.append(gp)\n",
    "bins = np.linspace(0, 1, 100)\n",
    "plt.hist(gps)\n",
    "plt.title(\"Gambling Probabilities\")\n",
    "plt.xlabel(\"Gambling Probability\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the Subject Data\n",
    "subjs = ID.keys()#[0:5] + ID.keys()[25:30]\n",
    "c = 0\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "\n",
    "    plt.plot('time', 'CertainAmount', data=subjD, label=\"non gamble amount\")\n",
    "    plt.plot('time', 'Outcome1Amount', data=subjD, label=\"gamble amount 1\")\n",
    "    plt.plot('time', 'Outcome2Amount', data=subjD, label=\"gamble amount 2\")\n",
    "    plt.plot('time', 'Actual', data=subjD, label=\"round outcome amount\", marker='o')\n",
    "    plt.plot('time', 'PredictionError', data=subjD, label=\"prediction error\", marker='o')\n",
    "    plt.plot('time', 'ExpectedGamble', data=subjD, label=\"expected gamble amount\", marker='o')\n",
    "    plt.plot('time', 'CertainAmount', data=subjD, label=\"certain amount\", marker=\"o\")\n",
    "    plt.plot('time', 'DiffCertainExpectedGamble', data=subjD, label=\"diff\", marker=\"o\")\n",
    "    plt.title(subjID + \" \" + subjptype)\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()\n",
    "\n",
    "    bins = np.linspace(0, 1, 50)\n",
    "    plt.hist([x for x in subjD.loc[:,'Happiness'] if str(x) != 'nan'], bins)\n",
    "    plt.title(\"Moods of \" + subjID + \" \" + subjptype)\n",
    "    plt.show()\n",
    "\n",
    "    bins = np.linspace(-10, 10, 50)\n",
    "    plt.hist(subjD.loc[:, \"CertainAmount\"], bins, alpha=0.5, label=\"CR\")\n",
    "    plt.hist(subjD.loc[:,'HigherOutcome1'], bins, alpha=0.5, label=\"H\")\n",
    "    plt.hist(subjD.loc[:, 'LowerOutcome1'], bins, alpha=0.5, label=\"L\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    c += 1\n",
    "    print (c)\n",
    "\n",
    "    h = subjD.loc[:, 'Happiness']\n",
    "    xs = np.arange(len(h))\n",
    "    yh = np.array(h)\n",
    "    yhm = np.isfinite(yh)\n",
    "    plt.plot(xs[yhm], yh[yhm], label=\"happiness\", marker='o')\n",
    "    plt.plot('time', 'Gamble', data=subjD, label=\"decision to gamble\", marker='o')\n",
    "    plt.ylim((-1.5,1.5))\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04fb74",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce46b9",
   "metadata": {},
   "source": [
    "## Per Subject Data <a class=\"anchor\" id=\"per-subj\"></a>\n",
    "* How does overall gambling probability vary with subject characteristics?\n",
    "* Can we predict a subject's overall gambling probability from subject characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccf351",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341354b",
   "metadata": {},
   "source": [
    "### create a per subject data frame: gps <a class=\"anchor\" id=\"gps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Up Dataframe: gps\n",
    "# every row has a subject characteristics &\n",
    "# global gambling probabilities\n",
    "\n",
    "final_name = \"gps_random.csv\"\n",
    "\n",
    "gps = data.drop_duplicates(subset = ['subject_id', 'age', 'gender',\n",
    "                                     'ptype', 'MFQ', 'SHAPS'])\n",
    "gps = gps.drop(['time', 'CertainAmount',\n",
    "                'Outcome1Amount', 'Outcome2Amount',\n",
    "                'Gamble', 'Actual', 'Happiness'], axis = 1)\n",
    "\n",
    "gps['GamblingProbability'] = 0.0\n",
    "\n",
    "# index of 'Gamble'\n",
    "gi = list(data).index('Gamble')\n",
    "\n",
    "# index of 'GamblingProbability'\n",
    "gpi = list(gps).index('GamblingProbability')\n",
    "\n",
    "# calculate subject overall gambling probability\n",
    "for i in range(nsubj):\n",
    "    id = int(gps.iloc[i,0])\n",
    "    indices = ID[id]\n",
    "    gambles = data.iloc[indices,gi]\n",
    "    gps.iloc[i,gpi] = float(sum(gambles))/float(len(gambles))\n",
    "\n",
    "gps.to_csv(final_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c05ba",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f140f61",
   "metadata": {},
   "source": [
    "### explore <a class=\"anchor\" id=\"e2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3ede8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Data Characteristics get ranges of subject characteristics\n",
    "shapss = gps.loc[:,'SHAPS']\n",
    "sl = min(shapss)\n",
    "sh = max(shapss)\n",
    "print (sl, sh)\n",
    "\n",
    "mfqs = gps.loc[:,'MFQ']\n",
    "ml = min(mfqs)\n",
    "mh = max(mfqs)\n",
    "print (ml, mh)\n",
    "\n",
    "ages = gps.loc[:,'age']\n",
    "al = min(ages)\n",
    "ah = max(ages)\n",
    "print (al, ah)\n",
    "\n",
    "genders = gps.loc[:,'gender']\n",
    "counts = genders.value_counts()\n",
    "f = counts[1] / ntrials\n",
    "m = counts[2] / ntrials\n",
    "print (f, m)\n",
    "\n",
    "dep = gps.loc[:,'ptype']\n",
    "counts = dep.value_counts()\n",
    "d = counts[1] / ntrials\n",
    "h = counts[2] / ntrials\n",
    "print (d, h)\n",
    "\n",
    "## Plot data in gps\n",
    "xnames = ['age', 'gender', 'ptype', 'MFQ', 'SHAPS']\n",
    "for xname in xnames:\n",
    "    gps.plot(kind='scatter',x=xname,y='GamblingProbability',color='red')\n",
    "    plt.show()\n",
    "\n",
    "## Run linear regressions\n",
    "y = gps.iloc[:,gpi]\n",
    "X = gps.loc[:,xnames]\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "feature = 'ptype'\n",
    "y = gps.iloc[:,gpi]\n",
    "x = gps.loc[:,feature].values.reshape((-1, 1))\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "y_pred = model.intercept_ + model.coef_ * x\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, model.intercept_ + model.coef_ * x, '-')\n",
    "plt.xlabel(feature)\n",
    "plt.ylabel(\"Gambling Probability\")\n",
    "plt.title(\"GP vs. \" + feature + \": r^2 = \" + str(r_sq))\n",
    "plt.show()\n",
    "\n",
    "## Pearson Correlation\n",
    "\n",
    "feature = 'ptype'\n",
    "feature2 = 'SHAPS'\n",
    "#y = gps.iloc[:,gpi]\n",
    "x1 = gps.loc[:,feature]\n",
    "x2 = gps.loc[:,feature2]\n",
    "\n",
    "r = pearsonr(x1,x2)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253dc0e2",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0636da",
   "metadata": {},
   "source": [
    "# Evaluating Models: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Per Trial Predictive Accuracy\n",
    "\n",
    "\n",
    "def accthreshold(obsvs, preds, threshold):\n",
    "    '''\n",
    "    Evaluate the accuracy of a prediction given a list of probabilitstic predictions (preds), \n",
    "    a probabilititic threshold (threshold), and the binary ground truth (obsvs)\n",
    "    '''\n",
    "    preds_bool = preds > threshold\n",
    "    acc = sklearn.metrics.accuracy_score(obsvs, preds_bool)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def maxacc(obsvs, preds):\n",
    "    '''\n",
    "    Given a list of probabilitstic predictions (preds) and the binary ground truth (obsvs),\n",
    "    evaluate the largest accuracy achieved across probabilistic thresholds 0.01, 0.02, ... 1.00\n",
    "    '''\n",
    "    thresholds = [float(i) / float(100) for i in range(1,100,1)]\n",
    "    temp = 0\n",
    "    print (\"the accs are \")\n",
    "    for threshold in thresholds:\n",
    "        temp2 = accthreshold(obsvs, preds, threshold)\n",
    "        print (temp2)\n",
    "        temp = max(temp, temp2)\n",
    "    return temp\n",
    "\n",
    "def balancedacc(obsvs, preds, threshold):\n",
    "    '''\n",
    "    Evaluate the balanced accuracy of a prediction given a list of probabilitstic predictions (preds),\n",
    "    a probabilititic threshold (threshold), and the binary ground truth (obsvs)\n",
    "    '''\n",
    "    preds_bool = preds > threshold\n",
    "    conditionpositive = sum(obsvs)\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(obsvs, preds_bool).ravel()\n",
    "    tpr = float(tp) / float(tp + fn)\n",
    "    fpr = float(tn) / float(tn + fp)\n",
    "    bacc = (tpr + fpr) / 2\n",
    "    return bacc\n",
    "\n",
    "def maxbacc(obsvs, preds):\n",
    "    '''\n",
    "    Given a list of probabilitstic predictions (preds) and the binary ground truth (obsvs),\n",
    "    evaluate the largest balanced accuracy achieved across probabilistic thresholds 0.01, 0.02, ... 1.00\n",
    "    '''\n",
    "    thresholds = [float(i) / float(100) for i in range(1,100,1)]\n",
    "    temp = 0\n",
    "    print (\"the accs are \")\n",
    "    for threshold in thresholds:\n",
    "        temp2 = balancedacc(obsvs, preds, threshold)\n",
    "        print (temp2)\n",
    "        temp = max(temp, temp2)\n",
    "    return temp\n",
    "\n",
    "\n",
    "# Evaluate Per Subject Predictive Accuracy\n",
    "\n",
    "def get_indices(x,xs):\n",
    "    '''\n",
    "    Find the indicies of an element x in a list xs\n",
    "    '''\n",
    "    return [i for (i,y) in zip(range(len(xs)), xs) if y == x]\n",
    "\n",
    "\n",
    "def subjauc(subjs, obsvs, preds):\n",
    "    '''\n",
    "    Evaluate the AUC of a prediction averaged across subjects,\n",
    "    given the probabilistic predictions (preds),\n",
    "    the binary ground truth (obsvs),\n",
    "    and the subject ID of each trial (subjs)\n",
    "    e.g.\n",
    "    obsvs : [1  , 1  , 0  , 1  , 0  , 1  , 0  , 1  , 1]\n",
    "    preds : [0.8, 0.4, 0.2, 0.8, 0.6, 0.2, 0.6, 0.4, 0.8] \n",
    "    subjs : [a, a, a, b, b, b, c, c, c]\n",
    "\n",
    "    aucs  : [1.0, 0.5, 0.5]\n",
    "    average AUC : 0.66\n",
    "    '''\n",
    "    ss = set(subjs)\n",
    "    aucs = []\n",
    "    for s in ss:\n",
    "        sind = get_indices(s, subjs)\n",
    "        obsvs_s = obsvs[sind]\n",
    "        preds_s = preds[sind]\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvs_s, preds_s)\n",
    "        auc_s = sklearn.metrics.auc(fpr, tpr)\n",
    "        aucs.append(auc_s)\n",
    "    return np.nanmean(aucs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9565ba",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262093d0",
   "metadata": {},
   "source": [
    "# Subsetting Data Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Subsetting Data by Subject ID or Feature Column Index\n",
    "\n",
    "\n",
    "def subjXY(sid, x, y):\n",
    "    '''\n",
    "    Subset the x and y values corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    x = np.array([['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                  ['a', 0, 7, 2], \n",
    "                  ['b', 1, 4, 5], \n",
    "                  ['c', 1, 5, 3]])\n",
    "    y = np.array([['subject ID', 'gamble'],\n",
    "                  ['a', 1], \n",
    "                  ['b', 1], \n",
    "                  ['c', 0]])\n",
    "    subjXY(sid,x,y) = [np.array([0, 7, 2]), np.array([1])]\n",
    "    '''\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = np.where(sidscol == sid)\n",
    "    features = np.squeeze(x[inds,1:])\n",
    "    targets = np.squeeze(y[inds,1:])\n",
    "    return [features, targets]\n",
    "\n",
    "def getXY(sids, x, y):\n",
    "    \n",
    "    '''\n",
    "    Subset the x and y values corresponding to a list of subject IDs sids\n",
    "\n",
    "    sids = a list of subject ID strings\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sids = ['a','b']\n",
    "    x = np.array([['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                  ['a', 0, 7, 2], \n",
    "                  ['b', 1, 4, 5], \n",
    "                  ['c', 1, 5, 3]])\n",
    "    y = np.array([['subject ID', 'gamble'],\n",
    "                  ['a', 1], \n",
    "                  ['b', 1], \n",
    "                  ['c', 0]])\n",
    "    getXY(sids,x,y) = [np.array([0, 7, 2], [1, 4, 5]), np.array([1, 1])]\n",
    "    '''\n",
    "    \n",
    "    xs = np.zeros(shape=[x.shape[0]-1, x.shape[1]-1])\n",
    "    ys = np.zeros(shape=[y.shape[0]-1, 1])\n",
    "    counter = 0\n",
    "    for i in range(len(sids)):\n",
    "        [sx, sy] = subjXY(sids[i], x, y)\n",
    "        size = sx.shape[0]\n",
    "        xs[range(counter, counter+size), :] = sx\n",
    "        ys[range(counter, counter+size)] = sy.reshape(len(sy),1)\n",
    "        counter = counter + size\n",
    "    return [xs, ys]\n",
    "\n",
    "def subjY(sid, y):\n",
    "    '''\n",
    "    Subset the y values corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    y = np.array([['a', 1], ['b', 1], ['c', 0]])\n",
    "    subjY(sid,y) = np.array([1])\n",
    "    '''\n",
    "    sidscol = np.array(y[:,0])\n",
    "    inds = np.where(sidscol == sid)\n",
    "    targets = np.squeeze(y[inds,1:])\n",
    "    return targets\n",
    "\n",
    "def notsubjXY(sid, x, y):\n",
    "    '''\n",
    "    Subset the x and y values not corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    x = np.array([['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                  ['a', 0, 7, 2], \n",
    "                  ['b', 1, 4, 5], \n",
    "                  ['c', 1, 5, 3]])\n",
    "    y = np.array([['subject ID', 'gamble'],\n",
    "                  ['a', 1], \n",
    "                  ['b', 1], \n",
    "                  ['c', 0]])\n",
    "    notsubjXY(sid,x,y) = [np.array([[1, 4, 5],[1, 5, 3]]), np.array([1,0])]\n",
    "    '''\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = np.where(sidscol != sid)\n",
    "    features = np.squeeze(x[inds,1:])[1:,]\n",
    "    targets = np.squeeze(y[inds,1:])[1:]\n",
    "    return [features, targets]\n",
    "\n",
    "def notsubjY(sid, y):\n",
    "    '''\n",
    "    Subset the y values not corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    y = np.array([['subject ID', 'gamble'],\n",
    "                  ['a', 1], \n",
    "                  ['b', 1], \n",
    "                  ['c', 0]])\n",
    "    notsubjY(sid,y) = [np.array([1,0])]\n",
    "    '''\n",
    "    sidscol = np.array(y[:,0])\n",
    "    inds = np.where(sidscol != sid)\n",
    "    targets = np.squeeze(y[inds,1:])[1:]\n",
    "    return targets\n",
    "\n",
    "def subsetx(x, l):\n",
    "    '''\n",
    "    Subset a list of l specific features of x\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    l = 0 indexed list of desired features\n",
    "\n",
    "    e.g.\n",
    "    x = np.array([['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                  ['a', 0, 7, 2], \n",
    "                  ['b', 1, 4, 5], \n",
    "                  ['c', 1, 5, 3]])\n",
    "    l = [1,3]\n",
    "    subsetx(x,l) = np.array([['subject ID', 'depression status', 'anhedonia score'], \n",
    "                             ['a', 0, 2], \n",
    "                             ['b', 1, 5], \n",
    "                             ['c', 1, 3]])\n",
    "    '''\n",
    "    xnew = np.zeros(shape=(x.shape[0], len(l) + 1))\n",
    "    xnew[:,0] = x[:,0]\n",
    "    for col in range(len(l)):\n",
    "        colind = l[col]\n",
    "        xnew[:,col+1] = x[:,colind]\n",
    "    return xnew\n",
    "\n",
    "def subsetxfile(filename, feats):\n",
    "    '''\n",
    "    Subset a list of l specific features of x\n",
    "    filename = the filename of a file with the 0th row as the names of features \n",
    "               and the 0th column as subject ID strings\n",
    "    feats = 0-indexed list of desired features\n",
    "\n",
    "    e.g.\n",
    "    filename = 'feats.csv' containing {columns = ['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                                       data = [['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]]}\n",
    "    feats = [1,3]\n",
    "    subsetxfile(filename, feats) = np.array([['subject ID', 'depression status', 'anhedonia score'],\n",
    "                                             ['a', 0, 2], \n",
    "                                             ['b', 1, 5], \n",
    "                                             ['c', 1, 3]])\n",
    "    '''\n",
    "    x = np.genfromtxt(filename, delimiter=\",\")\n",
    "    xnew = np.zeros(shape=(x.shape[0], len(feats) + 1))\n",
    "    xnew[:,0] = x[:,0]\n",
    "    for col in range(len(feats)):\n",
    "        colind = feats[col]\n",
    "        xnew[:,col+1] = x[:,colind]\n",
    "    return xnew\n",
    "\n",
    "\n",
    "def subsetfilesubj(filename, sids):\n",
    "    '''\n",
    "    Subset all the features in filename corresponding to subjects in the list sids\n",
    "    filename = the filename of a file with the 0th row as the names of features \n",
    "               and the 0th column as subject ID strings\n",
    "    sids = list of desired subject IDs\n",
    "\n",
    "    e.g.\n",
    "    filename = 'feats.csv' containing {columns = ['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                                       data = [['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]]}\n",
    "    sids = ['a','b']\n",
    "    subsetfilesubj(filename,sids) = np.array([[0, 7, 2], [1, 4, 5]])\n",
    "    '''\n",
    "    x = np.genfromtxt(filename+'.csv', delimiter=\",\")\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = []\n",
    "    for sid in sids:\n",
    "        ind = np.where(sidscol == sid)\n",
    "        inds.append(ind)\n",
    "    inds = np.array(inds).reshape((-1))\n",
    "    fts = np.squeeze(x[inds,])\n",
    "    with open(filename + '_overlap_frm'+ traintype + '.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(np.zeros(x.shape[1]))\n",
    "        writer.writerows(fts)\n",
    "    writeFile.close()\n",
    "    return fts[:,1:]\n",
    "\n",
    "def subsetfilesubjSUBJS(filename, sids):\n",
    "    '''\n",
    "    Subset the list of subject ID indicies in filename corresponding to subjects in the list sids\n",
    "    filename = the filename of a file with the 0th row as the names of features \n",
    "               and the 0th column as subject ID strings\n",
    "    sids = list of desired subject IDs\n",
    "\n",
    "    e.g.\n",
    "    filename = 'feats.csv' containing {columns = ['subject ID', 'depression status', 'age', 'anhedonia score', 'trial'], \n",
    "                                       data = [['a', 0, 7, 2, 1], ['a', 0, 7, 2, 2],\n",
    "                                               ['b', 1, 4, 5, 1], ['b', 1, 4, 5, 2],\n",
    "                                               ['c', 1, 5, 3, 1], ['c', 1, 5, 3, 2]]}\n",
    "    sids = ['a','b']\n",
    "    subsetfilesubjSUBJS(filename,sids) = np.array(['a','a','b','b'])\n",
    "    '''\n",
    "    x = np.genfromtxt(filename+'.csv', delimiter=\",\")\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = []\n",
    "    for sid in sids:\n",
    "        ind = np.where(sidscol == sid)\n",
    "        inds.append(ind)\n",
    "    inds = np.array(inds).reshape((-1))\n",
    "    fts = np.squeeze(x[inds,])\n",
    "    with open(filename + '_overlap_frm'+ traintype + '.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(np.zeros(x.shape[1]))\n",
    "        writer.writerows(fts)\n",
    "    writeFile.close()\n",
    "    return fts[:,0]\n",
    "\n",
    "def strip(m):\n",
    "    '''\n",
    "    input: matrix\n",
    "    output: matrix without column names or row names\n",
    "    '''\n",
    "    m = m[1:, 1:]\n",
    "    return m\n",
    "\n",
    "def stripfile(filename):\n",
    "    '''\n",
    "    input: filename\n",
    "    output: matrix without column names or row names\n",
    "    '''\n",
    "    m = np.genfromtxt(filename, delimiter=\",\")\n",
    "    m = m[1:, 1:]\n",
    "    return m\n",
    "\n",
    "def getcolumnnames(filename):\n",
    "    '''\n",
    "    Given the filename of a csv file\n",
    "    return the columnnames\n",
    "    \n",
    "    e.g. getcolumnnames(\"results.csv\") = ['model #', 'AUC', 'ACC', 'Subject ACC']\n",
    "    '''\n",
    "    with open(filename, \"rt\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        i = next(reader)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699198d",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39fa45",
   "metadata": {},
   "source": [
    "# Heatmaps Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce87043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Visualize Heatmaps\n",
    "\n",
    "# https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    '''\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "    [Back to Table of Contents](#toc)Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    '''\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, orientation= \"horizontal\", ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=0, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    '''\n",
    "    A function to annotate a heatmap.\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    '''\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bcbeb",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ae135",
   "metadata": {},
   "source": [
    "# Feature Selection <a class=\"anchor\" id=\"feat\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968e6c1",
   "metadata": {},
   "source": [
    "## Create Normalized Main Features and Evaluate Collinearity\n",
    "\n",
    "Main Features:\n",
    "* normalization = (centered at 0 and with standard deviation 1) \n",
    "* The subject characteristics will be age, gender, and diagnosis. \n",
    "* The trial parameters pertaining to the current trial will an indicator (with 1 if for this trial, choosing to gamble will always yield more money than choosing not to gamble), current expected reward (average of not gamble reward option and two gamble reward options), and gambling range (difference between higher gamble reward option and lower gamble reward option). \n",
    "* The trial parameters pertaining to the outcomes of past trials are an exponential sum of past reward prediction errors and an exponential sum of past outcomes. To elaborate, reward prediction error is actual reward outcome - expected reward outcome while the exponential sums are weighed so more recent trials have large weights closer to 1 and more previous trials have smaller weights closer to 0.\n",
    "\n",
    "Output: \n",
    "* x and y csv files where x contains main features to predict gambling probability (0 - 1) and y contains the target gambles themselves (1's or 0's)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a22740",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01d411",
   "metadata": {},
   "source": [
    "### define helper functions <a class=\"anchor\" id=\"define-helper-functions-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ages 12 to 18\n",
    "# to ages -3 to 3\n",
    "def age(v):\n",
    "    return v - 15\n",
    "\n",
    "# convert 1 (female) to 0\n",
    "# convert 2 (male) to 1\n",
    "def gender(v):\n",
    "    return v-1\n",
    "\n",
    "# convert 1 (depression) to 1\n",
    "# convert 2 (healthy) to 0\n",
    "def diagnosis(v):\n",
    "    nv = 0\n",
    "    if v == 1:\n",
    "        nv = 1\n",
    "    return nv\n",
    "\n",
    "def bool2int(b):\n",
    "    i = 1 if b else 0\n",
    "    return i\n",
    "\n",
    "\n",
    "def ems(rewards, gamma):\n",
    "    pastrewards = np.zeros(len(rewards))\n",
    "    for i in range(len(rewards) - 1):\n",
    "        pastrewards[i+1] = gamma * pastrewards[i] + rewards.iloc[i]\n",
    "    return pastrewards\n",
    "\n",
    "# return the utility\n",
    "# as sigmoid function centered at 0 instead 0.5\n",
    "def utility(v):\n",
    "    u = math.exp(v) / (math.exp(v) + 1) - 0.5\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7f074",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054a0e3",
   "metadata": {},
   "source": [
    "### generate and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96535207",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacsv = \"10data_for_jess_random\"\n",
    "suffix = \"_random\"\n",
    "cn = 'Actual' \n",
    "gamma = 0.5\n",
    "\n",
    "data=pd.read_csv(datacsv + \".csv\", sep=',',header='infer')\n",
    "nrows = data.shape[0]\n",
    "data['HigherOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "data['LowerOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "data['Mood'] = data['Happiness']\n",
    "data['diagnosis'] = data['ptype']\n",
    "\n",
    "# update subject features\n",
    "data['age'] = data['age'].copy().map(age)\n",
    "data['gender'] = data['gender'].copy().map(gender)\n",
    "data['diagnosis'] = data['diagnosis'].copy().map(diagnosis)\n",
    "data['Indicator'] = data['CertainAmount'] < data['LowerOutcome']\n",
    "data['Indicator'] = data['Indicator'].map(bool2int)\n",
    "\n",
    "# standardize raw trial parameters\n",
    "v = data.loc[:,cn].copy()\n",
    "sd = np.std(v)\n",
    "print (\"for \", datacsv, \" sd of \", cn, \" : \", str(sd))\n",
    "data['CertainAmount'] = data['CertainAmount'].copy().map(lambda x : x / sd)\n",
    "data['HigherOutcome'] = data['HigherOutcome'].copy().map(lambda x : x / sd)\n",
    "data['LowerOutcome'] = data['LowerOutcome'].copy().map(lambda x : x / sd)\n",
    "data['Actual'] = data['Actual'].copy().map(lambda x : x / sd)\n",
    "\n",
    "# calculate utility over trial parameters\n",
    "data['CertainAmountUtility'] = data['CertainAmount'].copy().map(utility)\n",
    "data['HigherOutcomeUtility'] = data['HigherOutcome'].copy().map(utility)\n",
    "data['LowerOutcomeUtility'] = data['LowerOutcome'].copy().map(utility)\n",
    "data['ActualUtility'] = data['Actual'].copy().map(utility)\n",
    "\n",
    "# compute 2nd layer values\n",
    "data['CurrentExpectedReward'] = (data['CertainAmount'] + data['HigherOutcome'] + data['LowerOutcome']) / 3\n",
    "data['ExpectedGambleOutcome'] = (data['LowerOutcome'] + data['HigherOutcome']) / 2\n",
    "data['diff'] = data['ExpectedGambleOutcome'] - data['CertainAmount']\n",
    "data['PredictionError'] = data['Gamble'] * (data['Actual'] - data['ExpectedGambleOutcome'])\n",
    "data['CertainReward'] = (1 - data['Gamble']) * data['Actual']\n",
    "data['GamblingReward'] = data['Gamble'] * data['Actual']\n",
    "data['GamblingRange'] = data['HigherOutcome'] - data['LowerOutcome']\n",
    "\n",
    "# compute 2nd layer utilities\n",
    "data['ExpectedGambleOutcomeUtility'] = (data['LowerOutcomeUtility'] + data['HigherOutcomeUtility']) / 2\n",
    "data['diffUtility'] = data['ExpectedGambleOutcomeUtility'] - data['CertainAmountUtility']\n",
    "data['PredictionErrorUtility'] = data['Gamble'] * (data['ActualUtility'] - data['ExpectedGambleOutcomeUtility'])\n",
    "data['CertainRewardUtility'] = (1 - data['Gamble']) * data['ActualUtility']\n",
    "data['GamblingRewardUtility'] = data['Gamble'] * data['ActualUtility']\n",
    "data['GamblingRangeUtility'] = data['HigherOutcomeUtility'] - data['LowerOutcomeUtility']\n",
    "data['CurrentExpectedRewardUtility'] = (data['CertainAmountUtility'] + data['HigherOutcomeUtility'] + data['LowerOutcomeUtility']) / 3\n",
    "\n",
    "# print final column names\n",
    "colnames = data.columns\n",
    "\n",
    "## record the indices of each new subject\n",
    "ID = {}\n",
    "for i in range(nrows):\n",
    "    id = int(data.iloc[i,0])\n",
    "    ID[id] = ID.get(id, []) + [i]\n",
    "nsubj = len(ID)\n",
    "\n",
    "## Create a dictionary to store all subjects\n",
    "subjDF = {}\n",
    "subjs = ID.keys()\n",
    "for subj in subjs:\n",
    "    subjD = data.iloc[ID[subj],:].copy()\n",
    "    subjDF[subj] = subjD\n",
    "\n",
    "# compute timeseries values\n",
    "\n",
    "for subj in subjs:\n",
    "    # fill mood with most recently reported mood, otherwise pad with 0's\n",
    "    subjDF[subj].loc[:,'Mood'] = subjDF[subj].loc[:,'Mood'].copy().fillna(method='ffill').fillna(0)\n",
    "    # Calculate EMS features for the following features \n",
    "    for col in ['Actual', 'CertainReward', 'GamblingReward', 'PredictionError',\n",
    "                'ActualUtility', 'CertainRewardUtility', 'GamblingRewardUtility', 'PredictionErrorUtility']:\n",
    "        rewards = subjDF[subj].loc[:,col].copy()\n",
    "        pastrewards = ems(rewards, gamma)\n",
    "        subjDF[subj].loc[:,col+'EMS'+str(gamma)] = pastrewards\n",
    "\n",
    "# remove the first three trials of all subjects\n",
    "def strip3(df):\n",
    "    newdf = df.iloc[3:,:].copy()\n",
    "    return newdf\n",
    "stripsubjDF = dict(map(lambda kv: (kv[0], strip3(kv[1])), subjDF.items()))\n",
    "\n",
    "# put subject data into a new dataframe\n",
    "datanew = pd.concat(stripsubjDF.values())\n",
    "\n",
    "# save all the columns into a csv\n",
    "datanew.to_csv(datacsv + \"_normed_standardized_\" + cn + \".csv\", index=False)\n",
    "\n",
    "namesl = ['subject_id', 'age', 'gender', 'diagnosis', 'mood', \n",
    "          'current expected reward', 'current gambling range', 'current indicator',\n",
    "          'past rewards', 'past reward prediction error']\n",
    "\n",
    "# modelEV\n",
    "xl = ['subject_id', 'age', 'gender', 'diagnosis', 'Mood',\n",
    "      'CurrentExpectedReward', 'GamblingRange', 'Indicator', \n",
    "      'ActualEMS'+str(gamma), 'PredictionErrorEMS'+str(gamma)]\n",
    "yl = ['subject_id', 'Gamble']\n",
    "x = datanew[xl].copy()\n",
    "x = x.rename(index=str, columns=dict(zip(xl, namesl))).copy()\n",
    "y = datanew[yl].copy()\n",
    "x.to_csv(\"x\" + suffix + \"_normed_standardized_\" + cn + \"_EV\" + str(gamma) + \".csv\", index=False)\n",
    "y.to_csv(\"y\" + suffix + \"_normed_standardized_\" + cn + \"_EV\" + str(gamma) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c2a8",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb24e0",
   "metadata": {},
   "source": [
    "### evaluate collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacsv = \"10data_for_jess_random\"\n",
    "suffix = \"_random\"\n",
    "gamma = 0.5\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True, figsize=(15,8))\n",
    "\n",
    "data=pd.read_csv(datacsv + \".csv\", sep=',',header='infer')\n",
    "nrows = data.shape[0]\n",
    "data['HigherOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "data['LowerOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "data['Mood'] = data['Happiness']\n",
    "data['diagnosis'] = data['ptype']\n",
    "\n",
    "# update subject features\n",
    "data['age'] = data['age'].copy().map(age)\n",
    "data['gender'] = data['gender'].copy().map(gender)\n",
    "data['diagnosis'] = data['diagnosis'].copy().map(diagnosis)\n",
    "data['Indicator'] = data['CertainAmount'] < data['LowerOutcome']\n",
    "\n",
    "# compute 2nd layer values\n",
    "data['CurrentExpectedReward'] = (data['CertainAmount'] + data['HigherOutcome'] + data['LowerOutcome']) / 3\n",
    "data['ExpectedGambleOutcome'] = (data['LowerOutcome'] + data['HigherOutcome']) / 2\n",
    "data['diff'] = data['ExpectedGambleOutcome'] - data['CertainAmount']\n",
    "data['PredictionError'] = data['Gamble'] * (data['Actual'] - data['ExpectedGambleOutcome'])\n",
    "data['CertainReward'] = (1 - data['Gamble']) * data['Actual']\n",
    "data['GamblingReward'] = data['Gamble'] * data['Actual']\n",
    "data['GamblingRange'] = data['HigherOutcome'] - data['LowerOutcome']\n",
    "\n",
    "# update parameters that summarize the past (mood, cumulative reward, past not gamble reward, past gamble reward)\n",
    "## record the indices of each new subject\n",
    "ID = {}\n",
    "for i in range(nrows):\n",
    "    id = int(data.iloc[i,0])\n",
    "    ID[id] = ID.get(id, []) + [i]\n",
    "nsubj = len(ID)\n",
    "\n",
    "## Create a dictionary to store all subjects\n",
    "subjDF = {}\n",
    "subjs = ID.keys()\n",
    "for subj in subjs:\n",
    "    subjD = data.iloc[ID[subj],:].copy()\n",
    "    subjDF[subj] = subjD\n",
    "\n",
    "# compute timeseries values\n",
    "for subj in subjs:\n",
    "    # fill mood with most recently reported mood, otherwise pad with 0's\n",
    "    subjDF[subj].loc[:,'Mood'] = subjDF[subj].loc[:,'Mood'].copy().fillna(method='ffill').fillna(0)\n",
    "    # Calculate EMS features for the following features \n",
    "    for col in ['Actual', 'CertainReward', 'GamblingReward', 'PredictionError']:\n",
    "        rewards = subjDF[subj].loc[:,col].copy()\n",
    "        pastrewards = ems(rewards, gamma)\n",
    "        subjDF[subj].loc[:,col+'EMS'+str(gamma)] = pastrewards\n",
    "\n",
    "# remove the first three trials of all subjects\n",
    "def strip3(df):\n",
    "    newdf = df.iloc[3:,:].copy()\n",
    "    return newdf\n",
    "stripsubjDF = dict(map(lambda kv: (kv[0], strip3(kv[1])), subjDF.items()))\n",
    "\n",
    "# put subject data into a new dataframe\n",
    "datanew = pd.concat(stripsubjDF.values())\n",
    "\n",
    "## new features\n",
    "namesl = ['subject_id', 'age', 'gender', 'diagnosis', 'mood', \n",
    "          'current expected reward', 'current diff', 'current gambling range', 'current indicator',\n",
    "          'past rewards', 'past reward prediction error']\n",
    "\n",
    "xl = ['subject_id', 'age', 'gender', 'diagnosis', 'Mood',\n",
    "      'CurrentExpectedReward', 'diff', 'GamblingRange', 'Indicator', \n",
    "      'ActualEMS'+str(gamma), 'PredictionErrorEMS'+str(gamma)]\n",
    "yl = ['subject_id', 'Gamble']\n",
    "x = datanew[xl].copy()\n",
    "x = x.rename(index=str, columns=dict(zip(xl, namesl))).copy()\n",
    "y = datanew[yl].copy()\n",
    "\n",
    "print (suffix)\n",
    "im = axes.imshow(x.iloc[:,1:].copy().corr(), vmin=-1.0, vmax=1.0)\n",
    "\n",
    "print (\"new features : \")\n",
    "for i in range(1, len(namesl)):\n",
    "    print (i, namesl[i])\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig(\"corr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435bb12",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242ecbb",
   "metadata": {},
   "source": [
    "### visualize standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacsv=\"10data_for_jess_random\"\n",
    "suffix=\"_random\"\n",
    "cn = 'Actual'\n",
    "\n",
    "rows = ['Before', 'After']\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(15,8))\n",
    "st = fig.suptitle(\"Trial Values Before (Top) and After (Bottom) Standardizing with \" + cn, fontsize=\"x-large\")\n",
    "\n",
    "    \n",
    "# https://stackoverflow.com/questions/31726643/how-do-i-get-multiple-subplots-in-matplotlib\n",
    "\n",
    "data=pd.read_csv(datacsv + \".csv\", sep=',',header='infer')\n",
    "nrows = data.shape[0]\n",
    "data['HigherOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "data['LowerOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "data['Mood'] = data['Happiness']\n",
    "data['diagnosis'] = data['ptype']\n",
    "\n",
    "axes[0].hist(data['HigherOutcome'], label = 'Higher Outcome')\n",
    "axes[0].hist(data['LowerOutcome'], label = 'Lower Outcome')\n",
    "axes[0].hist(data['CertainAmount'], label = 'Certain Amount')\n",
    "axes[0].tick_params(axis='both', which='major', pad=10)\n",
    "\n",
    "# update subject features\n",
    "data['age'] = data['age'].copy().map(age)\n",
    "data['gender'] = data['gender'].copy().map(gender)\n",
    "data['diagnosis'] = data['diagnosis'].copy().map(diagnosis)\n",
    "\n",
    "# standardize raw trial parameters\n",
    "\n",
    "v = data.loc[:,cn].copy()\n",
    "sd = np.std(v)\n",
    "print (\"for \", datacsv, \" sd of \", cn, \" : \", str(sd))\n",
    "data['CertainAmount'] = data['CertainAmount'].copy().map(lambda x : x / sd)\n",
    "data['HigherOutcome'] = data['HigherOutcome'].copy().map(lambda x : x / sd)\n",
    "data['LowerOutcome'] = data['LowerOutcome'].copy().map(lambda x : x / sd)\n",
    "data['Actual'] = data['Actual'].copy().map(lambda x : x / sd)\n",
    "\n",
    "axes[1].hist(data['HigherOutcome'], label = 'Higher Outcome')\n",
    "axes[1].hist(data['LowerOutcome'], label = 'Lower Outcome')\n",
    "axes[1].hist(data['CertainAmount'], label = 'Certain Amount')\n",
    "axes[1].tick_params(axis='both', which='major', pad=10)\n",
    "    \n",
    "\n",
    "# https://stackoverflow.com/questions/39164828/global-legend-for-all-subplots\n",
    "axes.flatten()[-2].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=3)\n",
    "plt.savefig('standardize_'+cn+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12243c79",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e17c3",
   "metadata": {},
   "source": [
    "## Calculate Quadratic Features\n",
    "\n",
    "Output:\n",
    "* x csv file with interactions, npy file with all column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['age', 'gender', 'diagnosis', 'mood',\n",
    "      'current expected reward', 'current gambling range', 'current indicator',\n",
    "      'past rewards', 'past reward prediction error']\n",
    "nf = len(names)\n",
    "x = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "x2 = np.zeros(shape=(x.shape[0], int(1 + nf + nf * (nf-1) / 2)))\n",
    "\n",
    "# align subject ID column\n",
    "x2[:,range(nf+1)] = x[:,range(nf+1)]\n",
    "\n",
    "counter = nf+1\n",
    "for i in range(nf):\n",
    "    for j in range(i+1, nf):\n",
    "        v = x[:, i+1] * x[:, j+1]\n",
    "        x2[:,counter] = v\n",
    "        names.append(names[i] + ' * ' + names[j])\n",
    "        counter = counter + 1\n",
    "np.savetxt('x_random_normed_standardized_Actual_EV0.5_interactions.csv', x2, delimiter=\",\")\n",
    "np.save('x_random_normed_standardized_Actual_EV0.5_interactions_names.npy', names)\n",
    "\n",
    "with open('x_stdz_interactions.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbbf58",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b0279",
   "metadata": {},
   "source": [
    "## General Function to Standardize Features\n",
    "Start: \n",
    "* any x csv file\n",
    "\n",
    "Data Analysis: \n",
    "* standardize.py to z-score all the features\n",
    "\n",
    "Output: \n",
    "* x csv file with interactions, npy file with all column names, all features are standardized. This is useful for stability selection which has stability results/ rankings that are very sensitive to standard deviation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d932f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(filename):\n",
    "    '''\n",
    "    Given the name of a csv file, generate a new csv with each column standardized (mean of 0, std of 1)\n",
    "    Input: filename is a string corresponding to filename.csv,\n",
    "    Output: filename_stdz.csv with standardized columns, the 0th column is preserved\n",
    "    '''\n",
    "    temp = np.genfromtxt(filename + '.csv', delimiter=\",\")\n",
    "    ft = np.empty(shape=[temp.shape[0]-1,temp.shape[1]])\n",
    "    colnames = np.empty(shape=[temp.shape[1]])\n",
    "    \n",
    "    with open(filename+'.csv', 'rt') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        counter = -1\n",
    "        for row in reader:\n",
    "            if counter >= 0:\n",
    "                ft[counter, :] = row\n",
    "            else:\n",
    "                colnames = row\n",
    "            counter = counter + 1\n",
    "\n",
    "    csvfile.close()\n",
    "    means = []\n",
    "    stds = []\n",
    "    for col in range(1,temp.shape[1]):\n",
    "\n",
    "        m = ft[:,col].mean(axis=0)\n",
    "        means.append(m)\n",
    "        s = ft[:,col].std(axis=0)\n",
    "        stds.append(s)\n",
    "        if col == 1:\n",
    "            pass\n",
    "        if s == 0.0 :\n",
    "            ft[:,col] = (ft[:,col] - m)\n",
    "        else:\n",
    "            ft[:,col] = (ft[:,col] - m) / s\n",
    "\n",
    "    with open(filename + '_stdz.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(colnames)\n",
    "        writer.writerows(ft)\n",
    "\n",
    "    writeFile.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'x_random_normed_standardized_Actual_EV0.5_interactions'\n",
    "standardize(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932af3d",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a971b",
   "metadata": {},
   "source": [
    "## Stability Plots and Regularization Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e650619",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7b28d",
   "metadata": {},
   "source": [
    "### define helper functions <a class=\"anchor\" id=\"define-helper-functions-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose Regularization and Stability Plot Settings\n",
    "\n",
    "start = 0 # control the first np.random.seed(start)\n",
    "T = 100 # number of bootstraps = T\n",
    "LAMBDAS = list(map(lambda x : math.exp(float(x) / 2) , range(-8,20,1)))\n",
    "epsilon=0.0\n",
    "tolerance = 0.0005\n",
    "nonzero = lambda x : not (x < tolerance and x > - tolerance)\n",
    "\n",
    "# Write Helper Functions\n",
    "\n",
    "def fitmodel(xs, ys, LAMBDA):\n",
    "    '''\n",
    "    fit a linear regression on xs and ys\n",
    "    with regularization parameter lambda\n",
    "    and return a vector of feature weights\n",
    "    '''\n",
    "    clf = LogisticRegression(penalty = 'l1', C = 1/LAMBDA, solver='liblinear')\n",
    "    clf.fit(xs, ys.ravel())\n",
    "    w = clf.coef_.ravel()\n",
    "    return w\n",
    "\n",
    "def stabgraph_all(ORIGsids, x, y):\n",
    "    '''\n",
    "    generate a stability+ plot and a stability- plot\n",
    "    for fitting a logistic regression of y using x\n",
    "    subject to l1 regularizations in LAMBDAS\n",
    "    based on 100 resamplings of n/2 subjects in ORIGsids\n",
    "    '''\n",
    "    \n",
    "    # Generate Plot Data\n",
    "    \n",
    "    nf = x.shape[1]-1\n",
    "    pos_sums = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    neg_sums = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    \n",
    "    for counter, LAMBDA in enumerate(LAMBDAS):\n",
    "        all_signs = np.zeros(shape=(T, nf))\n",
    "        for t in range(start, start + T):\n",
    "            ns = int(len(ORIGsids) / 2)\n",
    "            np.random.seed(t) ; sids = np.random.choice(ORIGsids, ns, replace=False)\n",
    "            XY = getXY(sids, x, y)\n",
    "            xs = XY[0]\n",
    "            ys = XY[1]\n",
    "            w = fitmodel(xs, ys, LAMBDA)\n",
    "            all_signs[t - start, :] = list(map(np.sign, w))\n",
    "        pos_sum = np.sum(all_signs == 1, 0)\n",
    "        neg_sum = np.sum(all_signs == -1, 0)\n",
    "        pos_sums[counter, :] = list(map(lambda x : float(x) / float(T), pos_sum))\n",
    "        neg_sums[counter, :] = list(map(lambda x : float(x) / float(T), neg_sum))\n",
    "    \n",
    "    \n",
    "    # Create Plot\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 8), sharex=True)\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    for f in range(len(names)):\n",
    "        axes[0].plot(LAMBDAS, pos_sums[:,f])\n",
    "    axes[0].set(title=\"Stability+\",xlabel=\"λ\", ylabel=\"Percentage\")\n",
    "    axes[0].axhline(y=0.5, color='k', linestyle='-')\n",
    "        \n",
    "    for f in range(len(names)):\n",
    "        axes[1].plot(LAMBDAS, neg_sums[:,f])\n",
    "    axes[1].set(title=\"Stability-\", xlabel=\"λ\", ylabel=\"Percentage\")\n",
    "    axes[1].legend(names, loc='upper right')\n",
    "    axes[1].axhline(y=0.5, color='k', linestyle='-')\n",
    "    \n",
    "    \n",
    "    # Save and Display Plot\n",
    "    \n",
    "    plt.savefig(\"stabgplotsave.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def reggraph_all(ORIGsids, x, y):\n",
    "    '''\n",
    "    generate a regularization plot\n",
    "    for fitting a logistic regression of y using x\n",
    "    subject to l1 regularizations in LAMBDAS\n",
    "    based on all subjects in ORIGsids\n",
    "    '''\n",
    "    \n",
    "    # Generate Plot Data\n",
    "    \n",
    "    XY = getXY(ORIGsids, x, y)\n",
    "    xs = XY[0]\n",
    "    ys = XY[1]\n",
    "    nf = x.shape[1]-1\n",
    "    weights = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    for counter, LAMBDA in enumerate(LAMBDAS):\n",
    "        w = fitmodel(xs, ys, LAMBDA)\n",
    "        weights[counter, :] = w\n",
    "\n",
    "    # Create Plot\n",
    "    plt.figure(figsize=(9, 9))\n",
    "        \n",
    "    for f in range(len(names)):\n",
    "        plt.plot(LAMBDAS, weights[:,f])\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.legend(names, loc='upper right')\n",
    "    plt.xlabel('λ')\n",
    "    plt.ylabel('Beta Weight')\n",
    "    plt.title(\"Regularization Plot\")\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    \n",
    "    # Save and Display Plot\n",
    "    \n",
    "    plt.savefig(\"regplotsave.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d86880",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716041a",
   "metadata": {},
   "source": [
    "### create plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f47ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and Y Data\n",
    "\n",
    "x = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "y = np.genfromtxt('y_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "\n",
    "# Load Subject Data\n",
    "\n",
    "ORIGsids = np.array(np.genfromtxt('gps_random.csv', delimiter=\",\", usecols=np.arange(0,2))[1:,1]) # subject ids\n",
    "\n",
    "# Generate Plots\n",
    "\n",
    "stabgraph_all(ORIGsids, x, y)\n",
    "\n",
    "reggraph_all(ORIGsids, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e6d4c",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67b35f",
   "metadata": {},
   "source": [
    "## Average Stability Calculations\n",
    "\n",
    "Start: \n",
    "* npy files for the stability values at different levels of L1 regularization\n",
    "\n",
    "Output: \n",
    "* csv file with the names and average stabilities of all features (averaged across lambda values), npy file with ranking of indices of the top most stable (on average) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf189a",
   "metadata": {},
   "source": [
    "### save stability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e27aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = np.load(\"lr_sids_ratios_for_alllambdas_of_random_random_normed_standardized_Actual_EV0.5.npy\")\n",
    "LAMBDAS = np.load(\"lr_sids_lambdas_for_alllambdas_of_random_random_normed_standardized_Actual_EV0.5.npy\")\n",
    "\n",
    "\n",
    "i = getcolumnnames(\"x_random_random_normed_standardized_Actual_EV0.5.csv\")\n",
    "names = i[1:]\n",
    "\n",
    "def ca(x,y):\n",
    "    '''\n",
    "    Given x, a list of increments along the x axis,\n",
    "    and y, a list of heights along each increment,\n",
    "    compute the total area with left tapezoidal rules\n",
    "    '''\n",
    "    \n",
    "    area = 0.0\n",
    "    for i in range(len(x) - 1):\n",
    "        dx = x[i+1] - x[i]\n",
    "        dy = y[i+1] - y[i]\n",
    "        area = area + dx * y[i+1] + dx * dy / 2\n",
    "    maxarea = max(y) * (x[-1] - x[0]) \n",
    "    area = area\n",
    "    return area\n",
    "\n",
    "areas = np.zeros(shape=len(names))\n",
    "maxes = np.zeros(shape=len(names))\n",
    "with open('areas_random_random_normed_standardized_Actual_EV0.5.csv', 'a') as file:\n",
    "    filewriter = csv.writer(file, delimiter=',')\n",
    "    filewriter.writerow(['index', 'name','area','max'])\n",
    "    counter = 0\n",
    "    for f in range(len(names)):\n",
    "        fname = names[f]\n",
    "        fratios = ratios[:,f]\n",
    "        fa = ca(LAMBDAS, fratios)\n",
    "        fm = np.max(fratios)\n",
    "        filewriter.writerow([counter, fname, fa, fm])\n",
    "        areas[counter] = fa\n",
    "        maxes[counter] = fm\n",
    "        counter = counter + 1\n",
    "    np.save(\"areas_areas_random_random_normed_standardized_Actual_EV0.5.npy\", areas)\n",
    "    np.save(\"areas_maxes_random_random_normed_standardized_Actual_EV0.5.npy\", maxes)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f572",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7555e0",
   "metadata": {},
   "source": [
    "### save indicies of stability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = np.load(\"areas_areas_random_random_normed_standardized_Actual_EV0.5.npy\")\n",
    "maxes = np.load(\"areas_maxes_random_random_normed_standardized_Actual_EV0.5.npy\")\n",
    "# sort indices based on largest to smallest area\n",
    "ind_a = np.flip(np.argsort(areas))\n",
    "# sort indices based on largest to smallest max stability\n",
    "ind_m = np.flip(np.argsort(maxes))\n",
    "\n",
    "np.save(\"areas_areas_indices_random_random_normed_standardized_Actual_EV0.5.npy\", ind_a)\n",
    "np.save(\"areas_maxes_indices_random_random_normed_standardized_Actual_EV0.5.npy\", ind_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfb1da",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca561cc",
   "metadata": {},
   "source": [
    "## Picking Number of Most Stable Features\n",
    "\n",
    "Start: \n",
    "* ranking of indices of most stable features\n",
    "\n",
    "Output: \n",
    "* Graph of LOOCV accuracy of models trained on 0 features, 1 top most stable feature, top 2 most stable features, top 3 most stable features, ..., all the way to 45 features (9 main features + 36 interactions). \n",
    "* We can pick a sufficient number of features by seeing which # of features achieves the global maximum LOOCV accuracy (by leaving out one subject instead of one trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081234d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_acc(sids, x, y):\n",
    "    '''\n",
    "    Find the leave one subject out cross validation accuracy\n",
    "    of fitting a logistic regression on y using x\n",
    "    with an array of unique subject IDs specified by sids\n",
    "    '''\n",
    "    obsvs = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    print (preds)\n",
    "    if len(x) == 0:\n",
    "        for f in range(n):\n",
    "            test_y = subjY(sids[f], y)\n",
    "            train_y = notsubjY(sids[f], y)\n",
    "            ym = np.mean(train_y )\n",
    "            yb = ym > 0.5\n",
    "            test_p = [yb for _i in range(len(test_y))]\n",
    "            test_prob = [ym for _i in range(len(test_y))]\n",
    "            preds.extend(test_p)\n",
    "            obsvs.extend(test_y)\n",
    "            probs.extend(test_prob)\n",
    "    else:\n",
    "        for f in range(n):\n",
    "            if sids[f] > 10:\n",
    "                [test_x, test_y] = subjXY(sids[f], x, y)\n",
    "                [train_x, train_y] = notsubjXY(sids[f], x, y)\n",
    "                if x.shape[1] == 2:\n",
    "                    test_x = test_x.reshape(len(test_x),1)\n",
    "                    train_x = train_x.reshape(len(train_x),1)\n",
    "                mf = LogisticRegression(solver='liblinear') \n",
    "                mf.fit(train_x, train_y)\n",
    "                test_p = mf.predict(test_x)\n",
    "                temp = mf.predict_proba(test_x)\n",
    "                test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "                preds.extend(test_p)\n",
    "                obsvs.extend(test_y)\n",
    "                probs.extend(test_prob)\n",
    "    predsbool = np.reshape(np.array(preds).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(obsvs).astype(bool), (-1))\n",
    "    probs = np.reshape(np.array(probs), (-1))\n",
    "    bacc = balancedacc(obsvsbool, probs, 0.5)\n",
    "    print (\"balanced accuracy is : \", bacc)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, probs)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    return [bacc, auc]\n",
    "\n",
    "\n",
    "# Load Data\n",
    "\n",
    "x = np.genfromtxt(\"x_random_normed_standardized_Actual_EV0.5.csv\", delimiter=\",\")\n",
    "i = getcolumnnames(\"x_random_normed_standardized_Actual_EV0.5.csv\")\n",
    "names = i[1:]\n",
    "\n",
    "y = np.genfromtxt(\"y_random_normed_standardized_Actual_EV0.5.csv\", delimiter=\",\")\n",
    "ORIGsids = np.array(np.genfromtxt('gps_random.csv', delimiter=\",\", usecols=np.arange(0,2))[1:,1])\n",
    "n = len(ORIGsids)\n",
    "\n",
    "ind_a = np.load(\"areas_areas_indices_random_random_normed_standardized_Actual_EV0.5.npy\") # 0 indexed\n",
    "\n",
    "featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "\n",
    "# Specify number of bootstraps\n",
    "\n",
    "T = 100\n",
    "\n",
    "# Create Result Data Arrays\n",
    "\n",
    "baccs = np.zeros(shape=(T, len(featrank) + 1))\n",
    "aucs = np.zeros(shape=(T, len(featrank) + 1))\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        sids = ORIGsids\n",
    "    else:\n",
    "        np.random.seed(t) ; sids = np.random.choice(ORIGsids, n, replace=True)\n",
    "    [mbacc,auc] = loocv_acc(sids, [], y)\n",
    "    baccs[t,0] = mbacc\n",
    "    aucs[t,0] = auc\n",
    "    for m in range(len(featrank)):\n",
    "        ml = featrank[0: m+1]\n",
    "        xnew = subsetx(x,ml)\n",
    "        [mbacc, mauc] = loocv_acc(sids, xnew, y)\n",
    "        baccs[t, m+1] = mbacc\n",
    "        aucs[t, m+1] = mauc\n",
    "\n",
    "# Plot Figures\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Max Balanced Accuracy in LOOCV\")\n",
    "x = [\"Baseline\"]\n",
    "for name in names:\n",
    "    x.append(\"+ \" + name)\n",
    "\n",
    "plt.plot(x, baccs.mean(axis=0))\n",
    "for i, txt in enumerate(baccs.mean(axis=0)):\n",
    "    plt.annotate(round(txt,2), (x[i], baccs.mean(axis=0)[i]))\n",
    "plt.title(\"Mean Balanced Accuracy for T = \" + str(T) + \" bootstrap resamplings for \" + ET)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"AUC in LOOCV\")\n",
    "plt.plot(x,aucs.mean(axis=0))\n",
    "for i, txt in enumerate(aucs.mean(axis=0)):\n",
    "    plt.annotate(round(txt,2), (x[i], aucs.mean(axis=0)[i]))\n",
    "plt.title(\"Means and STDs for T = \" + str(T) + \" bootstrap resamplings for \" + ET)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f780185",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c8b4c",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12fbaf",
   "metadata": {},
   "source": [
    "## Estimate LR LOOCV accuracy\n",
    "\n",
    "Output: \n",
    "* csv file of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runmodel(gps, NFEATS, x, y, SEED, LAMBDA, ind_a, names):\n",
    "    '''\n",
    "    Generate and evaluate the predictions of a model with the following inputs:\n",
    "\n",
    "    gps \n",
    "        the name of the per subject data file\n",
    "    NFEATS \n",
    "        the number of features of x the model should use\n",
    "    x \n",
    "        an array of features, with the 0th column as subject ids\n",
    "    y \n",
    "        an array of targets, with the 0th column as subject ids\n",
    "    SEED\n",
    "        the np.random setting we should use to when we're bootstrapping. Use SEED=0 to train on the original sample.\n",
    "    LAMBDA \n",
    "        the parameter used to control the l1 penalty on the solver\n",
    "    ind_a \n",
    "        an array of the indicies of the most stable features in x\n",
    "    names \n",
    "        an array of the names of the most stable features in x\n",
    "    '''\n",
    "    \n",
    "    featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "    ml = featrank[0: NFEATS]\n",
    "    xnew = subsetx(x,ml)\n",
    "\n",
    "    ORIGsids = np.array(np.genfromtxt(gps, delimiter=\",\",usecols=np.arange(0,2))[1:,1]) # subject ids\n",
    "    n = len(ORIGsids)\n",
    "    if SEED == 0:\n",
    "        sids = ORIGsids\n",
    "    else:\n",
    "        np.random.seed(SEED) ; sids = np.random.choice(ORIGsids, n)\n",
    "    obsvs = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    subjs = []\n",
    "    for fold in range(n):\n",
    "        [test_x, test_y] = subjXY(sids[fold], xnew, y)\n",
    "        [train_x, train_y] = notsubjXY(sids[fold], xnew, y)\n",
    "        mf = LogisticRegression(solver='liblinear', penalty = 'l1', C = 1/LAMBDA)\n",
    "        mf.fit(train_x, train_y)\n",
    "        test_pred = mf.predict(test_x)\n",
    "        temp = mf.predict_proba(test_x)\n",
    "        test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "        preds.append(test_pred)\n",
    "        obsvs.append(test_y)\n",
    "        probs.append(test_prob)\n",
    "        subjs.append([sids[fold] for i in range(len(test_pred))])\n",
    "    subjs = np.array(subjs).reshape(-1)\n",
    "    probs = np.array(probs).reshape(-1)\n",
    "    predsbool = np.reshape(np.array(preds).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(obsvs).astype(bool), (-1))\n",
    "    bacc3 = balancedacc(obsvsbool, probs, 0.3)\n",
    "    bacc5 = balancedacc(obsvsbool, probs, 0.5)\n",
    "    bacc7 = balancedacc(obsvsbool, probs, 0.7)\n",
    "    mbacc = maxbacc(obsvsbool, probs)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, probs)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    sauc = subjauc(subjs, obsvsbool, probs)\n",
    "\n",
    "    ll = sklearn.metrics.log_loss(np.squeeze(obsvsbool), np.squeeze(probs)) / float(len(probs))\n",
    "    return [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll]\n",
    "\n",
    "# Specify Model Regularization Levels and Seeds\n",
    "\n",
    "LAMBDAS = [0.1, 1.0, 10.0]\n",
    "nSEEDS = 3\n",
    "\n",
    "# Load Data\n",
    "\n",
    "x = np.genfromtxt(\"x_random_normed_standardized_Actual_EV0.5_interactions.csv\", delimiter=\",\")\n",
    "y = np.genfromtxt(\"y_random_normed_standardized_Actual_EV0.5.csv\", delimiter=\",\")\n",
    "\n",
    "gps = 'gps_random.csv'\n",
    "\n",
    "ind_a = np.load(\"areas_areas_indices_random_random_normed_standardized_Actual_EV0.5.npy\")\n",
    "names = np.load('x_random_normed_standardized_Actual_EV0.5_interactions_names.npy')\n",
    "\n",
    "NFEATSS = len(ind_a)\n",
    "\n",
    "# Run Models and Transcribe Results\n",
    "\n",
    "with open('lr_results.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    results = []\n",
    "    for SEED in range(nSEEDS):\n",
    "        for LAMBDA in LAMBDAS:\n",
    "            [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll] = runmodel(gps, NFEATS, x, y, SEED, LAMBDA, ind_a, names)\n",
    "            writer.writerow([bacc5, str(LAMBDA), str(SEED)])\n",
    "            results.append([bacc5, str(LAMBDA), str(SEED)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18573b0",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a91d9",
   "metadata": {},
   "source": [
    "## Estimate Weights of a Model Trained on Entire Data Set\n",
    "\n",
    "Output: \n",
    "* npy files with weights of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4459f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDAs = [0.1, 1.0, 10.0]\n",
    "\n",
    "x = np.genfromtxt(\"x_random_normed_standardized_Actual_EV0.5_interactions.csv\", delimiter=\",\")\n",
    "y = np.genfromtxt(\"y_random_normed_standardized_Actual_EV0.5.csv\", delimiter=\",\")\n",
    "\n",
    "\n",
    "for LAMBDA in LAMBDAs:\n",
    "    xs = x[1:,1:]\n",
    "    ys = y[1:,1:]\n",
    "    print (xs.shape)\n",
    "    print (ys.shape)\n",
    "\n",
    "    clf = LogisticRegression(penalty = 'l1', C = 1/LAMBDA, solver='liblinear')\n",
    "    clf.fit(xs, ys.ravel())\n",
    "    w = clf.coef_.ravel()\n",
    "\n",
    "    np.save(ET +\"_lambda\" + str(LAMBDA) +  \"_weights\", w)\n",
    "    print (ET +\"_lambda\" + str(LAMBDA) +  \"_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69da05",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7402b3",
   "metadata": {},
   "source": [
    "## Visualize Weights of a Model Trained on Entire Data Set\n",
    "\n",
    "Output: \n",
    "* csv file of model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnames = np.load('x_random_normed_standardized_Actual_EV0.5_interactions_names.npy')\n",
    "w1 = np.load(\"random_lambda1.0_weights.npy\")\n",
    "\n",
    "wm = [np.absolute(i) for i in w1]\n",
    "\n",
    "w1 = [x for _,x in sorted(zip(wm,w1), reverse=True)]\n",
    "wnames = [x for _,x in sorted(zip(wm,wnames), reverse=True)]\n",
    "\n",
    "with open('model_weights_random.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    for i in range(len(wnames)):\n",
    "        writer.writerow([wnames[i], w1[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e2736",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d25c",
   "metadata": {},
   "source": [
    "## Compare Weights of a Model Trained on Entire Data Set\n",
    "\n",
    "Output: \n",
    "* heatmap with weights of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10.0\n",
    "\n",
    "ET1 = \"random\"\n",
    "ET2 = \"random\"\n",
    "\n",
    "wnames = np.load('x_random_normed_standardized_Actual_EV0.5_interactions_names.npy')\n",
    "w1 = np.load(ET1 +\"_lambda\" + str(LAMBDA) + \"_weights.npy\")\n",
    "w2 = np.load(ET2 +\"_lambda\" + str(LAMBDA) + \"_weights.npy\")\n",
    "\n",
    "wm = [np.maximum(np.absolute(i),np.absolute(j)) for i,j in zip(w1,w2)]\n",
    "\n",
    "w1 = [x for _,x in sorted(zip(wm,w1), reverse=True)]\n",
    "w2 = [x for _,x in sorted(zip(wm,w2), reverse=True)]\n",
    "wnames = [x for _,x in sorted(zip(wm,wnames), reverse=True)]\n",
    "\n",
    "NFEATS = 10\n",
    "\n",
    "with open('names.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    for i in range(NFEATS):\n",
    "        writer.writerow([wnames[i]])\n",
    "\n",
    "\n",
    "matrix = np.transpose(np.array([w1[:NFEATS], w2[:NFEATS]]))\n",
    "\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "ax = sns.heatmap(matrix, center=0, annot=True, fmt=\".1f\",\n",
    "                yticklabels=wnames[:NFEATS], xticklabels=[ET1, ET2], cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78284c3",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a47e34",
   "metadata": {},
   "source": [
    "# Neural Network Modeling and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9647ef7",
   "metadata": {},
   "source": [
    "## Estimate NN LOOCV accuracy\n",
    "\n",
    "Output: \n",
    "* csv file of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Neural Network Specifications\n",
    "\n",
    "UNITS = 5\n",
    "SUFFIX = \"nn\"\n",
    "training_epochs = 3000\n",
    "LAYERS = 2\n",
    "\n",
    "def run(NOISE, adv, LAMBDA, epsilon, REG, SEED):\n",
    "    tf.set_random_seed(SEED)\n",
    "\n",
    "    x = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5_stdz.csv', delimiter=\",\")\n",
    "    y = np.genfromtxt('y_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "\n",
    "    ORIGsids = np.array(np.genfromtxt('gps_random.csv', delimiter=\",\", usecols=((0,1)))[1:,1])\n",
    "    n = len(ORIGsids)\n",
    "\n",
    "    if SEED == 0:\n",
    "        sids = ORIGsids\n",
    "    else:\n",
    "        np.random.seed(SEED) ; sids = np.random.choice(ORIGsids, len(ORIGsids))\n",
    "\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape = [None,x.shape[1]-1])\n",
    "    N = tf.placeholder(tf.float32, shape = [None,x.shape[1]-1])\n",
    "    Ga = tf.placeholder(tf.float32, shape = [None,x.shape[1]-1])\n",
    "    Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "    if REG == \"l2\":\n",
    "        l = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
    "    elif REG == \"l1\":\n",
    "        l = tf.contrib.layers.l1_regularizer(scale=LAMBDA)\n",
    "    else:\n",
    "        print(\"parameter 'REG' not recognized\")\n",
    "    if LAYERS == 2:\n",
    "        H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "        H2 = tf.contrib.layers.fully_connected(H1,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "        L = tf.contrib.layers.fully_connected(H2,1,activation_fn=None,weights_regularizer=l)\n",
    "    elif LAYERS == 1:\n",
    "        H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "        L = tf.contrib.layers.fully_connected(H1,1,activation_fn=None,weights_regularizer=l)\n",
    "    else:\n",
    "        print (\"LAYERS parameter not recognized\")\n",
    "    l_loss = tf.losses.get_regularization_loss()\n",
    "    P = tf.nn.sigmoid(L)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=L)) # cross entropy\n",
    "    if adv == \"sign\":\n",
    "        A = tf.stop_gradient(tf.sign(tf.gradients(cost,X)))\n",
    "    elif adv == \"nosign\":\n",
    "        A = tf.stop_gradient(tf.gradients(cost,X))\n",
    "    else:\n",
    "        print (\"not recognized value of adv\")\n",
    "    XA = tf.stop_gradient(X + N * A)\n",
    "    XGa = tf.stop_gradient(X + N * Ga)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost+l_loss)\n",
    "    G = tf.stop_gradient(tf.gradients(L,X))\n",
    "    H = tf.stop_gradient(tf.reduce_sum(tf.squeeze(tf.hessians(L,X)),axis=2))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    final_test_costs = []\n",
    "    obsvs = []\n",
    "    preds = []\n",
    "    grads = []\n",
    "    for fold in range(n):\n",
    "        [test_x, test_y] = subjXY(sids[fold], x, y)\n",
    "        [train_x, train_y] = notsubjXY(sids[fold], x, y)\n",
    "        test_y = np.reshape(test_y, (-1,1))\n",
    "        train_y = np.reshape(train_y, (-1,1))\n",
    "\n",
    "        o = test_y\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            # try new version from nn_boot_hessians.py\n",
    "            for epoch in range(training_epochs):\n",
    "                n = np.random.uniform(low=0.0, high=epsilon, size=train_x.shape)\n",
    "                xa = np.squeeze(sess.run(XA, feed_dict={X : train_x, Y: 1.0 - train_y, N : n}))\n",
    "                if epoch != training_epochs - 1:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c = sess.run([optimizer, cost], feed_dict={X: np.concatenate([train_x, xa], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = train_x.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : train_x, Ga: ga, N : n}))\n",
    "                        _, c= sess.run([optimizer, cost], feed_dict={X: np.concatenate([train_x, xga], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "                else:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c, g = sess.run([optimizer, cost, G], feed_dict={X: np.concatenate([train_x, xa], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = xs.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : train_x, Ga: ga, N : n}))\n",
    "                        _, c, g = sess.run([optimizer, cost, G], feed_dict={X: np.concatenate([train_x, xga], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "                print(\"Epoch: {} - train cost: {}\".format(epoch, c))\n",
    "            p = sess.run(P, feed_dict={X: test_x})\n",
    "            print (p)\n",
    "\n",
    "            saver.save(sess, \"./model\"+str(fold+1)+\"_\"+SUFFIX+\"_lambda\"+str(LAMBDA)+\".ckpt\") # checkpoint\n",
    "        preds.append(p)\n",
    "        obsvs.append(o)\n",
    "        grads.append(g)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_pred.npy\", preds)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_obsv.npy\", obsvs)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_grads.npy\", grads)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_testcosts.npy\", final_test_costs)\n",
    "\n",
    "\n",
    "    f = lambda x : x > 0.5\n",
    "    predsbool = np.array(map(f, preds))\n",
    "    totaln = len(predsbool) * len(predsbool[0])\n",
    "    preds = np.reshape(np.array(preds), [totaln])\n",
    "    predsbool = np.reshape(predsbool, [totaln])\n",
    "    obsvsbool = np.squeeze(np.array(obsvs).astype(bool))\n",
    "    obsvsbool = np.reshape(obsvsbool, [totaln])\n",
    "\n",
    "    bacc = balancedacc(obsvsbool, preds, 0.5)\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(obsvsbool, predsbool)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, predsbool)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "    return [round(bacc,4), auc]#, ll]\n",
    "\n",
    "# Specify Model Parameters\n",
    "\n",
    "REG = \"l1\"\n",
    "NOISE = \"adversarial\"\n",
    "adv = \"sign\" # can include \"nosign\"\n",
    "LAMBDA = 1e-1\n",
    "epsilon = 0.0\n",
    "nSEEDS = 3\n",
    "\n",
    "# Run and Save Model Results\n",
    "\n",
    "with open('nn_folds_results.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    results = []\n",
    "    for SEED in range(nSEEDS):\n",
    "        print (epsilon, LAMBDA, SEED)\n",
    "        result = run(NOISE, adv, LAMBDA, epsilon, REG, SEED)\n",
    "        result.extend([\"epsilon : \" + str(epsilon), \"lambda : \"+ str(LAMBDA), \"SEED : \" + str(SEED)])\n",
    "        w.writerow(result)\n",
    "        results.append(result)\n",
    "    f.close()\n",
    "\n",
    "with open('nn_folds_results_all.csv', 'w') as f:\n",
    "    fw = csv.writer(f)\n",
    "    fw.writerows(results)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8a5b3",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11b5eb",
   "metadata": {},
   "source": [
    "## Generate Hessian Values\n",
    "Output\n",
    "* npy files with hessian variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a254d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Neural Network Specifications\n",
    "\n",
    "SUFFIX = \"nnboot\"\n",
    "UNITS = 5\n",
    "training_epochs = 3000\n",
    "LAYERS = 2\n",
    "nSEEDS = 1\n",
    "nSMOOTHGRAD = 50\n",
    "\n",
    "\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "def runseeds(NOISE, adv, LAMBDA, epsilon, REG, epsilonSG):\n",
    "    hesss = np.zeros(shape=[nSEEDS*nSUBJ*nTRIALS,nFEATS,nFEATS])\n",
    "    for SEED in range(1,nSEEDS + 1):\n",
    "\n",
    "        tf.set_random_seed(SEED)\n",
    "\n",
    "        if SEED == 0:\n",
    "            sids = ORIGsids\n",
    "        else:\n",
    "            np.random.seed(SEED) ; sids = np.random.choice(ORIGsids, len(ORIGsids))\n",
    "\n",
    "        xs = np.zeros(shape=[x.shape[0]-1, x.shape[1]-1])\n",
    "        ys = np.zeros(shape=[y.shape[0]-1, 1])\n",
    "        xbool = np.zeros(shape=y.shape[0]-1)\n",
    "\n",
    "        counter = 0\n",
    "        for i in range(len(sids)):\n",
    "            [sx, sy, size] = subjXY(sids[i], x, y)\n",
    "            xs[range(counter, counter+size), :] = sx\n",
    "            ys[range(counter, counter+size)] = sy\n",
    "            counter = counter + size\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape = [None,xs.shape[1]])\n",
    "        N = tf.placeholder(tf.float32, shape = [None,xs.shape[1]])\n",
    "        Ga = tf.placeholder(tf.float32, shape = [None,xs.shape[1]])\n",
    "        Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "        if REG == \"l2\":\n",
    "            l = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
    "        elif REG == \"l1\":\n",
    "            l = tf.contrib.layers.l1_regularizer(scale=LAMBDA)\n",
    "        else:\n",
    "            print(\"parameter 'REG' not recognized\")\n",
    "        if LAYERS == 2:\n",
    "            H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "            H2 = tf.contrib.layers.fully_connected(H1,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "            L = tf.contrib.layers.fully_connected(H2,1,activation_fn=None,weights_regularizer=l)\n",
    "        elif LAYERS == 1:\n",
    "            H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "            L = tf.contrib.layers.fully_connected(H1,1,activation_fn=None,weights_regularizer=l)\n",
    "        else:\n",
    "            print (\"LAYERS parameter not recognized\")\n",
    "        l_loss = tf.losses.get_regularization_loss()\n",
    "        P = tf.nn.sigmoid(L)\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=L)) # cross entropy\n",
    "        if adv == \"sign\":\n",
    "            A = tf.stop_gradient(tf.sign(tf.gradients(cost,X)))\n",
    "        elif adv == \"nosign\":\n",
    "            A = tf.stop_gradient(tf.gradients(cost,X))\n",
    "        else:\n",
    "            print (\"not recognized value of adv\")\n",
    "        XA = tf.stop_gradient(X + N * A)\n",
    "        XGa = tf.stop_gradient(X + N * Ga)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost+l_loss)\n",
    "        G = tf.stop_gradient(tf.gradients(L,X))\n",
    "        H = tf.stop_gradient(tf.reduce_sum(tf.squeeze(tf.hessians(L,X)),axis=2))\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            for epoch in range(training_epochs):\n",
    "                n = np.random.uniform(low=0.0, high=epsilon, size=xs.shape)\n",
    "                xa = np.squeeze(sess.run(XA, feed_dict={X : xs, Y: 1.0 - ys, N : n}))\n",
    "                if epoch != training_epochs - 1:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c = sess.run([optimizer, cost], feed_dict={X: np.concatenate([xs, xa], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = xs.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : xs, Ga: ga, N : n}))\n",
    "                        _, c= sess.run([optimizer, cost], feed_dict={X: np.concatenate([xs, xga], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "                else:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c, g, h = sess.run([optimizer, cost, G, H], feed_dict={X: np.concatenate([xs, xa], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[0,:,:]))\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[211,:,:]))\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[562,:,:]))\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[736,:,:]))\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = xs.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : xs, Ga: ga, N : n}))\n",
    "                        _, c, g, h = sess.run([optimizer, cost, G, H], feed_dict={X: np.concatenate([xs, xga], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "            _save_path = saver.save(sess, \"./model_temp\" +\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+ \".ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "            saver.restore(sess, \"./model_temp\" +\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+ \".ckpt\")\n",
    "\n",
    "            h = sess.run(H, feed_dict={X: xs, Y: ys})\n",
    "\n",
    "            # regular hessian\n",
    "            h = np.squeeze(h)\n",
    "            print (\"hess non-zero entries : \", np.not_equal(h,0.0).sum())\n",
    "            print (\"hess shape is : \", h.shape)\n",
    "            hesss[(SEED-1)*nSUBJ*nTRIALS: (SEED)*nSUBJ*nTRIALS,:,:] = h[range(nSUBJ*nTRIALS), :, :]\n",
    "            if NOISE == \"adversarial\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_hesss.npy\", np.squeeze(h))\n",
    "            elif NOISE == \"gaussian\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_units\"+str(UNITS)+\"_hesss.npy\", np.squeeze(h))\n",
    "            else:\n",
    "                print (\"encountered error\")\n",
    "\n",
    "            # smooth grad hessians\n",
    "\n",
    "            # smooth grad\n",
    "            hess_sums = np.zeros(shape = np.squeeze(h).shape)\n",
    "            for i in range(nSMOOTHGRAD):\n",
    "                np.random.seed(i); n = np.random.uniform(low=0.0, high=epsilonSG, size=xs.shape)\n",
    "                np.random.seed(i); ga = np.random.normal(size = xs.shape)\n",
    "                xga = np.squeeze(sess.run(XGa, feed_dict={X : xs, Ga: ga, N : n}))\n",
    "                h = sess.run(H, feed_dict={X: xga, Y: ys})\n",
    "                hess_sums = hess_sums + np.squeeze(h)\n",
    "\n",
    "            if NOISE == \"adversarial\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_points\"+ str(nSMOOTHGRAD) + \"_hesss_sums.npy\", hess_sums)\n",
    "            elif NOISE == \"gaussian\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_units\"+str(UNITS)+\"_points\"+ str(nSMOOTHGRAD) +\"_hesss_sums.npy\", hess_sums)\n",
    "            else:\n",
    "                print (\"encountered error\")\n",
    "\n",
    "    if NOISE == \"adversarial\":\n",
    "        np.save(SUFFIX+\"_nSEED\"+str(nSEEDS)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_hesss.npy\", hesss)\n",
    "    elif NOISE == \"gaussian\":\n",
    "        np.save(SUFFIX+\"_nSEED\"+str(nSEEDS)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_units\"+str(UNITS)+\"_hesss.npy\", hesss)\n",
    "    else:\n",
    "        print (\"encountered error\")\n",
    "        \n",
    "    return\n",
    "\n",
    "# Specify Model Parameters\n",
    "        \n",
    "REGs = [\"l1\"]\n",
    "noises = [\"adversarial\"]\n",
    "advs = [\"sign\"] \n",
    "LAMBDAs = [1e-4]\n",
    "epsilons = [1e-1]\n",
    "epsilonSGs = [0.2] \n",
    "        \n",
    "# Load Data\n",
    "\n",
    "x = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5_stdz.csv', delimiter=\",\")\n",
    "y = np.genfromtxt('y_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "\n",
    "xtest = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5_stdz.csv', delimiter=\",\", names=True)\n",
    "xcolnames = xtest.dtype.names\n",
    "fnames = xcolnames[1:]\n",
    "nFEATS = len(fnames)\n",
    "np.save(\"featnames\", fnames)\n",
    "\n",
    "ORIGsids = np.array(np.genfromtxt('gps_random.csv', delimiter=\",\", usecols=((0,1)))[1:,1])\n",
    "\n",
    "nSUBJ =  len(ORIGsids)\n",
    "nOBSV = y.shape[0] - 1\n",
    "nTRIALS = nOBSV / nSUBJ\n",
    "\n",
    "# Run Models\n",
    "        \n",
    "for NOISE in noises:\n",
    "    for REG in REGs:\n",
    "        for adv in advs:\n",
    "            for LAMBDA in LAMBDAs:\n",
    "                for epsilon in epsilons:\n",
    "                    for epsilonSG in epsilonSGs:\n",
    "                        print NOISE, adv, LAMBDA, epsilon, epsilonSG\n",
    "                        runseeds(NOISE, adv, LAMBDA, epsilon, REG, epsilonSG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f824a",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff293aa",
   "metadata": {},
   "source": [
    "## Visualize Hessian Matrices\n",
    "\n",
    "Output: \n",
    "* heatmaps of prevalence, positive ratios, negative ratios and average hessian values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subprocess_cmd(command):\n",
    "    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "def posrate(l):\n",
    "    '''\n",
    "    return the ratio of positives in list l\n",
    "    '''\n",
    "    b = len(l)\n",
    "    a = 0\n",
    "    for e in l:\n",
    "        if e > 0.0:\n",
    "            a = a + 1\n",
    "    return float(a) / float(b)\n",
    "\n",
    "def negrate(l):\n",
    "    '''\n",
    "    return the ratio of negatives in list l\n",
    "    '''\n",
    "    b = len(l)\n",
    "    a = 0\n",
    "    for e in l:\n",
    "        if e < 0.0:\n",
    "            a = a + 1\n",
    "    return float(a) / float(b)\n",
    "\n",
    "def visSG(NOISE, adv, LAMBDA, epsilon, REG, SEED):\n",
    "    # smooth grads\n",
    "\n",
    "    fn = SUFFIX+\"_SEED\"+str(SEED)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_points\"+ str(nSMOOTHGRAD) + \"_hesss_sums.npy\"\n",
    "    hessssums = np.load(fn)\n",
    "    hesss = hessssums / nSMOOTHGRAD\n",
    "    print (hesss.shape)\n",
    "    for ind in range(hesss.shape[0]):\n",
    "        hi = hesss[ind,:,:]\n",
    "\n",
    "    means = hesss.mean(axis=0)\n",
    "    stds = hesss.std(axis=0)\n",
    "\n",
    "    poss = np.zeros(shape = means.shape)\n",
    "    negs = np.zeros(shape = means.shape)\n",
    "\n",
    "    for i in range(nFEATS):\n",
    "        for j in range(nFEATS):\n",
    "            v = hesss[:,i,j]\n",
    "            poss[i,j] = posrate(v)\n",
    "            negs[i,j] = negrate(v)\n",
    "\n",
    "    print (\"are the poss symmetric? : \", check_symmetric(poss))\n",
    "    print (\"are the negs symmetric? : \", check_symmetric(negs))\n",
    "\n",
    "    with open('hessian_smoothgrad_results.csv', 'w') as f:\n",
    "        fw = csv.writer(f)\n",
    "        fw.writerow(['means'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(means)\n",
    "        fw.writerow(['stds'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(stds)\n",
    "        fw.writerow(['poss'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(poss)\n",
    "        fw.writerow(['negs'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(negs)\n",
    "        f.close()\n",
    "\n",
    "    return [means, stds, poss, negs]\n",
    "\n",
    "def visREG(NOISE, adv, LAMBDA, epsilon, REG, SEED):\n",
    "    fn = SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_hesss\"\n",
    "    hesss = np.load(fn+\".npy\")\n",
    "\n",
    "    means = hesss.mean(axis=0)\n",
    "    stds = hesss.std(axis=0)\n",
    "\n",
    "    poss = np.zeros(shape = means.shape)\n",
    "    negs = np.zeros(shape = means.shape)\n",
    "\n",
    "    for i in range(nFEATS):\n",
    "        for j in range(nFEATS):\n",
    "            poss[i,j] = posrate(hesss[i,j])\n",
    "            negs[i,j] = negrate(hesss[i,j])\n",
    "\n",
    "\n",
    "    with open('hessian_results.csv', 'w') as f:\n",
    "        fw = csv.writer(f)\n",
    "        fw.writerow(['means'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(means)\n",
    "        fw.writerow(['stds'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(stds)\n",
    "        fw.writerow(['poss'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(poss)\n",
    "        fw.writerow(['negs'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(negs)\n",
    "        f.close()\n",
    "        f.close()\n",
    "\n",
    "    return [means, stds, poss, negs]\n",
    "\n",
    "for NOISE in noises:\n",
    "    for REG in REGs:\n",
    "        for adv in advs:\n",
    "            for LAMBDA in LAMBDAs:\n",
    "                for epsilon in epsilons:\n",
    "                    for epsilonSG in epsilonSGs:\n",
    "                        # positive ratio\n",
    "                        psgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        # negative ratio\n",
    "                        nsgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        # positive ratio minus negative ratio in smooth grads\n",
    "                        pmnsgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        # means\n",
    "                        msgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        for SEED in range(1,nSEEDS+1):\n",
    "                            print (NOISE, adv, LAMBDA, epsilon, SEED)\n",
    "                            msg, ssg, psg, nsg = visSG(NOISE, adv, LAMBDA, epsilon, REG, SEED)\n",
    "                            print (\"is psg symmetric? : \", check_symmetric(psg))\n",
    "                            print (\"is nsg symmetric? : \", check_symmetric(nsg))\n",
    "                            psgs[SEED - 1, :, :] = psg\n",
    "                            nsgs[SEED - 1, :, :] = nsg\n",
    "                            pmnsgs[SEED - 1, :, :] = psg - nsg\n",
    "                            msgs[SEED - 1, :, :] = msg\n",
    "\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(msgs.mean(axis=0), center=0, annot=True, fmt=\".4f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad average Hessian value across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(pmnsgs.mean(axis=0), center=0, annot=True, fmt=\".4f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad expected prevalence across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(psgs.mean(axis=0), center = 0.5, annot=True, fmt=\".3f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad expected positive ratio across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(nsgs.mean(axis=0), center = 0.5, annot=True, fmt=\".3f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad expected negative ratio across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6f122",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ef808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496b28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119498ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
