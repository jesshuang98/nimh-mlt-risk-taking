{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a23b447",
   "metadata": {},
   "source": [
    "# Context\n",
    "* Scientific Goal: Understand if and how emotional valence affects risk-taking\n",
    "* Data Set: 35 subjects participating in a 90 trial mood manipulation task, divided into a 20 subject train and 15 subject test set\n",
    "* Statistical Goal: Create a statistical test to detect if mood belongs in the Markov blanket of risk-taking behavior\n",
    "* Computational Goal: Explore the data sets, build task-specific features, build features that encode competing scientific hypotheses about emotional valence and risk-taking, examine the stability of features over varying regularization and data resampling, build logisitc regressions and neural networks and interpret their coefficients\n",
    "* Full thesis: https://drive.google.com/file/d/1rFG2a6AN9RBRsbDSq0blgwnVahAHY399/view?usp=sharing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbff184",
   "metadata": {},
   "source": [
    "# Table Of Contents: <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Requirements and Imports](#Requirements-and-Imports)\n",
    "* [Load and Explore Data](#Load)\n",
    "    * [Per Trial Data](#per-trial)\n",
    "        * [load data and calculate task-based features](#task-features)\n",
    "        * [explore](#e1)\n",
    "    * [Per Subject Data](#per-subj)\n",
    "        * [create a per subject data frame 'gps'](#gps)\n",
    "        * [explore](#e2)\n",
    "* [Model Evaluation Helper Functions](#Evaluating-Models:-Helper-Functions)\n",
    "* [Subsetting Data Helper Functions](#Subsetting-Data-Helper-Functions)\n",
    "* [Heatmaps Helper Functions](#Heatmaps-Helper-Functions)\n",
    "* [Feature Selection](#feat)\n",
    "    * [Create Normalized Main Features and Evaluate Collinearity](#Create-Normalized-Main-Features-and-Evaluate-Collinearity)\n",
    "        * [define helper functions](#define-helper-functions-1)\n",
    "        * [generate and save features](#generate-and-save-features)\n",
    "        * [evaluate collinearity](#evaluate-collinearity)\n",
    "        * [visualize standardization](#visualize-standardization)\n",
    "    * [Calculate Quadratic Features](#Calculate-Quadratic-Features)\n",
    "    * [General Function to Standardize Features](#General-Function-to-Standardize-Features)\n",
    "    * [Stability Plots and Regularization Plots](#Stability-Plots-and-Regularization-Plots)\n",
    "        * [define helper functions](#define-helper-functions-2)\n",
    "        * [create plots](#create-plots)\n",
    "    * [Average Stability Calculations](#Average-Stability-Calculations)\n",
    "        * [save stability values](#save-stability-values)\n",
    "        * [save indicies of stability values](#save-indicies-of-stability-values)\n",
    "    * [Picking Number of Most Stable Features](#Picking-Number-of-Most-Stable-Features)\n",
    "* [Logistic Regression Modeling and Interpretation](#Logistic-Regression-Modeling-and-Interpretation)\n",
    "    * [Estimate LOOCV accuracy](#Estimate-LOOCV-accuracy-(leaving-out-one-subject's-90-trials))\n",
    "    * [Estimate Transfer Accuracy](#Estimate-Transfer-Accuracy)\n",
    "    * [Estimate Weights of a Model Trained on Entire Data Set](#Estimate-Weights-of-a-Model-Trained-on-Entire-Data-Set)\n",
    "    * [Visualize Weights of a Model Trained on Entire Data Set](#Visualize-Weights-of-a-Model-Trained-on-Entire-Data-Set)\n",
    "    * [Compare Weights of a Model Trained on Entire Data Set](#Compare-Weights-of-a-Model-Trained-on-Entire-Data-Set)\n",
    "* [Neural Network Modeling and Interpretation](#Neural-Network-Modeling-and-Interpretation)\n",
    "    * [Estimate LOOCV accuracy](#Estimate-LOOCV-accuracy)\n",
    "    * [Generate Hessian Values](#Generate-Hessian-Values)\n",
    "    * [Visualize Hessian Matrices](#Visualize-Hessian-Matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eaf0f2",
   "metadata": {},
   "source": [
    "# Requirements and Imports <a class=\"anchor\" id=\"Requirements-and-Imports\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f903810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics\n",
    "from scipy.stats.stats import pearsonr\n",
    "import math\n",
    "import csv\n",
    "from decimal import Decimal\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef5775",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff18bda",
   "metadata": {},
   "source": [
    "# Load and Explore Data <a class=\"anchor\" id=\"Load\"></a>\n",
    "Output: \n",
    "* visualizations of per trial and per subject data\n",
    "* a csv file called 'gps_[experiment].csv' which lists subjects and their characteristics. The subject ids in this file will be used for future analyses that look at cross-subject validation for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b2295",
   "metadata": {},
   "source": [
    "## Per Trial Data <a class=\"anchor\" id=\"per-trial\"></a>\n",
    "* What is the demographic information of subjects?\n",
    "* What is the range of all subject characteristics?\n",
    "* How does the choice to gamble change over time per subject?\n",
    "* How do attributes of the gambling task change over time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3271a",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5168cd",
   "metadata": {},
   "source": [
    "### load data and calculate task-based features <a class=\"anchor\" id=\"task-features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"10data_for_jess_random.csv\")\n",
    "ntrials = data.time.max() + 1\n",
    "nrows = data.shape[0]\n",
    "\n",
    "data['PredictionError'] = data['Actual'] \\\n",
    "                          - data['Gamble'] * (data['Outcome1Amount'] + data['Outcome2Amount']) / 2 \\\n",
    "                          - (1 - data['Gamble']) * data['CertainAmount']\n",
    "data['ExpectedGamble'] = (data['Outcome1Amount'] + data['Outcome2Amount']) / 2\n",
    "data['DiffCertainExpectedGamble'] = data['CertainAmount'] - data['ExpectedGamble']\n",
    "data['HigherOutcome1'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "data['LowerOutcome1'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "data['Mood'] = data['Happiness']\n",
    "\n",
    "# map subject to all row indices\n",
    "ID = {}\n",
    "for i in range(nrows):\n",
    "    id = int(data.iloc[i,0])\n",
    "    ID[id] = ID.get(id, []) + [i]\n",
    "nsubj = len(ID)\n",
    "\n",
    "# map subject characteristics to strings\n",
    "def ptype2str(x):\n",
    "    str = \"\"\n",
    "    if x == 1:\n",
    "        str = \"Depressed\"\n",
    "    else:\n",
    "        str = \"Healthy\"\n",
    "    return str\n",
    "\n",
    "def gender2str(x):\n",
    "    str = \"\"\n",
    "    if x == 1:\n",
    "        str = \"Female\"\n",
    "    else:\n",
    "        str = \"Male\"\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589e668",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e764f",
   "metadata": {},
   "source": [
    "### explore <a class=\"anchor\" id=\"e1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a32fc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store all subjects' trials\n",
    "subjDF = {}\n",
    "subjs = ID.keys()\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "    subjDF[subj] = subjD\n",
    "\n",
    "subjs = ID.keys()\n",
    "gps = []\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "    gp = sum(subjD.loc[:,'Gamble']) / subjD.loc[:,'Gamble'].size\n",
    "    gps.append(gp)\n",
    "hgp = max(gps)\n",
    "lgp = min(gps)\n",
    "print (hgp, lgp)\n",
    "\n",
    "# Visualizing all probabilities\n",
    "subjs = ID.keys()\n",
    "c = 0\n",
    "gps = []\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "    gp = sum(subjD.loc[:,'Gamble']) / subjD.loc[:,'Gamble'].size\n",
    "    gps.append(gp)\n",
    "bins = np.linspace(0, 1, 100)\n",
    "plt.hist(gps)\n",
    "plt.title(\"Gambling Probabilities\")\n",
    "plt.xlabel(\"Gambling Probability\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the Subject Data\n",
    "subjs = ID.keys()#[0:5] + ID.keys()[25:30]\n",
    "c = 0\n",
    "for subj in subjs:\n",
    "    # subject data frame, string ID, string patient type\n",
    "    subjD = data.iloc[ID[subj],:]\n",
    "    subjID = str(subjD.loc[:,'subject_id'].iloc[0])\n",
    "    subjptype = ptype2str(subjD.loc[:,'ptype'].iloc[0])\n",
    "\n",
    "    plt.plot('time', 'CertainAmount', data=subjD, label=\"non gamble amount\")\n",
    "    plt.plot('time', 'Outcome1Amount', data=subjD, label=\"gamble amount 1\")\n",
    "    plt.plot('time', 'Outcome2Amount', data=subjD, label=\"gamble amount 2\")\n",
    "    plt.plot('time', 'Actual', data=subjD, label=\"round outcome amount\", marker='o')\n",
    "    plt.plot('time', 'PredictionError', data=subjD, label=\"prediction error\", marker='o')\n",
    "    plt.plot('time', 'ExpectedGamble', data=subjD, label=\"expected gamble amount\", marker='o')\n",
    "    plt.plot('time', 'CertainAmount', data=subjD, label=\"certain amount\", marker=\"o\")\n",
    "    plt.plot('time', 'DiffCertainExpectedGamble', data=subjD, label=\"diff\", marker=\"o\")\n",
    "    plt.title(subjID + \" \" + subjptype)\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()\n",
    "\n",
    "    bins = np.linspace(0, 1, 50)\n",
    "    plt.hist([x for x in subjD.loc[:,'Happiness'] if str(x) != 'nan'], bins)\n",
    "    plt.title(\"Moods of \" + subjID + \" \" + subjptype)\n",
    "    plt.show()\n",
    "\n",
    "    bins = np.linspace(-10, 10, 50)\n",
    "    plt.hist(subjD.loc[:, \"CertainAmount\"], bins, alpha=0.5, label=\"CR\")\n",
    "    plt.hist(subjD.loc[:,'HigherOutcome1'], bins, alpha=0.5, label=\"H\")\n",
    "    plt.hist(subjD.loc[:, 'LowerOutcome1'], bins, alpha=0.5, label=\"L\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    c += 1\n",
    "    print (c)\n",
    "\n",
    "    h = subjD.loc[:, 'Happiness']\n",
    "    xs = np.arange(len(h))\n",
    "    yh = np.array(h)\n",
    "    yhm = np.isfinite(yh)\n",
    "    plt.plot(xs[yhm], yh[yhm], label=\"happiness\", marker='o')\n",
    "    plt.plot('time', 'Gamble', data=subjD, label=\"decision to gamble\", marker='o')\n",
    "    plt.ylim((-1.5,1.5))\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04fb74",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce46b9",
   "metadata": {},
   "source": [
    "## Per Subject Data <a class=\"anchor\" id=\"per-subj\"></a>\n",
    "* How does overall gambling probability vary with subject characteristics?\n",
    "* Can we predict a subject's overall gambling probability from subject characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccf351",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341354b",
   "metadata": {},
   "source": [
    "### create a per subject data frame: gps <a class=\"anchor\" id=\"gps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Up Dataframe: gps\n",
    "# every row has a subject characteristics &\n",
    "# global gambling probabilities\n",
    "\n",
    "final_name = \"gps_random.csv\"\n",
    "\n",
    "gps = data.drop_duplicates(subset = ['subject_id', 'age', 'gender',\n",
    "                                     'ptype', 'MFQ', 'SHAPS'])\n",
    "gps = gps.drop(['time', 'CertainAmount',\n",
    "                'Outcome1Amount', 'Outcome2Amount',\n",
    "                'Gamble', 'Actual', 'Happiness'], axis = 1)\n",
    "\n",
    "gps['GamblingProbability'] = 0.0\n",
    "\n",
    "# index of 'Gamble'\n",
    "gi = list(data).index('Gamble')\n",
    "\n",
    "# index of 'GamblingProbability'\n",
    "gpi = list(gps).index('GamblingProbability')\n",
    "\n",
    "# calculate subject overall gambling probability\n",
    "for i in range(nsubj):\n",
    "    id = int(gps.iloc[i,0])\n",
    "    indices = ID[id]\n",
    "    gambles = data.iloc[indices,gi]\n",
    "    gps.iloc[i,gpi] = float(sum(gambles))/float(len(gambles))\n",
    "\n",
    "gps.to_csv(final_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c05ba",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f140f61",
   "metadata": {},
   "source": [
    "### explore <a class=\"anchor\" id=\"e2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3ede8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Data Characteristics get ranges of subject characteristics\n",
    "shapss = gps.loc[:,'SHAPS']\n",
    "sl = min(shapss)\n",
    "sh = max(shapss)\n",
    "print (sl, sh)\n",
    "\n",
    "mfqs = gps.loc[:,'MFQ']\n",
    "ml = min(mfqs)\n",
    "mh = max(mfqs)\n",
    "print (ml, mh)\n",
    "\n",
    "ages = gps.loc[:,'age']\n",
    "al = min(ages)\n",
    "ah = max(ages)\n",
    "print (al, ah)\n",
    "\n",
    "genders = gps.loc[:,'gender']\n",
    "counts = genders.value_counts()\n",
    "f = counts[1] / ntrials\n",
    "m = counts[2] / ntrials\n",
    "print (f, m)\n",
    "\n",
    "dep = gps.loc[:,'ptype']\n",
    "counts = dep.value_counts()\n",
    "d = counts[1] / ntrials\n",
    "h = counts[2] / ntrials\n",
    "print (d, h)\n",
    "\n",
    "## Plot data in gps\n",
    "xnames = ['age', 'gender', 'ptype', 'MFQ', 'SHAPS']\n",
    "for xname in xnames:\n",
    "    gps.plot(kind='scatter',x=xname,y='GamblingProbability',color='red')\n",
    "    plt.show()\n",
    "\n",
    "## Run linear regressions\n",
    "y = gps.iloc[:,gpi]\n",
    "X = gps.loc[:,xnames]\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "feature = 'ptype'\n",
    "y = gps.iloc[:,gpi]\n",
    "x = gps.loc[:,feature].values.reshape((-1, 1))\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "y_pred = model.intercept_ + model.coef_ * x\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, model.intercept_ + model.coef_ * x, '-')\n",
    "plt.xlabel(feature)\n",
    "plt.ylabel(\"Gambling Probability\")\n",
    "plt.title(\"GP vs. \" + feature + \": r^2 = \" + str(r_sq))\n",
    "plt.show()\n",
    "\n",
    "## Pearson Correlation\n",
    "\n",
    "feature = 'ptype'\n",
    "feature2 = 'SHAPS'\n",
    "#y = gps.iloc[:,gpi]\n",
    "x1 = gps.loc[:,feature]\n",
    "x2 = gps.loc[:,feature2]\n",
    "\n",
    "r = pearsonr(x1,x2)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253dc0e2",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0636da",
   "metadata": {},
   "source": [
    "# Evaluating Models: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Per Trial Predictive Accuracy\n",
    "\n",
    "\n",
    "def accthreshold(obsvs, preds, threshold):\n",
    "    '''\n",
    "    Evaluate the accuracy of a prediction given a list of probabilitstic predictions (preds), \n",
    "    a probabilititic threshold (threshold), and the binary ground truth (obsvs)\n",
    "    '''\n",
    "    preds_bool = preds > threshold\n",
    "    acc = sklearn.metrics.accuracy_score(obsvs, preds_bool)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def maxacc(obsvs, preds):\n",
    "    '''\n",
    "    Given a list of probabilitstic predictions (preds) and the binary ground truth (obsvs),\n",
    "    evaluate the largest accuracy achieved across probabilistic thresholds 0.01, 0.02, ... 1.00\n",
    "    '''\n",
    "    thresholds = [float(i) / float(100) for i in range(1,100,1)]\n",
    "    temp = 0\n",
    "    print (\"the accs are \")\n",
    "    for threshold in thresholds:\n",
    "        temp2 = accthreshold(obsvs, preds, threshold)\n",
    "        print (temp2)\n",
    "        temp = max(temp, temp2)\n",
    "    return temp\n",
    "\n",
    "def balancedacc(obsvs, preds, threshold):\n",
    "    '''\n",
    "    Evaluate the balanced accuracy of a prediction given a list of probabilitstic predictions (preds),\n",
    "    a probabilititic threshold (threshold), and the binary ground truth (obsvs)\n",
    "    '''\n",
    "    preds_bool = preds > threshold\n",
    "    conditionpositive = sum(obsvs)\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(obsvs, preds_bool).ravel()\n",
    "    tpr = float(tp) / float(tp + fn)\n",
    "    fpr = float(tn) / float(tn + fp)\n",
    "    bacc = (tpr + fpr) / 2\n",
    "    return bacc\n",
    "\n",
    "def maxbacc(obsvs, preds):\n",
    "    '''\n",
    "    Given a list of probabilitstic predictions (preds) and the binary ground truth (obsvs),\n",
    "    evaluate the largest balanced accuracy achieved across probabilistic thresholds 0.01, 0.02, ... 1.00\n",
    "    '''\n",
    "    thresholds = [float(i) / float(100) for i in range(1,100,1)]\n",
    "    temp = 0\n",
    "    print (\"the accs are \")\n",
    "    for threshold in thresholds:\n",
    "        temp2 = balancedacc(obsvs, preds, threshold)\n",
    "        print (temp2)\n",
    "        temp = max(temp, temp2)\n",
    "    return temp\n",
    "\n",
    "\n",
    "# Evaluate Per Subject Predictive Accuracy\n",
    "\n",
    "def get_indices(x,xs):\n",
    "    '''\n",
    "    Find the indicies of an element x in a list xs\n",
    "    '''\n",
    "    return [i for (i,y) in zip(range(len(xs)), xs) if y == x]\n",
    "\n",
    "\n",
    "def subjauc(subjs, obsvs, preds):\n",
    "    '''\n",
    "    Evaluate the AUC of a prediction averaged across subjects,\n",
    "    given the probabilistic predictions (preds),\n",
    "    the binary ground truth (obsvs),\n",
    "    and the subject ID of each trial (subjs)\n",
    "    e.g.\n",
    "    obsvs : [1  , 1  , 0  , 1  , 0  , 1  , 0  , 1  , 1]\n",
    "    preds : [0.8, 0.4, 0.2, 0.8, 0.6, 0.2, 0.6, 0.4, 0.8] \n",
    "    subjs : [a, a, a, b, b, b, c, c, c]\n",
    "\n",
    "    aucs  : [1.0, 0.5, 0.5]\n",
    "    average AUC : 0.66\n",
    "    '''\n",
    "    ss = set(subjs)\n",
    "    aucs = []\n",
    "    for s in ss:\n",
    "        sind = get_indices(s, subjs)\n",
    "        obsvs_s = obsvs[sind]\n",
    "        preds_s = preds[sind]\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvs_s, preds_s)\n",
    "        auc_s = sklearn.metrics.auc(fpr, tpr)\n",
    "        aucs.append(auc_s)\n",
    "    return np.nanmean(aucs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9565ba",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262093d0",
   "metadata": {},
   "source": [
    "# Subsetting Data Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Subsetting Data by Subject ID or Feature Column Index\n",
    "\n",
    "\n",
    "def subjXY(sid, x, y):\n",
    "    '''\n",
    "    Subset the x and y values corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    x = np.array([['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]])\n",
    "    y = np.array([['a', 1], ['b', 1], ['c', 0]])\n",
    "    subjXY(sid,x,y) = [np.array([0, 7, 2]), np.array([1])]\n",
    "    '''\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = np.where(sidscol == sid)\n",
    "    features = np.squeeze(x[inds,1:])\n",
    "    targets = np.squeeze(y[inds,1:])\n",
    "    return [features, targets]\n",
    "\n",
    "def getXY(sids, x, y):\n",
    "    \n",
    "    '''\n",
    "    Subset the x and y values corresponding to a list of subject IDs sids\n",
    "\n",
    "    sids = a list of subject ID strings\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sids = ['a','b']\n",
    "    x = np.array([['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]])\n",
    "    y = np.array([['a', 1], ['b', 1], ['c', 0]])\n",
    "    getXY(sids,x,y) = [np.array([0, 7, 2], [1, 4, 5]), np.array([1, 1])]\n",
    "    '''\n",
    "    \n",
    "    xs = np.zeros(shape=[x.shape[0]-1, x.shape[1]-1])\n",
    "    ys = np.zeros(shape=[y.shape[0]-1, 1])\n",
    "    xbool = np.zeros(shape=y.shape[0]-1)\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(sids)):\n",
    "        [sx, sy] = subjXY(sids[i], x, y)\n",
    "        size = sx.shape[0]\n",
    "        xs[range(counter, counter+size), :] = sx\n",
    "        ys[range(counter, counter+size)] = sy.reshape(len(sy),1)\n",
    "        counter = counter + size\n",
    "    return [xs, ys]\n",
    "\n",
    "def subjY(sid, y):\n",
    "    '''\n",
    "    Subset the y values corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    y = np.array([['a', 1], ['b', 1], ['c', 0]])\n",
    "    subjY(sid,y) = np.array([1])\n",
    "    '''\n",
    "    sidscol = np.array(y[:,0])\n",
    "    inds = np.where(sidscol == sid)\n",
    "    targets = np.squeeze(y[inds,1:])\n",
    "    return targets\n",
    "\n",
    "def notsubjXY(sid, x, y):\n",
    "    '''\n",
    "    Subset the x and y values not corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    x = np.array([['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]])\n",
    "    y = np.array([['a', 1], ['b', 1], ['c', 0]])\n",
    "    notsubjXY(sid,x,y) = [np.array([[1, 4, 5],[1, 5, 3]]), np.array([1,0])]\n",
    "    '''\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = np.where(sidscol != sid)\n",
    "    features = np.squeeze(x[inds,1:])[1:,]\n",
    "    targets = np.squeeze(y[inds,1:])[1:]\n",
    "    return [features, targets]\n",
    "\n",
    "def notsubjY(sid, y):\n",
    "    '''\n",
    "    Subset the y values not corresponding to a given subject sid\n",
    "\n",
    "    sid = subject ID string\n",
    "    y = np array of targest including all subject ID strings as column 0\n",
    "\n",
    "    e.g.\n",
    "    sid = 'a'\n",
    "    y = np.array([['a', 1], ['b', 1], ['c', 0]])\n",
    "    notsubjY(sid,y) = [np.array([1,0])]\n",
    "    '''\n",
    "    sidscol = np.array(y[:,0])\n",
    "    inds = np.where(sidscol != sid)\n",
    "    targets = np.squeeze(y[inds,1:])[1:]\n",
    "    return targets\n",
    "\n",
    "def subsetx(x, l):\n",
    "    '''\n",
    "    Subset a list of l specific features of x\n",
    "    x = np array of features including all subject ID strings as column 0\n",
    "    l = 0 indexed list of desired features\n",
    "\n",
    "    e.g.\n",
    "    x = np.array([['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]])\n",
    "    l = [1,3]\n",
    "    subsetx(x,l) = np.array([['a', 0, 2], ['b', 1, 5], ['c', 1, 3]])\n",
    "    '''\n",
    "    xnew = np.zeros(shape=(x.shape[0], len(l) + 1))\n",
    "    xnew[:,0] = x[:,0]\n",
    "    for col in range(len(l)):\n",
    "        colind = l[col]\n",
    "        xnew[:,col+1] = x[:,colind]\n",
    "    return xnew\n",
    "\n",
    "def subsetxfile(filename, feats):\n",
    "    '''\n",
    "    Subset a list of l specific features of x\n",
    "    filename = the filename of a file with the 0th row as the names of features \n",
    "               and the 0th column as subject ID strings\n",
    "    feats = 0-indexed list of desired features\n",
    "\n",
    "    e.g.\n",
    "    filename = 'feats.csv' containing {columns = ['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                                       data = [['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]]}\n",
    "    feats = [1,3]\n",
    "    subsetxfile(filename, feats) = np.array([['a', 0, 2], ['b', 1, 5], ['c', 1, 3]])\n",
    "    '''\n",
    "    x = np.genfromtxt(filename, delimiter=\",\")\n",
    "    xnew = np.zeros(shape=(x.shape[0], len(feats) + 1))\n",
    "    xnew[:,0] = x[:,0]\n",
    "    for col in range(len(feats)):\n",
    "        colind = feats[col]\n",
    "        xnew[:,col+1] = x[:,colind]\n",
    "    return xnew\n",
    "\n",
    "\n",
    "def subsetfilesubj(filename, sids):\n",
    "    '''\n",
    "    Subset all the features in filename corresponding to subjects in the list sids\n",
    "    filename = the filename of a file with the 0th row as the names of features \n",
    "               and the 0th column as subject ID strings\n",
    "    sids = list of desired subject IDs\n",
    "\n",
    "    e.g.\n",
    "    filename = 'feats.csv' containing {columns = ['subject ID', 'depression status', 'age', 'anhedonia score'], \n",
    "                                       data = [['a', 0, 7, 2], ['b', 1, 4, 5], ['c', 1, 5, 3]]}\n",
    "    sids = ['a','b']\n",
    "    subsetfilesubj(filename,sids) = np.array([[0, 7, 2], [1, 4, 5]])\n",
    "    '''\n",
    "    x = np.genfromtxt(filename+'.csv', delimiter=\",\")\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = []\n",
    "    for sid in sids:\n",
    "        ind = np.where(sidscol == sid)\n",
    "        inds.append(ind)\n",
    "    inds = np.array(inds).reshape((-1))\n",
    "    fts = np.squeeze(x[inds,])\n",
    "    with open(filename + '_overlap_frm'+ traintype + '.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(np.zeros(x.shape[1]))\n",
    "        writer.writerows(fts)\n",
    "    writeFile.close()\n",
    "    return fts[:,1:]\n",
    "\n",
    "def subsetfilesubjSUBJS(filename, sids):\n",
    "    '''\n",
    "    Subset the list of subject ID indicies in filename corresponding to subjects in the list sids\n",
    "    filename = the filename of a file with the 0th row as the names of features \n",
    "               and the 0th column as subject ID strings\n",
    "    sids = list of desired subject IDs\n",
    "\n",
    "    e.g.\n",
    "    filename = 'feats.csv' containing {columns = ['subject ID', 'depression status', 'age', 'anhedonia score', 'trial'], \n",
    "                                       data = [['a', 0, 7, 2, 1], ['a', 0, 7, 2, 2],\n",
    "                                               ['b', 1, 4, 5, 1], ['b', 1, 4, 5, 2],\n",
    "                                               ['c', 1, 5, 3, 1], ['c', 1, 5, 3, 2]]}\n",
    "    sids = ['a','b']\n",
    "    subsetfilesubjSUBJS(filename,sids) = np.array(['a','a','b','b'])\n",
    "    '''\n",
    "    x = np.genfromtxt(filename+'.csv', delimiter=\",\")\n",
    "    sidscol = np.array(x[:,0])\n",
    "    inds = []\n",
    "    for sid in sids:\n",
    "        ind = np.where(sidscol == sid)\n",
    "        inds.append(ind)\n",
    "    inds = np.array(inds).reshape((-1))\n",
    "    fts = np.squeeze(x[inds,])\n",
    "    with open(filename + '_overlap_frm'+ traintype + '.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(np.zeros(x.shape[1]))\n",
    "        writer.writerows(fts)\n",
    "    writeFile.close()\n",
    "    return fts[:,0]\n",
    "\n",
    "def strip(m):\n",
    "    '''\n",
    "    input: matrix\n",
    "    output: matrix without column names or row names\n",
    "    '''\n",
    "    m = m[1:, 1:]\n",
    "    return m\n",
    "\n",
    "def stripfile(filename):\n",
    "    '''\n",
    "    input: filename\n",
    "    output: matrix without column names or row names\n",
    "    '''\n",
    "    m = np.genfromtxt(filename, delimiter=\",\")\n",
    "    m = m[1:, 1:]\n",
    "    return m\n",
    "\n",
    "def getcolumnnames(filename):\n",
    "    '''\n",
    "    Given the filename of a csv file\n",
    "    return the columnnames\n",
    "    \n",
    "    e.g. getcolumnnames(\"results.csv\") = ['model #', 'AUC', 'ACC', 'Subject ACC']\n",
    "    '''\n",
    "    with open(filename, \"rt\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        i = next(reader)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699198d",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39fa45",
   "metadata": {},
   "source": [
    "# Heatmaps Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce87043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Visualize Heatmaps\n",
    "\n",
    "# https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    '''\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "    [Back to Table of Contents](#toc)Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    '''\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, orientation= \"horizontal\", ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=0, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    '''\n",
    "    A function to annotate a heatmap.\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    '''\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bcbeb",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ae135",
   "metadata": {},
   "source": [
    "# Feature Selection <a class=\"anchor\" id=\"feat\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968e6c1",
   "metadata": {},
   "source": [
    "## Create Normalized Main Features and Evaluate Collinearity\n",
    "\n",
    "Main Features:\n",
    "* normalization = (centered at 0 and with standard deviation 1) \n",
    "* The subject characteristics will be age, gender, and diagnosis. \n",
    "* The trial parameters pertaining to the current trial will an indicator (with 1 if for this trial, choosing to gamble will always yield more money than choosing not to gamble), current expected reward (average of not gamble reward option and two gamble reward options), and gambling range (difference between higher gamble reward option and lower gamble reward option). \n",
    "* The trial parameters pertaining to the outcomes of past trials are an exponential sum of past reward prediction errors and an exponential sum of past outcomes. To elaborate, reward prediction error is actual reward outcome - expected reward outcome while the exponential sums are weighed so more recent trials have large weights closer to 1 and more previous trials have smaller weights closer to 0.\n",
    "\n",
    "Output: \n",
    "* x and y csv files where x contains main features to predict gambling probability (0 - 1) and y contains the target gambles themselves (1's or 0's)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a22740",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c57c88",
   "metadata": {},
   "source": [
    "### define helper functions <a class=\"anchor\" id=\"define-helper-functions-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input single value\n",
    "# convert ages 12 to 18\n",
    "# to ages -3 to 3\n",
    "def age(v):\n",
    "    return v - 15\n",
    "\n",
    "# input single value\n",
    "# convert 1 (female) to 0\n",
    "# convert 2 (male) to 1\n",
    "def gender(v):\n",
    "    return v-1\n",
    "\n",
    "# input single value\n",
    "# convert 1 (depression) to 1\n",
    "# convert 2 (healthy) to 0\n",
    "def diagnosis(v):\n",
    "    nv = 0\n",
    "    if v == 1:\n",
    "        nv = 1\n",
    "    return nv\n",
    "\n",
    "def bool2int(b):\n",
    "    i = 1 if b else 0\n",
    "    return i\n",
    "\n",
    "\n",
    "def ems(rewards, gamma):\n",
    "    pastrewards = np.zeros(len(rewards))\n",
    "    for i in range(len(rewards) - 1):\n",
    "        pastrewards[i+1] = gamma * pastrewards[i] + rewards.iloc[i]\n",
    "    return pastrewards\n",
    "\n",
    "# input a single value\n",
    "# return the utility\n",
    "# as sigmoid function centered at 0 instead 0.5\n",
    "def utility(v):\n",
    "    u = math.exp(v) / (math.exp(v) + 1) - 0.5\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05177e9",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054a0e3",
   "metadata": {},
   "source": [
    "### generate and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96535207",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacsvs = [\"10data_for_jess_random\"]\n",
    "suffices = [\"_random\"]\n",
    "\n",
    "standardize = True\n",
    "cn = 'Actual' \n",
    "\n",
    "for datacsv, suffix in zip(datacsvs, suffices):\n",
    "    data=pd.read_csv(datacsv + \".csv\", sep=',',header='infer')\n",
    "    nrows = data.shape[0]\n",
    "    data['HigherOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "    data['LowerOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "    data['Mood'] = data['Happiness']\n",
    "    data['diagnosis'] = data['ptype']\n",
    "    \n",
    "    # update subject features\n",
    "    data['age'] = data['age'].copy().map(age)\n",
    "    data['gender'] = data['gender'].copy().map(gender)\n",
    "    data['diagnosis'] = data['diagnosis'].copy().map(diagnosis)\n",
    "    data['Indicator'] = data['CertainAmount'] < data['LowerOutcome']\n",
    "    data['Indicator'] = data['Indicator'].map(bool2int)\n",
    "    \n",
    "    # standardize raw trial parameters\n",
    "    if standardize:\n",
    "        v = data.loc[:,cn].copy()\n",
    "        sd = np.std(v)\n",
    "        print (\"for \", datacsv, \" sd of \", cn, \" : \", str(sd))\n",
    "        data['CertainAmount'] = data['CertainAmount'].copy().map(lambda x : x / sd)\n",
    "        data['HigherOutcome'] = data['HigherOutcome'].copy().map(lambda x : x / sd)\n",
    "        data['LowerOutcome'] = data['LowerOutcome'].copy().map(lambda x : x / sd)\n",
    "        data['Actual'] = data['Actual'].copy().map(lambda x : x / sd)\n",
    "        \n",
    "    # calculate utility over trial parameters\n",
    "    data['CertainAmountUtility'] = data['CertainAmount'].copy().map(utility)\n",
    "    data['HigherOutcomeUtility'] = data['HigherOutcome'].copy().map(utility)\n",
    "    data['LowerOutcomeUtility'] = data['LowerOutcome'].copy().map(utility)\n",
    "    data['ActualUtility'] = data['Actual'].copy().map(utility)\n",
    "    \n",
    "    # compute 2nd layer values\n",
    "    data['CurrentExpectedReward'] = (data['CertainAmount'] + data['HigherOutcome'] + data['LowerOutcome']) / 3\n",
    "    data['ExpectedGambleOutcome'] = (data['LowerOutcome'] + data['HigherOutcome']) / 2\n",
    "    data['diff'] = data['ExpectedGambleOutcome'] - data['CertainAmount']\n",
    "    data['PredictionError'] = data['Gamble'] * (data['Actual'] - data['ExpectedGambleOutcome'])\n",
    "    data['CertainReward'] = (1 - data['Gamble']) * data['Actual']\n",
    "    data['GamblingReward'] = data['Gamble'] * data['Actual']\n",
    "    data['GamblingRange'] = data['HigherOutcome'] - data['LowerOutcome']\n",
    "    \n",
    "    # compute 2nd layer utilities\n",
    "    data['ExpectedGambleOutcomeUtility'] = (data['LowerOutcomeUtility'] + data['HigherOutcomeUtility']) / 2\n",
    "    data['diffUtility'] = data['ExpectedGambleOutcomeUtility'] - data['CertainAmountUtility']\n",
    "    data['PredictionErrorUtility'] = data['Gamble'] * (data['ActualUtility'] - data['ExpectedGambleOutcomeUtility'])\n",
    "    data['CertainRewardUtility'] = (1 - data['Gamble']) * data['ActualUtility']\n",
    "    data['GamblingRewardUtility'] = data['Gamble'] * data['ActualUtility']\n",
    "    data['GamblingRangeUtility'] = data['HigherOutcomeUtility'] - data['LowerOutcomeUtility']\n",
    "    data['CurrentExpectedRewardUtility'] = (data['CertainAmountUtility'] + data['HigherOutcomeUtility'] + data['LowerOutcomeUtility']) / 3\n",
    "    \n",
    "    # print final column names\n",
    "    colnames = data.columns\n",
    "    \n",
    "    # update parameters that summarize the past (mood, cumulative reward, past not gamble reward, past gamble reward)\n",
    "    ## record the indices of each new subject\n",
    "    ID = {}\n",
    "    for i in range(nrows):\n",
    "        id = int(data.iloc[i,0])\n",
    "        ID[id] = ID.get(id, []) + [i]\n",
    "    nsubj = len(ID)\n",
    "    \n",
    "    ## Create a dictionary to store all subjects\n",
    "    subjDF = {}\n",
    "    subjs = ID.keys()\n",
    "    for subj in subjs:\n",
    "        subjD = data.iloc[ID[subj],:].copy()\n",
    "        subjDF[subj] = subjD\n",
    "    \n",
    "    # compute timeseries values\n",
    "    gammas = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "    for subj in subjs:\n",
    "        # fill mood with most recently reported mood, otherwise pad with 0's\n",
    "        subjDF[subj].loc[:,'Mood'] = subjDF[subj].loc[:,'Mood'].copy().fillna(method='ffill').fillna(0)\n",
    "        # Calculate EMS features for the following features \n",
    "        for col in ['Actual', 'CertainReward', 'GamblingReward', 'PredictionError',\n",
    "                    'ActualUtility', 'CertainRewardUtility', 'GamblingRewardUtility', 'PredictionErrorUtility']:\n",
    "            rewards = subjDF[subj].loc[:,col].copy()\n",
    "            for gamma in gammas:\n",
    "                pastrewards = ems(rewards, gamma)\n",
    "                subjDF[subj].loc[:,col+'EMS'+str(gamma)] = pastrewards\n",
    "\n",
    "    # remove the first three trials of all subjects\n",
    "    def strip3(df):\n",
    "        newdf = df.iloc[3:,:].copy()\n",
    "        return newdf\n",
    "    stripsubjDF = dict(map(lambda kv: (kv[0], strip3(kv[1])), subjDF.items()))\n",
    "\n",
    "    # put subject data into a new dataframe\n",
    "    datanew = pd.concat(stripsubjDF.values())\n",
    "\n",
    "    # save all the columns into a csv\n",
    "    if standardize:\n",
    "        datanew.to_csv(datacsv + \"_normed_standardized_\" + cn + \".csv\", index=False) # don't include the indices\n",
    "    else:\n",
    "        datanew.to_csv(datacsv + \"_normed.csv\", index=False) # don't include the indices\n",
    "    \n",
    "    namesl = ['subject_id', 'age', 'gender', 'diagnosis', 'mood', \n",
    "              'current expected reward', 'current gambling range', 'current indicator',\n",
    "              'past rewards', 'past reward prediction error']\n",
    "\n",
    "    # modelEV\n",
    "    for gamma in gammas:\n",
    "        xl = ['subject_id', 'age', 'gender', 'diagnosis', 'Mood',\n",
    "              'CurrentExpectedReward', 'GamblingRange', 'Indicator', \n",
    "              'ActualEMS'+str(gamma), 'PredictionErrorEMS'+str(gamma)]\n",
    "        yl = ['subject_id', 'Gamble']\n",
    "        x = datanew[xl].copy()\n",
    "        x = x.rename(index=str, columns=dict(zip(xl, namesl))).copy()\n",
    "        y = datanew[yl].copy()\n",
    "        if standardize:\n",
    "            x.to_csv(\"x\" + suffix + \"_normed_standardized_\" + cn + \"_EV\" + str(gamma) + \".csv\", index=False) # don't include the indices\n",
    "            y.to_csv(\"y\" + suffix + \"_normed_standardized_\" + cn + \"_EV\" + str(gamma) + \".csv\", index=False) # don't include the indices\n",
    "        else:\n",
    "            x.to_csv(\"x\" + suffix + \"_normed_EV\" + str(gamma) + \".csv\", index=False) # don't include the indices\n",
    "            y.to_csv(\"y\" + suffix + \"_normed_EV\" + str(gamma) + \".csv\", index=False) # don't include the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c2a8",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb24e0",
   "metadata": {},
   "source": [
    "### evaluate collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacsvs = [\"10data_for_jess_random\"]\n",
    "suffices = [\"_random\"]\n",
    "cols = [\"random\"]\n",
    "        \n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(19,15))\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "j = 0\n",
    "for datacsv, suffix in zip(datacsvs, suffices):\n",
    "    data=pd.read_csv(datacsv + \".csv\", sep=',',header='infer')\n",
    "    nrows = data.shape[0]\n",
    "    data['HigherOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "    data['LowerOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "    data['Mood'] = data['Happiness']\n",
    "    data['diagnosis'] = data['ptype']\n",
    "    \n",
    "    # update subject features\n",
    "    data['age'] = data['age'].copy().map(age)\n",
    "    data['gender'] = data['gender'].copy().map(gender)\n",
    "    data['diagnosis'] = data['diagnosis'].copy().map(diagnosis)\n",
    "    data['Indicator'] = data['CertainAmount'] < data['LowerOutcome']\n",
    "    \n",
    "    # compute 2nd layer values\n",
    "    data['CurrentExpectedReward'] = (data['CertainAmount'] + data['HigherOutcome'] + data['LowerOutcome']) / 3\n",
    "    data['ExpectedGambleOutcome'] = (data['LowerOutcome'] + data['HigherOutcome']) / 2\n",
    "    data['diff'] = data['ExpectedGambleOutcome'] - data['CertainAmount']\n",
    "    data['PredictionError'] = data['Gamble'] * (data['Actual'] - data['ExpectedGambleOutcome'])\n",
    "    data['CertainReward'] = (1 - data['Gamble']) * data['Actual']\n",
    "    data['GamblingReward'] = data['Gamble'] * data['Actual']\n",
    "    data['GamblingRange'] = data['HigherOutcome'] - data['LowerOutcome']\n",
    "    \n",
    "    # update parameters that summarize the past (mood, cumulative reward, past not gamble reward, past gamble reward)\n",
    "    ## record the indices of each new subject\n",
    "    ID = {}\n",
    "    for i in range(nrows):\n",
    "        id = int(data.iloc[i,0])\n",
    "        ID[id] = ID.get(id, []) + [i]\n",
    "    nsubj = len(ID)\n",
    "    \n",
    "    ## Create a dictionary to store all subjects\n",
    "    subjDF = {}\n",
    "    subjs = ID.keys()\n",
    "    for subj in subjs:\n",
    "        subjD = data.iloc[ID[subj],:].copy()\n",
    "        subjDF[subj] = subjD\n",
    "    \n",
    "    # compute timeseries values\n",
    "    gammas = [0.5]\n",
    "    for subj in subjs:\n",
    "        # fill mood with most recently reported mood, otherwise pad with 0's\n",
    "        subjDF[subj].loc[:,'Mood'] = subjDF[subj].loc[:,'Mood'].copy().fillna(method='ffill').fillna(0)\n",
    "        # Calculate EMS features for the following features \n",
    "        for col in ['Actual', 'CertainReward', 'GamblingReward', 'PredictionError']:\n",
    "            rewards = subjDF[subj].loc[:,col].copy()\n",
    "            for gamma in gammas:\n",
    "                pastrewards = ems(rewards, gamma)\n",
    "                subjDF[subj].loc[:,col+'EMS'+str(gamma)] = pastrewards\n",
    "\n",
    "    # remove the first three trials of all subjects\n",
    "    def strip3(df):\n",
    "        newdf = df.iloc[3:,:].copy()\n",
    "        return newdf\n",
    "    stripsubjDF = dict(map(lambda kv: (kv[0], strip3(kv[1])), subjDF.items()))\n",
    "\n",
    "    # put subject data into a new dataframe\n",
    "    datanew = pd.concat(stripsubjDF.values())\n",
    "    \n",
    "    \n",
    "    ## new features\n",
    "    \n",
    "    namesl = ['subject_id', 'age', 'gender', 'diagnosis', 'mood', \n",
    "              'current expected reward', 'current diff', 'current gambling range', 'current indicator',\n",
    "              'past rewards', 'past reward prediction error']\n",
    "\n",
    "    xl = ['subject_id', 'age', 'gender', 'diagnosis', 'Mood',\n",
    "          'CurrentExpectedReward', 'diff', 'GamblingRange', 'Indicator', \n",
    "          'ActualEMS'+str(gamma), 'PredictionErrorEMS'+str(gamma)]\n",
    "    yl = ['subject_id', 'Gamble']\n",
    "    x = datanew[xl].copy()\n",
    "    x = x.rename(index=str, columns=dict(zip(xl, namesl))).copy()\n",
    "    y = datanew[yl].copy()\n",
    "\n",
    "    print (suffix)\n",
    "    im = axes[1,j].imshow(x.iloc[:,1:].copy().corr(), vmin=-1.0, vmax=1.0)\n",
    "        \n",
    "    print (\"new features : \")\n",
    "    for i in range(1, len(namesl)):\n",
    "        print (i, namesl[i])\n",
    "        \n",
    "    j = j + 1\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig(\"corr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435bb12",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242ecbb",
   "metadata": {},
   "source": [
    "### visualize standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacsvs = [\"10data_for_jess_random\"]\n",
    "suffices = [\"_random\"]\n",
    "cols = [\"random\"]\n",
    "\n",
    "standardize = True\n",
    "cn = 'Actual'\n",
    "\n",
    "rows = ['Before', 'After']\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(15,8))\n",
    "st = fig.suptitle(\"Trial Values Before (Top) and After (Bottom) Standardizing with \" + cn, fontsize=\"x-large\")\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "    \n",
    "j = 0\n",
    "for datacsv, suffix in zip(datacsvs, suffices):\n",
    "    # https://stackoverflow.com/questions/31726643/how-do-i-get-multiple-subplots-in-matplotlib\n",
    "    \n",
    "    data=pd.read_csv(datacsv + \".csv\", sep=',',header='infer')\n",
    "    nrows = data.shape[0]\n",
    "    data['HigherOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].max(axis=1)\n",
    "    data['LowerOutcome'] = data[[\"Outcome1Amount\", \"Outcome2Amount\"]].min(axis=1)\n",
    "    data['Mood'] = data['Happiness']\n",
    "    data['diagnosis'] = data['ptype']\n",
    "    \n",
    "    print (suffix, str(j))\n",
    "    \n",
    "    axes[0,j].hist(data['HigherOutcome'], label = 'Higher Outcome')\n",
    "    axes[0,j].hist(data['LowerOutcome'], label = 'Lower Outcome')\n",
    "    axes[0,j].hist(data['CertainAmount'], label = 'Certain Amount')\n",
    "    axes[0,j].tick_params(axis='both', which='major', pad=10)\n",
    "    \n",
    "    # update subject features\n",
    "    data['age'] = data['age'].copy().map(age)\n",
    "    data['gender'] = data['gender'].copy().map(gender)\n",
    "    data['diagnosis'] = data['diagnosis'].copy().map(diagnosis)\n",
    "    \n",
    "    # standardize raw trial parameters\n",
    "    if standardize:\n",
    "        \n",
    "        v = data.loc[:,cn].copy()\n",
    "        sd = np.std(v)\n",
    "        print (\"for \", datacsv, \" sd of \", cn, \" : \", str(sd))\n",
    "        data['CertainAmount'] = data['CertainAmount'].copy().map(lambda x : x / sd)\n",
    "        data['HigherOutcome'] = data['HigherOutcome'].copy().map(lambda x : x / sd)\n",
    "        data['LowerOutcome'] = data['LowerOutcome'].copy().map(lambda x : x / sd)\n",
    "        data['Actual'] = data['Actual'].copy().map(lambda x : x / sd)\n",
    "    \n",
    "    axes[1,j].hist(data['HigherOutcome'], label = 'Higher Outcome')\n",
    "    axes[1,j].hist(data['LowerOutcome'], label = 'Lower Outcome')\n",
    "    axes[1,j].hist(data['CertainAmount'], label = 'Certain Amount')\n",
    "    axes[1,j].tick_params(axis='both', which='major', pad=10)\n",
    "    \n",
    "    j = j + 1\n",
    "\n",
    "# https://stackoverflow.com/questions/39164828/global-legend-for-all-subplots\n",
    "axes.flatten()[-2].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=3)\n",
    "plt.savefig('standardize_'+cn+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12243c79",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e17c3",
   "metadata": {},
   "source": [
    "## Calculate Quadratic Features\n",
    "\n",
    "Output:\n",
    "* x csv file with interactions, npy file with all column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + nf + nf * (nf-1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['x_random_normed_standardized_Actual_EV0.5']\n",
    "for filename in filenames:\n",
    "    names = ['age', 'gender', 'diagnosis', 'mood',\n",
    "          'current expected reward', 'current gambling range', 'current indicator',\n",
    "          'past rewards', 'past reward prediction error']\n",
    "    nf = len(names)\n",
    "    x = np.genfromtxt(filename+ '.csv', delimiter=\",\")\n",
    "    x2 = np.zeros(shape=(x.shape[0], int(1 + nf + nf * (nf-1) / 2)))\n",
    "    \n",
    "    # align subject ID column\n",
    "    x2[:,range(nf+1)] = x[:,range(nf+1)]\n",
    "    \n",
    "    counter = nf+1\n",
    "    for i in range(nf):\n",
    "        for j in range(i+1, nf):\n",
    "            v = x[:, i+1] * x[:, j+1]\n",
    "            x2[:,counter] = v\n",
    "            names.append(names[i] + ' * ' + names[j])\n",
    "            counter = counter + 1\n",
    "    print (\"done with file : \", filename)\n",
    "    np.savetxt(filename+'_interactions.csv', x2, delimiter=\",\")\n",
    "\n",
    "    np.save(filename+'_interactions_names.npy', names)\n",
    "with open('x_stdz_interactions.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbbf58",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b0279",
   "metadata": {},
   "source": [
    "## General Function to Standardize Features\n",
    "Start: \n",
    "* any x csv file\n",
    "\n",
    "Data Analysis: \n",
    "* standardize.py to z-score all the features\n",
    "\n",
    "Output: \n",
    "* x csv file with interactions, npy file with all column names, all features are standardized. This is useful for stability selection which has stability results/ rankings that are very sensitive to standard deviation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d932f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(filename):\n",
    "    '''\n",
    "    Given the name of a csv file, generate a new csv with each column standardized (mean of 0, std of 1)\n",
    "    Input: filename is a string corresponding to filename.csv,\n",
    "    Output: filename_stdz.csv with standardized columns, the 0th column is preserved\n",
    "    '''\n",
    "    temp = np.genfromtxt(filename + '.csv', delimiter=\",\")\n",
    "    ft = np.empty(shape=[temp.shape[0]-1,temp.shape[1]])\n",
    "    colnames = np.empty(shape=[temp.shape[1]])\n",
    "    \n",
    "    with open(filename+'.csv', 'rt') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        counter = -1\n",
    "        for row in reader:\n",
    "            if counter >= 0:\n",
    "                ft[counter, :] = row\n",
    "            else:\n",
    "                colnames = row\n",
    "            counter = counter + 1\n",
    "\n",
    "    csvfile.close()\n",
    "    means = []\n",
    "    stds = []\n",
    "    for col in range(1,temp.shape[1]):\n",
    "\n",
    "        m = ft[:,col].mean(axis=0)\n",
    "        means.append(m)\n",
    "        s = ft[:,col].std(axis=0)\n",
    "        stds.append(s)\n",
    "        if col == 1:\n",
    "            pass\n",
    "        if s == 0.0 :\n",
    "            ft[:,col] = (ft[:,col] - m)\n",
    "        else:\n",
    "            ft[:,col] = (ft[:,col] - m) / s\n",
    "\n",
    "    with open(filename + '_stdz.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(colnames)\n",
    "        writer.writerows(ft)\n",
    "\n",
    "    writeFile.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'x_random_normed_standardized_Actual_EV0.5_interactions'\n",
    "standardize(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932af3d",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a971b",
   "metadata": {},
   "source": [
    "## Stability Plots and Regularization Plots\n",
    "\n",
    "Start: \n",
    "* x csv file with interactions, npy file with all column names, all features are standardized.\n",
    "Data Analysis: \n",
    "* create regularization plot with the functions reggraph(x, y, NFEATS) then reggraphsave(x,y)\n",
    "* create stability plot with functions \n",
    "    * stabgraph(x, y) \n",
    "    * stabgraphsavenolegend(x, y) if you don't want the legend \n",
    "    * stabgraphsave(x, y) or stabgraphsavenolegend(x, y) or stabgraphsavefeat(x, y, l) if you want a specific list of them)\n",
    "Output: \n",
    "* npy files for the stability values and regression weights at different levels of L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e650619",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7b28d",
   "metadata": {},
   "source": [
    "### define helper functions <a class=\"anchor\" id=\"define-helper-functions-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of bootstraps\n",
    "start = 0\n",
    "T = 100\n",
    "\n",
    "LAMBDAS = list(map(lambda x : math.exp(float(x) / 2) , range(-8,20,1)))\n",
    "\n",
    "nonzero = lambda x : not (x < tolerance and x > - tolerance)\n",
    "\n",
    "# return a vector of feature weights\n",
    "def fitmodel(xs, ys, LAMBDA):\n",
    "    clf = LogisticRegression(penalty = 'l1', C = 1/LAMBDA, solver='liblinear')\n",
    "    clf.fit(xs, ys.ravel())\n",
    "    w = clf.coef_.ravel()\n",
    "    return w\n",
    "\n",
    "# return a list with a vector of feature weights and the intercept\n",
    "def fitmodelwi(xs, ys, LAMBDA):\n",
    "    clf = LogisticRegression(penalty = 'l1', C = 1/LAMBDA, solver='liblinear')\n",
    "    clf.fit(xs, ys.ravel())\n",
    "    w = clf.coef_.ravel()\n",
    "    i = clf.intercept_.ravel()\n",
    "    return [w,i]\n",
    "\n",
    "def reggraph(x, y):\n",
    "    '''\n",
    "    generate weights for a regularization graph of predicting y from x\n",
    "    '''\n",
    "    XY = getXY(ORIGsids, x, y)\n",
    "    xs = XY[0]\n",
    "    ys = XY[1]\n",
    "    tj = time.time()\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        ti = time.time()\n",
    "        print (\"time taken : \", ti - tj)\n",
    "        tj = ti\n",
    "        print (\"lambda : {}\".format(LAMBDA))\n",
    "        w = fitmodel(xs, ys, LAMBDA)\n",
    "        np.save(\"lr_origsids_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\", w)\n",
    "    return\n",
    "\n",
    "def reggraphinteractions(x, y, NFEATS):\n",
    "    '''\n",
    "    generate weights for a regularization graph of predicting y from x\n",
    "    using only the first NFEATS features of x\n",
    "    '''\n",
    "    # get x, only some features\n",
    "    ind_a = np.load(\"areas_areas_indices_\" + data + \".npy\") # 0 indexed\n",
    "    names = np.load('x_' + ET + filename + str(gamma) + '_interactions_names.npy')\n",
    "    featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "    featranknames = [names[f-1] for f in featrank]\n",
    "    ml = featrank[0: NFEATS]\n",
    "    xnew = subsetx(x,ml)\n",
    "    # get x, y\n",
    "    XY = getXY(ORIGsids, xnew, y)\n",
    "    xs = XY[0]\n",
    "    ys = XY[1]\n",
    "\n",
    "    tj = time.time()\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        ti = time.time()\n",
    "        print (\"time taken : \", ti - tj)\n",
    "        tj = ti\n",
    "        print (\"lambda : {}\".format(LAMBDA))\n",
    "        [w,i] = fitmodelwi(xs, ys, LAMBDA)\n",
    "        np.save(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma) + \"_overlaps\"+ str(overlaps)+  \"_weights.npy\", w)\n",
    "        np.save(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma) + \"_overlaps\"+ str(overlaps)+\"_intercept.npy\", i)\n",
    "    return\n",
    "\n",
    "def reggraphsaveinteractions(ET1, ET2, NFEATS):\n",
    "    '''\n",
    "    read the weights for the regularization graphs ET1 and ET2\n",
    "    plot the first NFEATS features of both\n",
    "    including the interactions\n",
    "    excluding the intercept\n",
    "    '''\n",
    "    \n",
    "    ind_a = np.load(\"areas_areas_indices_\" + data + \".npy\") # 0 indexed\n",
    "    names = np.load('x_' + ET + filename + str(gamma) + '_interactions_names.npy')\n",
    "    featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "    featranknames = [names[f-1] for f in featrank]\n",
    "    ml = featrank[0: NFEATS]\n",
    "    fn = featranknames[0: NFEATS]\n",
    "\n",
    "    weights1 = np.zeros(shape=(len(LAMBDAS), NFEATS))\n",
    "    weights2 = np.zeros(shape=(len(LAMBDAS), NFEATS))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        w1 = np.load(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ \"_\" + ET1 + filename + str(gamma)+ \"_overlaps\"+ str(overlaps)+\"_weights.npy\")\n",
    "        w2 = np.load(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ \"_\" + ET2 + filename + str(gamma)+ \"_overlaps\"+ str(overlaps)+\"_weights.npy\")\n",
    "        print (w1)\n",
    "        print (w2)\n",
    "        weights1[counter, :] = w1\n",
    "        weights2[counter, :] = w2\n",
    "        counter = counter + 1\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    NUM_COLORS = NFEATS\n",
    "    originalcycle = [cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "    doublescycle = []\n",
    "    for e in originalcycle:\n",
    "        doublescycle.append(e)\n",
    "        doublescycle.append(e)\n",
    "    markerdoublecycle = []\n",
    "    for _i in range(NFEATS):\n",
    "        markerdoublecycle.append('.')\n",
    "        markerdoublecycle.append('+')\n",
    "    doublefn = []\n",
    "    for featurename in fn:\n",
    "        doublefn.append(featurename + ' (' + ET1 + ')')\n",
    "        doublefn.append(featurename + ' (' + ET2 + ')')\n",
    "    ax.set_prop_cycle(color = doublescycle, marker = markerdoublecycle)\n",
    "    for f in range(NFEATS):\n",
    "        ax.plot(LAMBDAS, weights1[:,f]) # 'o'\n",
    "        ax.plot(LAMBDAS, weights2[:,f]) # '+'\n",
    "    ax.axhline(y=0, color='k')\n",
    "    plt.xscale('log')\n",
    "    plt.legend(doublefn, loc='upper right')\n",
    "    plt.title(\"Regularization Plot\")\n",
    "    plt.savefig(\"regplotsave.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def reggraphsaveinteractionsintercept(ET1, ET2, NFEATS):\n",
    "    '''\n",
    "    read the weights for the regularization graphs ET1 and ET2\n",
    "    plot the first NFEATS features of both\n",
    "    including the interactions\n",
    "    including the intercept\n",
    "    '''\n",
    "    ind_a = np.load(\"areas_areas_indices_\" + data + \".npy\") # 0 indexed\n",
    "    names = np.load('x_' + ET + filename + str(gamma) + '_interactions_names.npy')\n",
    "    featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "    featranknames = [names[f-1] for f in featrank]\n",
    "    ml = featrank[0: NFEATS]\n",
    "    fn = featranknames[0: NFEATS]\n",
    "\n",
    "    weights1 = np.zeros(shape=(len(LAMBDAS), NFEATS+1))\n",
    "    weights2 = np.zeros(shape=(len(LAMBDAS), NFEATS+1))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        i1 = np.load(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ \"_\" + ET1 + filename + str(gamma)+\"_intercept.npy\")\n",
    "        i2 = np.load(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ \"_\" + ET2 + filename + str(gamma)+\"_intercept.npy\")\n",
    "        w1 = np.load(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ \"_\" + ET1 + filename + str(gamma)+\"_weights.npy\")\n",
    "        w2 = np.load(\"lr_origsids_interactions_NFEATS\" + str(NFEATS) + \"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ \"_\" + ET2 + filename + str(gamma)+\"_weights.npy\")\n",
    "        print (w1)\n",
    "        print (w2)\n",
    "        weights1[counter, :] = np.concatenate((w1, i1))\n",
    "        weights2[counter, :] = np.concatenate((w2, i2))\n",
    "        counter = counter + 1\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    NUM_COLORS = NFEATS+1\n",
    "    originalcycle = [cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "    doublescycle = []\n",
    "    for e in originalcycle:\n",
    "        doublescycle.append(e)\n",
    "        doublescycle.append(e)\n",
    "    markerdoublecycle = []\n",
    "    for _i in range(NFEATS+1):\n",
    "        markerdoublecycle.append('.')\n",
    "        markerdoublecycle.append('+')\n",
    "    doublefn = []\n",
    "    for featurename in fn:\n",
    "        doublefn.append(featurename + ' (' + ET1 + ')')\n",
    "        doublefn.append(featurename + ' (' + ET2 + ')')\n",
    "    doublefn.append('intercept (' + ET1 + ')')\n",
    "    doublefn.append('intercept (' + ET2 + ')')\n",
    "    ax.set_prop_cycle(color = doublescycle, marker = markerdoublecycle)\n",
    "    for f in range(NFEATS+1):\n",
    "        ax.plot(LAMBDAS, weights1[:,f]) # 'o'\n",
    "        ax.plot(LAMBDAS, weights2[:,f]) # '+'\n",
    "    ax.axhline(y=0, color='k')\n",
    "    plt.xscale('log')\n",
    "    plt.legend(doublefn, loc='upper right')\n",
    "    plt.title(\"Regularization Plot\")\n",
    "    plt.savefig(\"regplotsave.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def reggraphsave(x, y):\n",
    "    '''\n",
    "    read the weights for and plot the regularization graph of predicting y from x\n",
    "    '''\n",
    "    XY = getXY(ORIGsids, x, y)\n",
    "    xs = XY[0]\n",
    "    ys = XY[1]\n",
    "    weights = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        w = np.load(\"lr_origsids_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\")\n",
    "        weights[counter, :] = w\n",
    "        counter = counter + 1\n",
    "    for f in range(len(names)):\n",
    "        plt.plot(LAMBDAS, weights[:,f])\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.legend(names, loc='upper right')\n",
    "    plt.title(\"Regularization Plot\")\n",
    "    plt.savefig(\"regplotsave.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def reggraphsavefeat(x, y, l):\n",
    "    '''\n",
    "    read the weights for and plot the regularization graph of predicting y from x\n",
    "    only for features in list l\n",
    "    '''\n",
    "    XY = getXY(ORIGsids, x, y)\n",
    "    xs = XY[0]\n",
    "    ys = XY[1]\n",
    "    weights = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        w = np.load(\"lr_origsids_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\")\n",
    "        weights[counter, :] = w\n",
    "        counter = counter + 1\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    NUM_COLORS = len(l)\n",
    "    ax.set_color_cycle([cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "    for f in l:\n",
    "        ax.plot(LAMBDAS, weights[:,f])\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.legend([names[l[i]] for i in range(len(l))], loc='lower right')\n",
    "    plt.title(\"Regularization Plot\")\n",
    "    plt.savefig(\"regplotsave.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def stabgraph(x, y):\n",
    "    '''\n",
    "    generate the subsamples of a stability graph of predicting y from x\n",
    "    '''\n",
    "    index = 0\n",
    "    tj = time.time()\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        counts = np.zeros(8)\n",
    "        for t in range(start, start + T):\n",
    "            ti = time.time()\n",
    "            print (\"time taken : \", ti - tj)\n",
    "            tj = ti\n",
    "            index = index + 1\n",
    "            print (\"lambda : {} - Subsample : {}\".format(LAMBDA, t))\n",
    "            ns = int(len(ORIGsids) / 2)\n",
    "            np.random.seed(t) ; sids = np.random.choice(ORIGsids, ns, replace=False)\n",
    "            XY = getXY(sids, x, y)\n",
    "            xs = XY[0]\n",
    "            ys = XY[1]\n",
    "            w = fitmodel(xs, ys, LAMBDA)\n",
    "            np.save(\"lr2_sids_subsample\"+str(t)+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\", w)\n",
    "    return\n",
    "\n",
    "\n",
    "def stabgraphsave(x, y):\n",
    "    '''\n",
    "    use the subsamples of a stability graph of predicting y from x\n",
    "    to save and plot the ratios with a legend\n",
    "    '''\n",
    "    ratios = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        all_signs = np.zeros(shape=(T, nf))\n",
    "        counts = np.zeros(nf)\n",
    "        for t in range(start, start+T):\n",
    "            w = np.load(\"lr2_sids_subsample\"+str(t)+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\")\n",
    "            all_signs[t - start, :] = list(map(np.sign, w))\n",
    "        pos_sum = np.sum(all_signs == 1, 0)\n",
    "        neg_sum = np.sum(all_signs == -1, 0)\n",
    "        counts = np.maximum(pos_sum, neg_sum)\n",
    "        ratios[counter, :] = list(map(lambda x : float(x) / float(T), counts))\n",
    "        counter = counter + 1\n",
    "    plt.figure()\n",
    "    for f in range(len(names)):\n",
    "        plt.plot(LAMBDAS, ratios[:,f])\n",
    "    np.save(\"lr_sids_ratios_for_alllambdas_of_\" + ET + suffix + filename + str(gamma) + \".npy\", ratios)\n",
    "    np.save(\"lr_sids_lambdas_for_alllambdas_of_\" + ET + suffix + filename + str(gamma) + \".npy\", LAMBDAS)\n",
    "    plt.legend(names, loc='lower left')\n",
    "    plt.title(\"Stability Plot\")\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(\"stabgplotsave.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def stabgraphsavenolegend(x, y):\n",
    "    '''\n",
    "    use the subsamples of a stability graph of predicting y from x\n",
    "    to save and plot the ratios with a legend\n",
    "    '''\n",
    "    ratios = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        all_signs = np.zeros(shape=(T, nf))\n",
    "        counts = np.zeros(nf)\n",
    "        for t in range(start, start+T):\n",
    "            w = np.load(\"lr2_sids_subsample\"+str(t)+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\")\n",
    "            all_signs[t - start, :] = list(map(np.sign, w))\n",
    "        pos_sum = np.sum(all_signs == 1, 0)\n",
    "        neg_sum = np.sum(all_signs == -1, 0)\n",
    "        counts = np.maximum(pos_sum, neg_sum)\n",
    "        ratios[counter, :] = list(map(lambda x : float(x) / float(T), counts))\n",
    "        counter = counter + 1\n",
    "    plt.figure()\n",
    "    for f in range(len(names)):\n",
    "        plt.plot(LAMBDAS, ratios[:,f])\n",
    "    np.save(\"lr_sids_ratios_for_alllambdas_of_\" + ET + suffix + filename + str(gamma)+ \".npy\", ratios)\n",
    "    np.save(\"lr_sids_lambdas_for_alllambdas_of_\" + ET + suffix + filename + str(gamma)+ \".npy\", LAMBDAS)\n",
    "    plt.title(\"Stability Plot\")\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(\"stabgplotsave.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def stabgraphsavefeat(x, y, l):\n",
    "    '''\n",
    "    use the subsamples of a stability graph of predicting y from x\n",
    "    to save and plot ratios with only for x's with indices in the list l (0-indexed)\n",
    "    '''\n",
    "    ratios = np.zeros(shape=(len(LAMBDAS), nf))\n",
    "    counter = 0\n",
    "    for LAMBDA in LAMBDAS:\n",
    "        all_signs = np.zeros(shape=(T, nf))\n",
    "        counts = np.zeros(nf)\n",
    "        for t in range(start, start+T):\n",
    "            w = np.load(\"lr2_sids_subsample\"+str(t)+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+ suffix + filename + str(gamma)+\"_weights.npy\")\n",
    "            all_signs[t - start, :] = list(map(np.sign, w))\n",
    "        pos_sum = np.sum(all_signs == 1, 0)\n",
    "        neg_sum = np.sum(all_signs == -1, 0)\n",
    "        counts = np.maximum(pos_sum, neg_sum)\n",
    "        ratios[counter, :] = list(map(lambda x : float(x) / float(T), counts))\n",
    "        counter = counter + 1\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    NUM_COLORS = len(l)\n",
    "    ax.set_color_cycle([cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "    for f in l:\n",
    "        ax.plot(LAMBDAS, ratios[:,f])\n",
    "    namesl = [names[l[i]] for i in range(len(l))]\n",
    "    np.save(\"lr_sids_ratios_for_alllambdas_of_\" + ET + suffix + filename + str(gamma)+ \".npy\", ratios)\n",
    "    np.save(\"lr_sids_lambdas_for_alllambdas_of_\" + ET + suffix + filename + str(gamma)+ \".npy\", LAMBDAS)\n",
    "    plt.legend(namesl, loc='lower left')\n",
    "    plt.title(\"Stability Plot\")\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(\"stabgplotsave.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d86880",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716041a",
   "metadata": {},
   "source": [
    "### create plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f47ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = False\n",
    "\n",
    "epsilon=0.0\n",
    "tolerance = 0.0005\n",
    "it = \"no interactions\"\n",
    "\n",
    "# where to get features from\n",
    "data = \"random\"\n",
    "NFEATS = 7\n",
    "\n",
    "# where to build model\n",
    "ET = \"random\"\n",
    "suffix = '_' + ET\n",
    "filename = '_normed_standardized_Actual_EV'\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "if it == \"no interactions\":\n",
    "    x = np.genfromtxt(\"x\" + suffix + filename + str(gamma) + \".csv\", delimiter=\",\")\n",
    "    i = getcolumnnames(\"x\" + suffix + filename + str(gamma) + \".csv\")\n",
    "    names = i[1:]\n",
    "    print (names)\n",
    "    nf = len(names)\n",
    "elif it == \"with interactions\":\n",
    "    if overlaps:\n",
    "        x = np.genfromtxt('x' + suffix + filename + str(gamma) + '_interactions_stdz_overlap.csv', delimiter=\",\")\n",
    "    else:\n",
    "        x = np.genfromtxt('x' + suffix + filename + str(gamma) + '_interactions_stdz.csv', delimiter=\",\")\n",
    "    names = np.load('x' + suffix + filename + str(gamma) + '_interactions_names.npy')\n",
    "    nf = len(names)\n",
    "else:\n",
    "    print (\"it parameter was not recognized\")\n",
    "\n",
    "if overlaps:\n",
    "    y = np.genfromtxt(\"y\" + suffix + filename + str(gamma) + \"_overlap.csv\", delimiter=\",\")\n",
    "else:\n",
    "    y = np.genfromtxt(\"y\" + suffix + filename + str(gamma) + \".csv\", delimiter=\",\")\n",
    "# select gps\n",
    "if ET == \"1block\":\n",
    "    gps = 'gps.csv'\n",
    "elif ET == \"3block\":\n",
    "    gps = 'gps_3block.csv'\n",
    "elif ET == \"random\":\n",
    "    gps = 'gps_random.csv'\n",
    "else:\n",
    "    print (\"error\")\n",
    "sf = np.genfromtxt(gps, delimiter=\",\", usecols=np.arange(0,2)) # subject file\n",
    "ORIGsids = np.array(sf[1:,1]) # subject ids\n",
    "n = len(ORIGsids)\n",
    "\n",
    "stabgraph(x, y)\n",
    "stabgraphsavenolegend(x, y)\n",
    "reggraph(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e6d4c",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67b35f",
   "metadata": {},
   "source": [
    "## Average Stability Calculations\n",
    "\n",
    "Start: \n",
    "* npy files for the stability values at different levels of L1 regularization\n",
    "\n",
    "Output: \n",
    "* csv file with the names and average stabilities of all features (averaged across lambda values), npy file with ranking of indices of the top most stable (on average) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf189a",
   "metadata": {},
   "source": [
    "### save stability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e27aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET = \"random\"\n",
    "it = \"no interactions\"\n",
    "filename = '_normed_standardized_Actual_EV'\n",
    "# filename = '_normed_EV'\n",
    "gamma = 0.5\n",
    "\n",
    "suffix = \"_\"+ET\n",
    "\n",
    "ratios = np.load(\"lr_sids_ratios_for_alllambdas_of_\" + ET + suffix + filename + str(gamma) + \".npy\")\n",
    "print (ratios.shape)\n",
    "LAMBDAS = np.load(\"lr_sids_lambdas_for_alllambdas_of_\" + ET + suffix + filename + str(gamma) + \".npy\")\n",
    "print (LAMBDAS.shape)\n",
    "\n",
    "\n",
    "if it == \"no interactions\":\n",
    "    i = getcolumnnames(\"x_\" + ET + filename + str(gamma) + \".csv\")\n",
    "    names = i[1:]\n",
    "elif it == \"with interactions\":\n",
    "    print (\"file : \" + 'x' + suffix + filename + str(gamma) + '_interactions_names.npy')\n",
    "    names = np.load('x' + suffix + filename + str(gamma) + '_interactions_names.npy')\n",
    "    print (names)\n",
    "else:\n",
    "    print (\"it parameter was not recognized\")\n",
    "\n",
    "# compute area\n",
    "# x is a list of increments along the x axis\n",
    "# y is a list of highs along each increment\n",
    "# left trapezoidal rules\n",
    "def ca(x,y):\n",
    "    area = 0.0\n",
    "    for i in range(len(x) - 1):\n",
    "        dx = x[i+1] - x[i]\n",
    "        dy = y[i+1] - y[i]\n",
    "        area = area + dx * y[i+1] + dx * dy / 2\n",
    "    maxarea = max(y) * (x[-1] - x[0]) # use this if you want to normalize\n",
    "    area = area\n",
    "    return area\n",
    "\n",
    "areas = np.zeros(shape=len(names))\n",
    "maxes = np.zeros(shape=len(names))\n",
    "with open('areas_' + ET + suffix + filename + str(gamma) + '.csv', 'a') as file:\n",
    "    filewriter = csv.writer(file, delimiter=',')\n",
    "    filewriter.writerow(['index', 'name','area','max'])\n",
    "    counter = 0\n",
    "    for f in range(len(names)):\n",
    "        fname = names[f]\n",
    "        fratios = ratios[:,f]\n",
    "        fa = ca(LAMBDAS, fratios)\n",
    "        fm = np.max(fratios)\n",
    "        filewriter.writerow([counter, fname, fa, fm])\n",
    "        areas[counter] = fa\n",
    "        maxes[counter] = fm\n",
    "        counter = counter + 1\n",
    "    np.save(\"areas_areas_\" + ET + suffix + filename + str(gamma) + \".npy\", areas)\n",
    "    np.save(\"areas_maxes_\" + ET + suffix + filename + str(gamma) + \".npy\", maxes)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f572",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7555e0",
   "metadata": {},
   "source": [
    "### save indicies of stability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET = \"random\"\n",
    "filename = '_normed_standardized_Actual_EV'\n",
    "gamma = 0.5\n",
    "suffix = \"_\"+ET\n",
    "\n",
    "areas = np.load(\"areas_areas_\" + ET + suffix + filename + str(gamma)+ \".npy\")\n",
    "maxes = np.load(\"areas_maxes_\" + ET + suffix + filename + str(gamma)+ \".npy\")\n",
    "# sort indices based on largest to smallest area\n",
    "ind_a = np.flip(np.argsort(areas))\n",
    "# sort indices based on largest to smallest max stability\n",
    "ind_m = np.flip(np.argsort(maxes))\n",
    "\n",
    "np.save(\"areas_areas_indices_\" + ET + suffix + filename + str(gamma)+ \".npy\", ind_a)\n",
    "np.save(\"areas_maxes_indices_\" + ET + suffix + filename + str(gamma)+ \".npy\", ind_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfb1da",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca561cc",
   "metadata": {},
   "source": [
    "## Picking Number of Most Stable Features\n",
    "\n",
    "Start: \n",
    "* ranking of indices of most stable features\n",
    "\n",
    "Output: \n",
    "* Graph of LOOCV accuracy of models trained on 0 features, 1 top most stable feature, top 2 most stable features, top 3 most stable features, ..., all the way to 45 features (9 main features + 36 interactions). \n",
    "* We can pick a sufficient number of features by seeing which # of features achieves the global maximum LOOCV accuracy (by leaving out one subject instead of one trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081234d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = \"no interactions\"\n",
    "\n",
    "ET = \"random\"\n",
    "suffix = '_' + ET\n",
    "filename = '_normed_standardized_Actual_EV'\n",
    "gamma = 0.5\n",
    "\n",
    "# x and y myst both have a subject_id column\n",
    "y = np.genfromtxt(\"y\" + suffix + filename + str(gamma) + \".csv\", delimiter=\",\")\n",
    "sf = np.genfromtxt('gps' + suffix + '.csv', delimiter=\",\", usecols=np.arange(0,2)) # subject file\n",
    "ORIGsids = np.array(sf[1:,1]) # subject ids\n",
    "n = len(ORIGsids)\n",
    "\n",
    "# number of bootstraps\n",
    "T = 100\n",
    "graphtype = \"m\" \n",
    "\n",
    "\n",
    "# x and y both have a subject_id column\n",
    "# return accuracy of a logistic regression model\n",
    "# through LOOCV\n",
    "def loocv_acc(sids, x, y):\n",
    "    # obsvs = np.zeros(shape=(n,nTRIALS))\n",
    "    # preds = np.zeros(shape=(n,nTRIALS))\n",
    "    obsvs = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    print (preds)\n",
    "    if len(x) == 0:\n",
    "        for f in range(n):\n",
    "            test_y = subjY(sids[f], y)\n",
    "            train_y = notsubjY(sids[f], y)\n",
    "            ym = np.mean(train_y )\n",
    "            yb = ym > 0.5\n",
    "            test_p = [yb for _i in range(len(test_y))]\n",
    "            test_prob = [ym for _i in range(len(test_y))]\n",
    "            preds.extend(test_p)\n",
    "            obsvs.extend(test_y)\n",
    "            probs.extend(test_prob)\n",
    "    else:\n",
    "        for f in range(n):\n",
    "            if sids[f] > 10:\n",
    "                [test_x, test_y] = subjXY(sids[f], x, y)\n",
    "                [train_x, train_y] = notsubjXY(sids[f], x, y)\n",
    "                if x.shape[1] == 2:\n",
    "                    test_x = test_x.reshape(len(test_x),1)\n",
    "                    train_x = train_x.reshape(len(train_x),1)\n",
    "                mf = LogisticRegression(solver='liblinear') # penalty = 'l1', C = 1/LAMBDA,\n",
    "                mf.fit(train_x, train_y)\n",
    "                test_p = mf.predict(test_x)\n",
    "                temp = mf.predict_proba(test_x)\n",
    "                test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "                preds.extend(test_p)\n",
    "                obsvs.extend(test_y)\n",
    "                probs.extend(test_prob)\n",
    "    predsbool = np.reshape(np.array(preds).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(obsvs).astype(bool), (-1))\n",
    "    probs = np.reshape(np.array(probs), (-1))\n",
    "    bacc = balancedacc(obsvsbool, probs, 0.5)\n",
    "    print (\"balanced accuracy is : \", bacc)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, probs)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    return [bacc, auc]\n",
    "\n",
    "if it == \"no interactions\":\n",
    "    x = np.genfromtxt(\"x\" + suffix + filename + str(gamma) + \".csv\", delimiter=\",\")\n",
    "    i = getcolumnnames(\"x\" + suffix + filename + str(gamma) + \".csv\")\n",
    "    names = i[1:]\n",
    "    # indices in x of the most stable features\n",
    "    ind_a = np.load(\"areas_areas_indices_\" + ET + suffix + filename + str(gamma) + \".npy\") # 0 indexed\n",
    "    \n",
    "    featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "    featranknames = [names[f-1] for f in featrank]\n",
    "elif it == \"with interactions\":\n",
    "    x = np.genfromtxt('x' + suffix + filename + str(gamma) + '_interactions.csv', delimiter=\",\")\n",
    "    names = np.load('x' + suffix + filename + str(gamma) + '_interactions_names.npy')\n",
    "    ind_a = np.load(\"areas_areas_indices_\"+ET+ suffix + filename + str(gamma)+\".npy\")\n",
    "    featrank = [ind_a[i]+1 for i in range(len(ind_a))]\n",
    "    featranknames = [names[f-1] for f in featrank]\n",
    "else:\n",
    "    print (\"it parameter was not recognized\")\n",
    "\n",
    "\n",
    "baccs = np.zeros(shape=(T, len(featrank) + 1))\n",
    "aucs = np.zeros(shape=(T, len(featrank) + 1))\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        sids = ORIGsids\n",
    "    else:\n",
    "        np.random.seed(t) ; sids = np.random.choice(ORIGsids, n, replace=True)\n",
    "    [mbacc,auc] = loocv_acc(sids, [], y)\n",
    "    baccs[t,0] = mbacc\n",
    "    aucs[t,0] = auc\n",
    "    for m in range(len(featrank)):\n",
    "        ml = featrank[0: m+1]\n",
    "        xnew = subsetx(x,ml)\n",
    "        [mbacc, mauc] = loocv_acc(sids, xnew, y)\n",
    "        baccs[t, m+1] = mbacc\n",
    "        aucs[t, m+1] = mauc\n",
    "np.save(\"bmaccs_\" + suffix + filename + str(gamma) + \".npy\", bmaccs)\n",
    "np.save(\"aucs_\" + suffix + filename + str(gamma) + \".npy\", aucs)\n",
    "accs = np.load(\"accs_\" + suffix + filename + str(gamma) + \".npy\")\n",
    "print (accs)\n",
    "\n",
    "baccs = np.load(\"baccs_\" + suffix + filename + str(gamma) + \".npy\")\n",
    "aucs = np.load(\"aucs_\" + suffix + filename + str(gamma) + \".npy\")\n",
    "\n",
    "print (\"experiment type is : \", ET)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Max Balanced Accuracy in LOOCV\")\n",
    "x = [\"Baseline\"]\n",
    "for name in featranknames:\n",
    "    x.append(\"+ \" + name)\n",
    "\n",
    "if graphtype == \"all\":\n",
    "    ### # all bootstraps\n",
    "    for t in range(T):\n",
    "        plt.plot(x, baccs[t,:])\n",
    "    plt.title(\"Each line is for a bootstrap resampling T = \" + str(T) + \" bootstrap resamplings\")\n",
    "elif graphtype == \"m\":\n",
    "    ### # just the means\n",
    "    plt.plot(x, baccs.mean(axis=0))\n",
    "    for i, txt in enumerate(baccs.mean(axis=0)):\n",
    "        plt.annotate(round(txt,2), (x[i], baccs.mean(axis=0)[i]))\n",
    "    plt.title(\"Mean Balanced Accuracy for T = \" + str(T) + \" bootstrap resamplings for \" + ET)\n",
    "elif graphtype == \"ms\":\n",
    "    ### # just the means and stds\n",
    "    plt.errorbar(x, baccs.mean(axis=0), yerr = baccs.std(axis=0))\n",
    "    for i, txt in enumerate(baccs.mean(axis=0)):\n",
    "        plt.annotate(round(txt,2), (x[i], baccs.mean(axis=0)[i]))\n",
    "    plt.title(\"Means and STDs for T = \" + str(T) + \" bootstrap resamplings for \" + ET)\n",
    "elif graphtype == \"one\":\n",
    "    plt.plot(x, baccs[0,:])\n",
    "    for i, txt in enumerate(baccs[0,:]):\n",
    "        plt.annotate(round(txt,2), (x[i], baccs[0,:][i]))\n",
    "    plt.title(\"Only for Original Sample\")\n",
    "else:\n",
    "    print (\"graphtype parameter not recognized\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"AUC in LOOCV\")\n",
    "plt.plot(x,aucs.mean(axis=0))\n",
    "for i, txt in enumerate(aucs.mean(axis=0)):\n",
    "    plt.annotate(round(txt,2), (x[i], aucs.mean(axis=0)[i]))\n",
    "plt.title(\"Means and STDs for T = \" + str(T) + \" bootstrap resamplings for \" + ET)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f780185",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c8b4c",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12fbaf",
   "metadata": {},
   "source": [
    "## Estimate LOOCV accuracy (leaving out one subject's 90 trials)\n",
    "\n",
    "Output: \n",
    "* csv file of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runmodel(gps, model, NFEATS, x, y, SEED, LAMBDA):\n",
    "    '''\n",
    "    Generate and evaluate the predictions of a model with the following inputs:\n",
    "\n",
    "    gps is the name of the per subject data gps file (see above)\n",
    "    model is a string denoting the type of model\n",
    "    x is an array of features, with the 0th column as subject ids\n",
    "    y is an array of targets, with the 0th column as subject ids\n",
    "    '''\n",
    "    \n",
    "    #ind_a = np.load(\"areas_areas_indices_\" + data + \".npy\") # 0 indexed\n",
    "    ind_a = np.load(\"areas_areas_indices_\" + ET + suffix + filename + str(gamma) + \".npy\") # 0 indexed\n",
    "    featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "    featranknames = [names[f-1] for f in featrank]\n",
    "    ml = featrank[0: NFEATS]\n",
    "    xnew = subsetx(x,ml)\n",
    "\n",
    "    sf = np.genfromtxt(gps, delimiter=\",\",usecols=np.arange(0,2)) # subject file\n",
    "    ORIGsids = np.array(sf[1:,1]) # subject ids\n",
    "    ORIGsids = list(filter(lambda x: x > 10, ORIGsids))\n",
    "    n = len(ORIGsids)\n",
    "    if SEED == 0:\n",
    "        sids = ORIGsids\n",
    "    else:\n",
    "        np.random.seed(SEED) ; sids = np.random.choice(ORIGsids, n)\n",
    "    obsvs = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    subjs = []\n",
    "    for fold in range(n):\n",
    "        if model == model == \"topfeatures\":\n",
    "            [test_x, test_y] = subjXY(sids[fold], xnew, y)\n",
    "            [train_x, train_y] = notsubjXY(sids[fold], xnew, y)\n",
    "            mf = LogisticRegression(solver='liblinear', penalty = 'l1', C = 1/LAMBDA)\n",
    "            mf.fit(train_x, train_y)\n",
    "            test_pred = mf.predict(test_x)\n",
    "            temp = mf.predict_proba(test_x)\n",
    "            test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "        elif model == \"allfeatures\"  or model == \"top2features\" or model == \"top3features\" or model == \"top5features\" or model == \"top10features\":\n",
    "            [test_x, test_y] = subjXY(sids[fold], x, y)\n",
    "            [train_x, train_y] = notsubjXY(sids[fold], x, y)\n",
    "            mf = LogisticRegression(solver='liblinear') # penalty = 'l1', C = 1/LAMBDA,\n",
    "            mf.fit(train_x, train_y)\n",
    "            test_pred = mf.predict(test_x)\n",
    "            temp = mf.predict_proba(test_x)\n",
    "            test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "        elif model == \"baseline\":\n",
    "            test_y = subjY(sids[fold], y)\n",
    "            train_y = notsubjY(sids[fold], y)\n",
    "            ym = np.mean(train_y)\n",
    "            yb = ym > 0.5\n",
    "            test_pred = [yb for _i in range(len(test_y))]\n",
    "            if yb == 1:\n",
    "                print (\"predict 1 always with probability \", ym)\n",
    "                test_prob = [ym for _i in range(len(test_y))]\n",
    "            else:\n",
    "                print (\"predict 0 always with probability \", 1- ym)\n",
    "                test_prob = [1- ym for _i in range(len(test_y))]\n",
    "        else:\n",
    "            print (\"model parameter not recognized\")\n",
    "        preds.append(test_pred)\n",
    "        obsvs.append(test_y)\n",
    "        probs.append(test_prob)\n",
    "        subjs.append([sids[fold] for i in range(len(test_pred))])\n",
    "    subjs = np.array(subjs).reshape(-1)\n",
    "    probs = np.array(probs).reshape(-1)\n",
    "    predsbool = np.reshape(np.array(preds).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(obsvs).astype(bool), (-1))\n",
    "    bacc3 = balancedacc(obsvsbool, probs, 0.3)\n",
    "    bacc5 = balancedacc(obsvsbool, probs, 0.5)\n",
    "    bacc7 = balancedacc(obsvsbool, probs, 0.7)\n",
    "    mbacc = maxbacc(obsvsbool, probs)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, probs)\n",
    "    print (\"\")\n",
    "    print (\"for model \", model)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    sauc = subjauc(subjs, obsvsbool, probs)\n",
    "    with open('predictions_'+data+'.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow([\"subjid\", \"truth\", \"prob\"])\n",
    "        writer.writerows(zip(subjs,obsvsbool, probs))\n",
    "    writeFile.close()\n",
    "    ll = sklearn.metrics.log_loss(np.squeeze(obsvsbool), np.squeeze(probs)) / float(len(probs))\n",
    "    print (\"balanced accuracy is : \", round(bacc5,4))\n",
    "    print (\"auc is : \", round(auc,4))\n",
    "    print (\"log loss is : \", '%.4E' % Decimal(ll))\n",
    "    return [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll]\n",
    "\n",
    "## Decide where features are coming from\n",
    "ET = \"random\"\n",
    "specificNFEATS = 45\n",
    "ETsuffix = \"_\" + ET\n",
    "## Decide what data to test in\n",
    "data = \"random\"\n",
    "suffix = \"_\"+data\n",
    "\n",
    "# select gps\n",
    "if data == \"1block\":\n",
    "    gps = 'gps.csv'\n",
    "elif data == \"3block\":\n",
    "    gps = 'gps_3block.csv'\n",
    "elif data == \"random\":\n",
    "    gps = 'gps_random.csv'\n",
    "else:\n",
    "    print (\"error\")\n",
    "\n",
    "filename = '_normed_standardized_Actual_EV' \n",
    "gamma = 0.5\n",
    "LAMBDA = 1.0\n",
    "LAMBDAS = [0.1, 1.0, 10.0]\n",
    "nSEEDS = 3\n",
    "\n",
    "ind_a = np.load(\"areas_areas_indices_\" + ET + ETsuffix + filename + str(gamma)+ \".npy\")\n",
    "names = np.load('x_' + ET + filename + str(gamma) + '_interactions_names.npy')\n",
    "\n",
    "\n",
    "models = [\"topfeatures\"] \n",
    "NFEATSs = [len(ind_a)] \n",
    "\n",
    "x = np.genfromtxt(\"x\" + suffix + filename + str(gamma) + \"_interactions.csv\", delimiter=\",\")\n",
    "y = np.genfromtxt(\"y\" + suffix + filename + str(gamma) + \".csv\", delimiter=\",\")\n",
    "\n",
    "with open('lr_results.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    results = []\n",
    "    for SEED in range(nSEEDS):\n",
    "        for LAMBDA in LAMBDAS:\n",
    "            for model, NFEATS in zip(models, NFEATSs):\n",
    "                [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll] = runmodel(gps, model, NFEATS, x, y, SEED, LAMBDA)\n",
    "                writer.writerow([bacc5, str(LAMBDA), str(SEED)])\n",
    "                results.append([bacc5, str(LAMBDA), str(SEED)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18573b0",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524718f",
   "metadata": {},
   "source": [
    "## Estimate Transfer Accuracy\n",
    "\n",
    "Output\n",
    "* csv file of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99203bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to get your features from\n",
    "ET = \"random\"\n",
    "specificNFEATS = 7\n",
    "it = \"no interactions\"\n",
    "filename = '_normed_standardized_Actual_EV'\n",
    "gamma = 0.5\n",
    "\n",
    "suffix = \"_\"+ET\n",
    "\n",
    "print (\"areas_areas_indices_\" + ET + suffix + filename + str(gamma)+ \".npy\")\n",
    "ind_a = np.load(\"areas_areas_indices_\" + ET + suffix + filename + str(gamma)+ \".npy\") # 0 indexed\n",
    "featrank = [ind + 1 for ind in ind_a] # 1 indexed\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "\n",
    "\n",
    "# train model on only subjects that overlap between train and test set\n",
    "# test model on only subjects also in train set\n",
    "def runmodeloverlap2(model, traintype, testtype, NFEATS):\n",
    "\n",
    "    # find overlap\n",
    "    trainsubj = np.genfromtxt('y'+str(trainsuffix)+filename1+'.csv', delimiter=',')[1:,0]\n",
    "    testsubj = np.genfromtxt('y'+str(testsuffix)+filename1+'.csv', delimiter=',')[1:,0]\n",
    "\n",
    "    overlapsubj = list(set(trainsubj) & set(testsubj))\n",
    "    print (str(len(overlapsubj)) + \" overlap subjects between \" + traintype + \" and \" + testtype)\n",
    "    print (overlapsubj)\n",
    "\n",
    "    # create gps file\n",
    "    rows = zip(range(len(overlapsubj)), overlapsubj)\n",
    "    with open('gps_overlap_'+ traintype + '_' + testtype + '.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(['index', 'subject_id'])\n",
    "        writer.writerows(rows)\n",
    "    writeFile.close()\n",
    "\n",
    "    train_y = subsetfilesubj('y'+str(trainsuffix)+filename1, overlapsubj)\n",
    "    test_y = subsetfilesubj('y'+str(testsuffix)+filename1, overlapsubj)\n",
    "    subjs = subsetfilesubjSUBJS('y'+str(testsuffix)+filename1, overlapsubj)\n",
    "\n",
    "    # define variables\n",
    "    np.save(\"test_obsv.npy\", test_y)\n",
    "    if model == \"topfeatures\":\n",
    "        train_x = strip(subsetxfile('x'+str(trainsuffix)+filename2+'_overlap_frm'+ testtype +'.csv', featrank[0:NFEATS]))\n",
    "        _temp = subsetfilesubj('x'+str(testsuffix)+filename2, overlapsubj)\n",
    "        test_x = strip(subsetxfile('x'+str(testsuffix) +filename2+ '_overlap_frm'+ traintype +'.csv', featrank[0:NFEATS]))\n",
    "    elif model == \"baseline\":\n",
    "        pass\n",
    "    else:\n",
    "        print (\"model parameter not recognized\")\n",
    "\n",
    "    # create model and start predictions\n",
    "    if model == \"topfeatures\":\n",
    "        mf = LogisticRegression(solver='liblinear')\n",
    "        mf.fit(train_x, train_y)\n",
    "        test_pred = mf.predict(test_x)\n",
    "        temp = mf.predict_proba(test_x)\n",
    "        test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "    elif model == \"baseline\":\n",
    "        ym = np.mean(train_y)\n",
    "        yb = ym > 0.5\n",
    "        test_pred = [yb for _i in range(len(test_y))]\n",
    "        if yb == 1:\n",
    "            print (\"predict 1 always with probability \", ym)\n",
    "            test_prob = [ym for _i in range(len(test_y))]\n",
    "        else:\n",
    "            print (\"predict 0 always with probability \", 1- ym)\n",
    "            test_prob = [1- ym for _i in range(len(test_y))]\n",
    "    else:\n",
    "        print (\"model parameter not recognized\")\n",
    "\n",
    "    test_prob = np.reshape(np.array(test_prob), (-1))\n",
    "    predsbool = np.reshape(np.array(test_pred).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(test_y).astype(bool), (-1))\n",
    "\n",
    "    bacc3 = balancedacc(obsvsbool, test_prob, 0.3)\n",
    "    bacc5 = balancedacc(obsvsbool, test_prob, 0.5)\n",
    "    bacc7 = balancedacc(obsvsbool, test_prob, 0.7)\n",
    "    mbacc = maxbacc(obsvsbool, test_prob)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, test_prob)\n",
    "    print (\"\")\n",
    "    print (\"for model \", model)\n",
    "    sauc = subjauc(subjs, obsvsbool, test_prob)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    ll = sklearn.metrics.log_loss(np.squeeze(obsvsbool), np.squeeze(test_prob)) / float(len(test_prob))\n",
    "    print (\"balanced accuracy is : \", round(bacc5,4))\n",
    "    print (\"auc is : \", round(auc,4))\n",
    "    print (\"log loss is : \", '%.4E' % Decimal(ll))\n",
    "    return [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll]\n",
    "\n",
    "\n",
    "# train model on full train set\n",
    "# test model on only subjects also in train set\n",
    "def runmodeloverlap(model, traintype, testtype, NFEATS):\n",
    "    # define variables\n",
    "    train_y = stripfile('y'+str(trainsuffix)+filename1+'.csv')\n",
    "\n",
    "    # find overlap\n",
    "    trainsubj = np.genfromtxt('y'+str(trainsuffix)+filename1+'.csv', delimiter=',')[1:,0]\n",
    "    testsubj = np.genfromtxt('y'+str(testsuffix)+filename1+'.csv', delimiter=',')[1:,0]\n",
    "\n",
    "    overlapsubj = list(set(trainsubj) & set(testsubj))\n",
    "    print (str(len(overlapsubj)) + \" overlap subjects between \" + traintype + \" and \" + testtype)\n",
    "    print (overlapsubj)\n",
    "\n",
    "    # create gps file\n",
    "    rows = zip(range(len(overlapsubj)), overlapsubj)\n",
    "    with open('gps_overlap_'+ traintype + '_' + testtype + '.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(['index', 'subject_id'])\n",
    "        writer.writerows(rows)\n",
    "    writeFile.close()\n",
    "\n",
    "    test_y = subsetfilesubj('y'+str(testsuffix)+filename1, overlapsubj)\n",
    "    subjs = subsetfilesubjSUBJS('y'+str(testsuffix)+filename1, overlapsubj)\n",
    "\n",
    "    # define variables\n",
    "    np.save(\"test_obsv.npy\", test_y)\n",
    "    if model == \"topfeatures\":\n",
    "        train_x = strip(subsetxfile('x'+str(trainsuffix)+filename2+'.csv', featrank[0:NFEATS]))\n",
    "        _temp = subsetfilesubj('x'+str(testsuffix)+filename2, overlapsubj)\n",
    "        test_x = strip(subsetxfile('x'+str(testsuffix) +filename2+ '_overlap_frm'+ traintype +'.csv', featrank[0:NFEATS]))\n",
    "    elif model == \"baseline\":\n",
    "        pass\n",
    "    else:\n",
    "        print (\"model parameter not recognized\")\n",
    "\n",
    "    # create model and start predictions\n",
    "    if model == \"topfeatures\":\n",
    "        mf = LogisticRegression(solver='liblinear')\n",
    "        mf.fit(train_x, train_y)\n",
    "        test_pred = mf.predict(test_x)\n",
    "        temp = mf.predict_proba(test_x)\n",
    "        test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "    elif model == \"baseline\":\n",
    "        ym = np.mean(train_y)\n",
    "        yb = ym > 0.5\n",
    "        test_pred = [yb for _i in range(len(test_y))]\n",
    "        if yb == 1:\n",
    "            print (\"predict 1 always with probability \", ym)\n",
    "            test_prob = [ym for _i in range(len(test_y))]\n",
    "        else:\n",
    "            print (\"predict 0 always with probability \", 1- ym)\n",
    "            test_prob = [1- ym for _i in range(len(test_y))]\n",
    "    else:\n",
    "        print (\"model parameter not recognized\")\n",
    "\n",
    "    test_prob = np.reshape(np.array(test_prob), (-1))\n",
    "    predsbool = np.reshape(np.array(test_pred).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(test_y).astype(bool), (-1))\n",
    "\n",
    "    bacc3 = balancedacc(obsvsbool, test_prob, 0.3)\n",
    "    bacc5 = balancedacc(obsvsbool, test_prob, 0.5)\n",
    "    bacc7 = balancedacc(obsvsbool, test_prob, 0.7)\n",
    "    mbacc = maxbacc(obsvsbool, test_prob)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, test_prob)\n",
    "    print (\"\")\n",
    "    print (\"for model \", model)\n",
    "    sauc = subjauc(subjs, obsvsbool, test_prob)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    ll = sklearn.metrics.log_loss(np.squeeze(obsvsbool), np.squeeze(test_prob)) / float(len(test_prob))\n",
    "    print (\"balanced accuracy is : \", round(bacc5,4))\n",
    "    print (\"auc is : \", round(auc,4))\n",
    "    print (\"log loss is : \", '%.4E' % Decimal(ll))\n",
    "    return [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll]\n",
    "\n",
    "def runmodel(model, traintype, testtype, NFEATS):\n",
    "    # define variables\n",
    "    train_y = stripfile('y'+str(trainsuffix)+filename1+'.csv')\n",
    "    test_y = stripfile('y'+str(testsuffix)+filename1+'.csv')\n",
    "    subjs = np.genfromtxt('y'+str(testsuffix)+filename1+'.csv', delimiter=\",\")[1:,0]\n",
    "    np.save(\"test_obsv.npy\", test_y)\n",
    "    if model == \"topfeatures\":\n",
    "        train_x = strip(subsetxfile('x'+str(trainsuffix) + filename2 +'.csv', featrank[0:NFEATS]))\n",
    "        test_x = strip(subsetxfile('x'+str(testsuffix) + filename2 +'.csv', featrank[0:NFEATS]))\n",
    "    elif model == \"baseline\":\n",
    "        pass\n",
    "    else:\n",
    "        print (\"model parameter not recognized\")\n",
    "\n",
    "    # create model and start predictions\n",
    "    if model == \"topfeatures\":\n",
    "        mf = LogisticRegression(solver='liblinear', penalty = 'l1', C = 1/LAMBDA)\n",
    "        mf.fit(train_x, train_y)\n",
    "        test_pred = mf.predict(test_x)\n",
    "        temp = mf.predict_proba(test_x)\n",
    "        test_prob = np.reshape(np.array(temp[:,1]), (-1))\n",
    "    elif model == \"baseline\":\n",
    "        ym = np.mean(train_y)\n",
    "        yb = ym > 0.5\n",
    "        test_pred = [yb for _i in range(len(test_y))]\n",
    "        if yb == 1:\n",
    "            print (\"predict 1 always with probability \", ym)\n",
    "            test_prob = [ym for _i in range(len(test_y))]\n",
    "        else:\n",
    "            print (\"predict 0 always with probability \", 1- ym)\n",
    "            test_prob = [1- ym for _i in range(len(test_y))]\n",
    "    else:\n",
    "        print (\"model parameter not recognized\")\n",
    "\n",
    "    test_prob = np.reshape(np.array(test_prob), (-1))\n",
    "    predsbool = np.reshape(np.array(test_pred).astype(bool), (-1))\n",
    "    obsvsbool = np.reshape(np.array(test_y).astype(bool), (-1))\n",
    "\n",
    "    bacc3 = balancedacc(obsvsbool, test_prob, 0.3)\n",
    "    bacc5 = balancedacc(obsvsbool, test_prob, 0.5)\n",
    "    bacc7 = balancedacc(obsvsbool, test_prob, 0.7)\n",
    "    mbacc = maxbacc(obsvsbool, test_prob)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, test_prob)\n",
    "    print (\"\")\n",
    "    print (\"for model \", model)\n",
    "    sauc = subjauc(subjs, obsvsbool, test_prob)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    ll = sklearn.metrics.log_loss(np.squeeze(obsvsbool), np.squeeze(test_prob)) / float(len(test_prob))\n",
    "    print (\"balanced accuracy is : \", round(bacc5,4))\n",
    "    print (\"auc is : \", round(auc,4))\n",
    "    print (\"log loss is : \", '%.4E' % Decimal(ll))\n",
    "    return [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll]\n",
    "\n",
    "models =  [\"topfeatures\"]\n",
    "NFEATSs = [len(featrank)]\n",
    "\n",
    "filename1 = '_normed_standardized_Actual_EV0.5' # to load y\n",
    "filename2 = '_normed_standardized_Actual_EV0.5_interactions' # to load x\n",
    "\n",
    "traintype = 'random'\n",
    "testtype = 'random'\n",
    "\n",
    "if traintype == \"1block\":\n",
    "    trainsuffix = \"_1block\"\n",
    "elif traintype == \"random\":\n",
    "    trainsuffix = \"_random\"\n",
    "\n",
    "if testtype == \"3block\":\n",
    "    testsuffix = \"_3block\"\n",
    "elif testtype == \"1block\":\n",
    "    testsuffix = \"_1block\"\n",
    "elif testtype == \"random\":\n",
    "    testsuffix = \"_random\"\n",
    "\n",
    "\n",
    "with open('results.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "\n",
    "    LAMBDA = 0.1\n",
    "\n",
    "    # all subjects\n",
    "    writer.writerow([\"all test subjects\"])\n",
    "    writer.writerow([LAMBDA])\n",
    "    writer.writerow([\"model\", \"number of features\", \"bacc3\",\"bacc5\",\"bacc7\", \"mbacc\", \"auc\", \"sauc\", \"ll\"])\n",
    "    for model, NFEATS, color in zip(models, NFEATSs, colors):\n",
    "        [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll] = runmodel(model, traintype, testtype, NFEATS)\n",
    "        writer.writerow([model, str(NFEATS), round(bacc3,4), round(bacc5,4),\n",
    "            round(bacc7,4), round(mbacc,4),\n",
    "            round(auc,4),round(sauc,4),'%.4E' % Decimal(ll)])\n",
    "\n",
    "    LAMBDA = 1.0\n",
    "\n",
    "    # all subjects\n",
    "    writer.writerow([\"all test subjects\"])\n",
    "    writer.writerow([LAMBDA])\n",
    "    writer.writerow([\"model\", \"number of features\", \"bacc3\",\"bacc5\",\"bacc7\", \"mbacc\", \"auc\", \"sauc\", \"ll\"])\n",
    "    for model, NFEATS, color in zip(models, NFEATSs, colors):\n",
    "        [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll] = runmodel(model, traintype, testtype, NFEATS)\n",
    "        writer.writerow([model, str(NFEATS), round(bacc3,4), round(bacc5,4),\n",
    "            round(bacc7,4), round(mbacc,4),\n",
    "            round(auc,4),round(sauc,4),'%.4E' % Decimal(ll)])\n",
    "\n",
    "    LAMBDA = 10.0\n",
    "\n",
    "    # all subjects\n",
    "    writer.writerow([\"all test subjects\"])\n",
    "    writer.writerow([LAMBDA])\n",
    "    writer.writerow([\"model\", \"number of features\", \"bacc3\",\"bacc5\",\"bacc7\", \"mbacc\", \"auc\", \"sauc\", \"ll\"])\n",
    "    for model, NFEATS, color in zip(models, NFEATSs, colors):\n",
    "        [bacc3, bacc5, bacc7, mbacc, fpr, tpr, auc, sauc, ll] = runmodel(model, traintype, testtype, NFEATS)\n",
    "        writer.writerow([model, str(NFEATS), round(bacc3,4), round(bacc5,4),\n",
    "            round(bacc7,4), round(mbacc,4),\n",
    "            round(auc,4),round(sauc,4),'%.4E' % Decimal(ll)])\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b16284",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a91d9",
   "metadata": {},
   "source": [
    "## Estimate Weights of a Model Trained on Entire Data Set\n",
    "\n",
    "Output: \n",
    "* npy files with weights of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4459f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET = \"random\"\n",
    "\n",
    "filename = '_normed_standardized_Actual_EV'\n",
    "gamma = 0.5\n",
    "suffix = \"_\"+ET\n",
    "\n",
    "LAMBDAs = [0.1, 1.0, 10.0]\n",
    "\n",
    "x = np.genfromtxt(\"x\" + suffix + filename + str(gamma) + \"_interactions.csv\", delimiter=\",\")\n",
    "y = np.genfromtxt(\"y\" + suffix + filename + str(gamma) + \".csv\", delimiter=\",\")\n",
    "\n",
    "\n",
    "for LAMBDA in LAMBDAs:\n",
    "    xs = x[1:,1:]\n",
    "    ys = y[1:,1:]\n",
    "    print (xs.shape)\n",
    "    print (ys.shape)\n",
    "\n",
    "    clf = LogisticRegression(penalty = 'l1', C = 1/LAMBDA, solver='liblinear')\n",
    "    clf.fit(xs, ys.ravel())\n",
    "    w = clf.coef_.ravel()\n",
    "\n",
    "    np.save(ET +\"_lambda\" + str(LAMBDA) +  \"_weights\", w)\n",
    "    print (ET +\"_lambda\" + str(LAMBDA) +  \"_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69da05",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7402b3",
   "metadata": {},
   "source": [
    "## Visualize Weights of a Model Trained on Entire Data Set\n",
    "\n",
    "Output: \n",
    "* csv file of model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 1.0\n",
    "\n",
    "\n",
    "ET1 = \"random\"\n",
    "\n",
    "wnames = np.load('x_random_normed_standardized_Actual_EV0.5_interactions_names.npy')\n",
    "w1 = np.load(ET1 +\"_lambda\" + str(LAMBDA) + \"_weights.npy\")\n",
    "\n",
    "wm = [np.absolute(i) for i in w1]\n",
    "\n",
    "w1 = [x for _,x in sorted(zip(wm,w1), reverse=True)]\n",
    "wnames = [x for _,x in sorted(zip(wm,wnames), reverse=True)]\n",
    "\n",
    "with open('model_weights_'+ET1+'.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    for i in range(len(wnames)):\n",
    "        writer.writerow([wnames[i], w1[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e2736",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d25c",
   "metadata": {},
   "source": [
    "## Compare Weights of a Model Trained on Entire Data Set\n",
    "\n",
    "Output: \n",
    "* heatmap with weights of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10.0\n",
    "\n",
    "\n",
    "ET1 = \"random\"\n",
    "ET2 = \"random\"\n",
    "\n",
    "wnames = np.load('x_random_normed_standardized_Actual_EV0.5_interactions_names.npy')\n",
    "w1 = np.load(ET1 +\"_lambda\" + str(LAMBDA) + \"_weights.npy\")\n",
    "w2 = np.load(ET2 +\"_lambda\" + str(LAMBDA) + \"_weights.npy\")\n",
    "\n",
    "\n",
    "\n",
    "wm = [np.maximum(np.absolute(i),np.absolute(j)) for i,j in zip(w1,w2)]\n",
    "\n",
    "\n",
    "w1 = [x for _,x in sorted(zip(wm,w1), reverse=True)]\n",
    "w2 = [x for _,x in sorted(zip(wm,w2), reverse=True)]\n",
    "wnames = [x for _,x in sorted(zip(wm,wnames), reverse=True)]\n",
    "\n",
    "\n",
    "\n",
    "NFEATS = 10\n",
    "\n",
    "with open('names.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    for i in range(NFEATS):\n",
    "        writer.writerow([wnames[i]])\n",
    "\n",
    "\n",
    "matrix = np.transpose(np.array([w1[:NFEATS], w2[:NFEATS]]))\n",
    "\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "ax = sns.heatmap(matrix, center=0, annot=True, fmt=\".1f\",\n",
    "                yticklabels=wnames[:NFEATS], xticklabels=[ET1, ET2], cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78284c3",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a47e34",
   "metadata": {},
   "source": [
    "# Neural Network Modeling and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9647ef7",
   "metadata": {},
   "source": [
    "## Estimate LOOCV accuracy\n",
    "\n",
    "Output: \n",
    "* csv file of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 5\n",
    "SUFFIX = \"nn\"\n",
    "training_epochs = 3000\n",
    "LAYERS = 2\n",
    "REG = \"l1\"\n",
    "NOISE = \"adversarial\"\n",
    "adv = \"sign\" # can include \"nosign\"\n",
    "\n",
    "LAMBDAs = [1e-1]\n",
    "epsilons = [0.0]\n",
    "nSEEDS = 3\n",
    "\n",
    "\n",
    "def run(NOISE, adv, LAMBDA, epsilon, REG, SEED):\n",
    "    tf.set_random_seed(SEED)\n",
    "\n",
    "    x = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5_stdz.csv', delimiter=\",\")\n",
    "    y = np.genfromtxt('y_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "\n",
    "    sf = np.genfromtxt('gps_random.csv', delimiter=\",\", usecols=((0,1)))  # subject file\n",
    "    ORIGsids = np.array(sf[1:,1]) # subject ids\n",
    "    n = len(ORIGsids)\n",
    "\n",
    "    if SEED == 0:\n",
    "        sids = ORIGsids\n",
    "    else:\n",
    "        np.random.seed(SEED) ; sids = np.random.choice(ORIGsids, len(ORIGsids))\n",
    "\n",
    "    # newer version\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape = [None,x.shape[1]-1])\n",
    "    N = tf.placeholder(tf.float32, shape = [None,x.shape[1]-1])\n",
    "    Ga = tf.placeholder(tf.float32, shape = [None,x.shape[1]-1])\n",
    "    Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "    if REG == \"l2\":\n",
    "        l = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
    "    elif REG == \"l1\":\n",
    "        l = tf.contrib.layers.l1_regularizer(scale=LAMBDA)\n",
    "    else:\n",
    "        print(\"parameter 'REG' not recognized\")\n",
    "    if LAYERS == 2:\n",
    "        H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "        H2 = tf.contrib.layers.fully_connected(H1,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "        L = tf.contrib.layers.fully_connected(H2,1,activation_fn=None,weights_regularizer=l)\n",
    "    elif LAYERS == 1:\n",
    "        H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "        L = tf.contrib.layers.fully_connected(H1,1,activation_fn=None,weights_regularizer=l)\n",
    "    else:\n",
    "        print (\"LAYERS parameter not recognized\")\n",
    "    l_loss = tf.losses.get_regularization_loss()\n",
    "    P = tf.nn.sigmoid(L)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=L)) # cross entropy\n",
    "    if adv == \"sign\":\n",
    "        A = tf.stop_gradient(tf.sign(tf.gradients(cost,X)))\n",
    "    elif adv == \"nosign\":\n",
    "        A = tf.stop_gradient(tf.gradients(cost,X))\n",
    "    else:\n",
    "        print (\"not recognized value of adv\")\n",
    "    XA = tf.stop_gradient(X + N * A)\n",
    "    XGa = tf.stop_gradient(X + N * Ga)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost+l_loss)\n",
    "    G = tf.stop_gradient(tf.gradients(L,X))\n",
    "    H = tf.stop_gradient(tf.reduce_sum(tf.squeeze(tf.hessians(L,X)),axis=2))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    final_test_costs = []\n",
    "    obsvs = []\n",
    "    preds = []\n",
    "    grads = []\n",
    "    for fold in range(n):\n",
    "        [test_x, test_y] = subjXY(sids[fold], x, y)\n",
    "        [train_x, train_y] = notsubjXY(sids[fold], x, y)\n",
    "        test_y = np.reshape(test_y, (-1,1))\n",
    "        train_y = np.reshape(train_y, (-1,1))\n",
    "\n",
    "        o = test_y\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            # try new version from nn_boot_hessians.py\n",
    "            for epoch in range(training_epochs):\n",
    "                n = np.random.uniform(low=0.0, high=epsilon, size=train_x.shape)\n",
    "                xa = np.squeeze(sess.run(XA, feed_dict={X : train_x, Y: 1.0 - train_y, N : n}))\n",
    "                if epoch != training_epochs - 1:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c = sess.run([optimizer, cost], feed_dict={X: np.concatenate([train_x, xa], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = train_x.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : train_x, Ga: ga, N : n}))\n",
    "                        _, c= sess.run([optimizer, cost], feed_dict={X: np.concatenate([train_x, xga], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "                else:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c, g = sess.run([optimizer, cost, G], feed_dict={X: np.concatenate([train_x, xa], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = xs.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : train_x, Ga: ga, N : n}))\n",
    "                        _, c, g = sess.run([optimizer, cost, G], feed_dict={X: np.concatenate([train_x, xga], axis=0), Y: np.concatenate([train_y, train_y], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "                print(\"Epoch: {} - train cost: {}\".format(epoch, c))\n",
    "            p = sess.run(P, feed_dict={X: test_x})\n",
    "            print (p)\n",
    "\n",
    "            saver.save(sess, \"./model\"+str(fold+1)+\"_\"+SUFFIX+\"_lambda\"+str(LAMBDA)+\".ckpt\") # checkpoint\n",
    "        preds.append(p)\n",
    "        obsvs.append(o)\n",
    "        grads.append(g)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_pred.npy\", preds)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_obsv.npy\", obsvs)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_grads.npy\", grads)\n",
    "    np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_lambda\"+str(LAMBDA)+\"_adv\"+str(adv)+\"_reg\"+str(REG)+\"_units\"+str(UNITS)+\"_testcosts.npy\", final_test_costs)\n",
    "\n",
    "\n",
    "    f = lambda x : x > 0.5\n",
    "    predsbool = np.array(map(f, preds))\n",
    "    totaln = len(predsbool) * len(predsbool[0])\n",
    "    preds = np.reshape(np.array(preds), [totaln])\n",
    "    predsbool = np.reshape(predsbool, [totaln])\n",
    "    obsvsbool = np.squeeze(np.array(obsvs).astype(bool))\n",
    "    obsvsbool = np.reshape(obsvsbool, [totaln])\n",
    "\n",
    "    bacc = balancedacc(obsvsbool, preds, 0.5)\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(obsvsbool, predsbool)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(obsvsbool, predsbool)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    print (SEED, LAMBDA, epsilon)\n",
    "    print (sids)\n",
    "    print (\"balanced accuracy is : \", round(bacc,4))\n",
    "    print (\"auc is : \", auc)\n",
    "\n",
    "    return [round(bacc,4), auc]#, ll]\n",
    "\n",
    "# just in case it breaks somewhere\n",
    "with open('nn_folds_results.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    results = []\n",
    "    for SEED in range(nSEEDS):\n",
    "        for epsilon in epsilons:\n",
    "            for LAMBDA in LAMBDAs:\n",
    "                print (epsilon, LAMBDA, SEED)\n",
    "                result = run(NOISE, adv, LAMBDA, epsilon, REG, SEED)\n",
    "                result.extend([\"epsilon : \" + str(epsilon), \"lambda : \"+ str(LAMBDA), \"SEED : \" + str(SEED)])\n",
    "                w.writerow(result)\n",
    "                results.append(result)\n",
    "    f.close()\n",
    "\n",
    "# all the results\n",
    "with open('nn_folds_results_all.csv', 'w') as f:\n",
    "    fw = csv.writer(f)\n",
    "    fw.writerows(results)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8a5b3",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11b5eb",
   "metadata": {},
   "source": [
    "## Generate Hessian Values\n",
    "Output\n",
    "* npy files with hessian variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a254d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5_stdz.csv', delimiter=\",\")\n",
    "y = np.genfromtxt('y_random_normed_standardized_Actual_EV0.5.csv', delimiter=\",\")\n",
    "\n",
    "# get column names\n",
    "xtest = np.genfromtxt('x_random_normed_standardized_Actual_EV0.5_stdz.csv', delimiter=\",\", names=True)\n",
    "xcolnames = xtest.dtype.names\n",
    "fnames = xcolnames[1:]\n",
    "nFEATS = len(fnames)\n",
    "np.save(\"featnames\", fnames)\n",
    "\n",
    "\n",
    "sf = np.genfromtxt('gps_random.csv', delimiter=\",\", usecols=((0,1))) # subject file\n",
    "ORIGsids = np.array(sf[1:,1]) # subject ids\n",
    "\n",
    "SUFFIX = \"nnboot\"\n",
    "UNITS = 5\n",
    "training_epochs = 3000\n",
    "LAYERS = 2\n",
    "REGs = [\"l1\"]\n",
    "noises = [\"adversarial\"]\n",
    "advs = [\"sign\"] # can include \"nosign\"\n",
    "LAMBDAs = [1e-4]\n",
    "epsilons = [1e-1] # adversarial noise\n",
    "nSEEDS = 1\n",
    "\n",
    "\n",
    "epsilonSGs = [0.2] # smooth grad\n",
    "\n",
    "nSMOOTHGRAD = 50\n",
    "nSUBJ =  len(ORIGsids)\n",
    "nOBSV = y.shape[0] - 1\n",
    "nTRIALS = nOBSV / nSUBJ\n",
    "\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "def runseeds(NOISE, adv, LAMBDA, epsilon, REG, epsilonSG):\n",
    "    hesss = np.zeros(shape=[nSEEDS*nSUBJ*nTRIALS,nFEATS,nFEATS])\n",
    "    for SEED in range(1,nSEEDS + 1):\n",
    "\n",
    "        tf.set_random_seed(SEED)\n",
    "\n",
    "        if SEED == 0:\n",
    "            sids = ORIGsids\n",
    "        else:\n",
    "            np.random.seed(SEED) ; sids = np.random.choice(ORIGsids, len(ORIGsids))\n",
    "\n",
    "        xs = np.zeros(shape=[x.shape[0]-1, x.shape[1]-1])\n",
    "        ys = np.zeros(shape=[y.shape[0]-1, 1])\n",
    "        xbool = np.zeros(shape=y.shape[0]-1)\n",
    "\n",
    "        counter = 0\n",
    "        for i in range(len(sids)):\n",
    "            [sx, sy, size] = subjXY(sids[i], x, y)\n",
    "            xs[range(counter, counter+size), :] = sx\n",
    "            ys[range(counter, counter+size)] = sy\n",
    "            counter = counter + size\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape = [None,xs.shape[1]])\n",
    "        N = tf.placeholder(tf.float32, shape = [None,xs.shape[1]])\n",
    "        Ga = tf.placeholder(tf.float32, shape = [None,xs.shape[1]])\n",
    "        Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "        if REG == \"l2\":\n",
    "            l = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
    "        elif REG == \"l1\":\n",
    "            l = tf.contrib.layers.l1_regularizer(scale=LAMBDA)\n",
    "        else:\n",
    "            print(\"parameter 'REG' not recognized\")\n",
    "        if LAYERS == 2:\n",
    "            H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "            H2 = tf.contrib.layers.fully_connected(H1,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "            L = tf.contrib.layers.fully_connected(H2,1,activation_fn=None,weights_regularizer=l)\n",
    "        elif LAYERS == 1:\n",
    "            H1 = tf.contrib.layers.fully_connected(X,UNITS,activation_fn=tf.nn.sigmoid,weights_regularizer=l)\n",
    "            L = tf.contrib.layers.fully_connected(H1,1,activation_fn=None,weights_regularizer=l)\n",
    "        else:\n",
    "            print (\"LAYERS parameter not recognized\")\n",
    "        l_loss = tf.losses.get_regularization_loss()\n",
    "        P = tf.nn.sigmoid(L)\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=L)) # cross entropy\n",
    "        if adv == \"sign\":\n",
    "            A = tf.stop_gradient(tf.sign(tf.gradients(cost,X)))\n",
    "        elif adv == \"nosign\":\n",
    "            A = tf.stop_gradient(tf.gradients(cost,X))\n",
    "        else:\n",
    "            print (\"not recognized value of adv\")\n",
    "        XA = tf.stop_gradient(X + N * A)\n",
    "        XGa = tf.stop_gradient(X + N * Ga)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost+l_loss)\n",
    "        G = tf.stop_gradient(tf.gradients(L,X))\n",
    "        H = tf.stop_gradient(tf.reduce_sum(tf.squeeze(tf.hessians(L,X)),axis=2))\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            for epoch in range(training_epochs):\n",
    "                n = np.random.uniform(low=0.0, high=epsilon, size=xs.shape)\n",
    "                xa = np.squeeze(sess.run(XA, feed_dict={X : xs, Y: 1.0 - ys, N : n}))\n",
    "                if epoch != training_epochs - 1:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c = sess.run([optimizer, cost], feed_dict={X: np.concatenate([xs, xa], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = xs.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : xs, Ga: ga, N : n}))\n",
    "                        _, c= sess.run([optimizer, cost], feed_dict={X: np.concatenate([xs, xga], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "                else:\n",
    "                    if NOISE == \"adversarial\":\n",
    "                        _, c, g, h = sess.run([optimizer, cost, G, H], feed_dict={X: np.concatenate([xs, xa], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[0,:,:]))\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[211,:,:]))\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[562,:,:]))\n",
    "                        print (\"is symmetric? : \", check_symmetric(h[736,:,:]))\n",
    "                    elif NOISE == \"gaussian\":\n",
    "                        ga = np.random.normal(size = xs.shape)\n",
    "                        xga = np.squeeze(sess.run(XGa, feed_dict={X : xs, Ga: ga, N : n}))\n",
    "                        _, c, g, h = sess.run([optimizer, cost, G, H], feed_dict={X: np.concatenate([xs, xga], axis=0), Y: np.concatenate([ys, ys], axis=0)})\n",
    "                    else:\n",
    "                        print (\"encountered error\")\n",
    "            _save_path = saver.save(sess, \"./model_temp\" +\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+ \".ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "            saver.restore(sess, \"./model_temp\" +\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+ \".ckpt\")\n",
    "\n",
    "            h = sess.run(H, feed_dict={X: xs, Y: ys})\n",
    "\n",
    "            # regular hessian\n",
    "            h = np.squeeze(h)\n",
    "            print (\"hess non-zero entries : \", np.not_equal(h,0.0).sum())\n",
    "            print (\"hess shape is : \", h.shape)\n",
    "            hesss[(SEED-1)*nSUBJ*nTRIALS: (SEED)*nSUBJ*nTRIALS,:,:] = h[range(nSUBJ*nTRIALS), :, :]\n",
    "            if NOISE == \"adversarial\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_hesss.npy\", np.squeeze(h))\n",
    "            elif NOISE == \"gaussian\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_units\"+str(UNITS)+\"_hesss.npy\", np.squeeze(h))\n",
    "            else:\n",
    "                print (\"encountered error\")\n",
    "\n",
    "            # smooth grad hessians\n",
    "\n",
    "            # smooth grad\n",
    "            hess_sums = np.zeros(shape = np.squeeze(h).shape)\n",
    "            for i in range(nSMOOTHGRAD):\n",
    "                np.random.seed(i); n = np.random.uniform(low=0.0, high=epsilonSG, size=xs.shape)\n",
    "                np.random.seed(i); ga = np.random.normal(size = xs.shape)\n",
    "                xga = np.squeeze(sess.run(XGa, feed_dict={X : xs, Ga: ga, N : n}))\n",
    "                h = sess.run(H, feed_dict={X: xga, Y: ys})\n",
    "                hess_sums = hess_sums + np.squeeze(h)\n",
    "\n",
    "            if NOISE == \"adversarial\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_points\"+ str(nSMOOTHGRAD) + \"_hesss_sums.npy\", hess_sums)\n",
    "            elif NOISE == \"gaussian\":\n",
    "                np.save(SUFFIX+\"_SEED\"+str(SEED)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_units\"+str(UNITS)+\"_points\"+ str(nSMOOTHGRAD) +\"_hesss_sums.npy\", hess_sums)\n",
    "            else:\n",
    "                print (\"encountered error\")\n",
    "\n",
    "    if NOISE == \"adversarial\":\n",
    "        np.save(SUFFIX+\"_nSEED\"+str(nSEEDS)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_hesss.npy\", hesss)\n",
    "    elif NOISE == \"gaussian\":\n",
    "        np.save(SUFFIX+\"_nSEED\"+str(nSEEDS)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_units\"+str(UNITS)+\"_hesss.npy\", hesss)\n",
    "    else:\n",
    "        print (\"encountered error\")\n",
    "\n",
    "for NOISE in noises:\n",
    "    for REG in REGs:\n",
    "        for adv in advs:\n",
    "            for LAMBDA in LAMBDAs:\n",
    "                for epsilon in epsilons:\n",
    "                    for epsilonSG in epsilonSGs:\n",
    "                        print NOISE, adv, LAMBDA, epsilon, epsilonSG\n",
    "                        runseeds(NOISE, adv, LAMBDA, epsilon, REG, epsilonSG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f824a",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff293aa",
   "metadata": {},
   "source": [
    "## Visualize Hessian Matrices\n",
    "\n",
    "Output: \n",
    "* heatmaps of prevalence, positive ratios, negative ratios and average hessian values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subprocess_cmd(command):\n",
    "    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "# ratio of positives in list l\n",
    "def posrate(l):\n",
    "    b = len(l)\n",
    "    a = 0\n",
    "    for e in l:\n",
    "        if e > 0.0:\n",
    "            a = a + 1\n",
    "    return float(a) / float(b)\n",
    "\n",
    "# ratio of negatives in list l\n",
    "def negrate(l):\n",
    "    b = len(l)\n",
    "    a = 0\n",
    "    for e in l:\n",
    "        if e < 0.0:\n",
    "            a = a + 1\n",
    "    return float(a) / float(b)\n",
    "\n",
    "def visSG(NOISE, adv, LAMBDA, epsilon, REG, SEED):\n",
    "    # smooth grads\n",
    "\n",
    "    fn = SUFFIX+\"_SEED\"+str(SEED)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_points\"+ str(nSMOOTHGRAD) + \"_hesss_sums.npy\"\n",
    "    hessssums = np.load(fn)\n",
    "    hesss = hessssums / nSMOOTHGRAD\n",
    "    print (hesss.shape)\n",
    "    for ind in range(hesss.shape[0]):\n",
    "        hi = hesss[ind,:,:]\n",
    "\n",
    "    means = hesss.mean(axis=0)\n",
    "    stds = hesss.std(axis=0)\n",
    "\n",
    "    poss = np.zeros(shape = means.shape)\n",
    "    negs = np.zeros(shape = means.shape)\n",
    "\n",
    "    for i in range(nFEATS):\n",
    "        for j in range(nFEATS):\n",
    "            v = hesss[:,i,j]\n",
    "            poss[i,j] = posrate(v)\n",
    "            negs[i,j] = negrate(v)\n",
    "\n",
    "    print (\"are the poss symmetric? : \", check_symmetric(poss))\n",
    "    print (\"are the negs symmetric? : \", check_symmetric(negs))\n",
    "\n",
    "    with open('hessian_smoothgrad_results.csv', 'w') as f:\n",
    "        fw = csv.writer(f)\n",
    "        fw.writerow(['means'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(means)\n",
    "        fw.writerow(['stds'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(stds)\n",
    "        fw.writerow(['poss'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(poss)\n",
    "        fw.writerow(['negs'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(negs)\n",
    "        f.close()\n",
    "\n",
    "    return [means, stds, poss, negs]\n",
    "\n",
    "def visREG(NOISE, adv, LAMBDA, epsilon, REG, SEED):\n",
    "    fn = SUFFIX+\"_SEED\"+str(SEED)+\"_layers\"+str(LAYERS)+\"_\"+REG+\"_lambda\"+str(LAMBDA)+\"_epsilon\"+str(epsilon)+\"_epsilonSG\"+str(epsilonSG)+\"_noise\"+NOISE+\"_sign\"+adv+\"_units\"+str(UNITS)+\"_hesss\"\n",
    "    hesss = np.load(fn+\".npy\")\n",
    "\n",
    "    means = hesss.mean(axis=0)\n",
    "    stds = hesss.std(axis=0)\n",
    "\n",
    "    poss = np.zeros(shape = means.shape)\n",
    "    negs = np.zeros(shape = means.shape)\n",
    "\n",
    "    for i in range(nFEATS):\n",
    "        for j in range(nFEATS):\n",
    "            poss[i,j] = posrate(hesss[i,j])\n",
    "            negs[i,j] = negrate(hesss[i,j])\n",
    "\n",
    "\n",
    "    with open('hessian_results.csv', 'w') as f:\n",
    "        fw = csv.writer(f)\n",
    "        fw.writerow(['means'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(means)\n",
    "        fw.writerow(['stds'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(stds)\n",
    "        fw.writerow(['poss'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(poss)\n",
    "        fw.writerow(['negs'])\n",
    "        fw.writerow(fnames)\n",
    "        fw.writerows(negs)\n",
    "        f.close()\n",
    "        f.close()\n",
    "\n",
    "    return [means, stds, poss, negs]\n",
    "\n",
    "for NOISE in noises:\n",
    "    for REG in REGs:\n",
    "        for adv in advs:\n",
    "            for LAMBDA in LAMBDAs:\n",
    "                for epsilon in epsilons:\n",
    "                    for epsilonSG in epsilonSGs:\n",
    "                        # positive ratio\n",
    "                        psgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        # negative ratio\n",
    "                        nsgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        # positive ratio minus negative ratio in smooth grads\n",
    "                        pmnsgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        # means\n",
    "                        msgs = np.zeros(shape=(nSEEDS, nFEATS, nFEATS))\n",
    "                        for SEED in range(1,nSEEDS+1):\n",
    "                            print (NOISE, adv, LAMBDA, epsilon, SEED)\n",
    "                            msg, ssg, psg, nsg = visSG(NOISE, adv, LAMBDA, epsilon, REG, SEED)\n",
    "                            print (\"is psg symmetric? : \", check_symmetric(psg))\n",
    "                            print (\"is nsg symmetric? : \", check_symmetric(nsg))\n",
    "                            psgs[SEED - 1, :, :] = psg\n",
    "                            nsgs[SEED - 1, :, :] = nsg\n",
    "                            pmnsgs[SEED - 1, :, :] = psg - nsg\n",
    "                            msgs[SEED - 1, :, :] = msg\n",
    "\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(msgs.mean(axis=0), center=0, annot=True, fmt=\".4f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad average Hessian value across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(pmnsgs.mean(axis=0), center=0, annot=True, fmt=\".4f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad expected prevalence across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(psgs.mean(axis=0), center = 0.5, annot=True, fmt=\".3f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad expected positive ratio across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()\n",
    "\n",
    "                        cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "                        ax = sns.heatmap(nsgs.mean(axis=0), center = 0.5, annot=True, fmt=\".3f\",\n",
    "                                         yticklabels=fnames , cmap=cmap) # xticklabels=fnames,\n",
    "                        ax.set_title(\"hessian smoothgrad expected negative ratio across \" +str(nSEEDS)+ \" resamplings\")\n",
    "                        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6f122",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ef808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496b28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119498ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
